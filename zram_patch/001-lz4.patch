Subject: [PATCH] incfs: Use ARM64 v8 ASM to accelerate lz4 decompression
crypto: lz4: Use ARM64 v8 ASM to accelerate decompression
lz4: 1.10
---
Index: fs/f2fs/Makefile
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fs/f2fs/Makefile b/fs/f2fs/Makefile
--- a/fs/f2fs/Makefile	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ b/fs/f2fs/Makefile	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -10,6 +10,3 @@
 f2fs-$(CONFIG_FS_VERITY) += verity.o
 f2fs-$(CONFIG_F2FS_FS_COMPRESSION) += compress.o
 f2fs-$(CONFIG_F2FS_IOSTAT) += iostat.o
-ifeq ($(CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT),y)
-f2fs-$(CONFIG_ARM64) += $(addprefix lz4armv8/, lz4accel.o lz4armv8.o)
-endif
Index: fs/f2fs/compress.c
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fs/f2fs/compress.c b/fs/f2fs/compress.c
--- a/fs/f2fs/compress.c	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ b/fs/f2fs/compress.c	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -20,7 +20,6 @@
 #include "segment.h"
 #include <trace/events/f2fs.h>
 #if defined(CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT) || defined(__ARCH_HAS_LZ4_ACCELERATOR)
-#include "lz4armv8/lz4accel.h"
 #include "f2fs_lz4.h"
 #endif
 
Index: include/linux/lz4.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/include/linux/lz4.h b/include/linux/lz4.h
--- a/include/linux/lz4.h	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ b/include/linux/lz4.h	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -1,648 +1,17 @@
-/* LZ4 Kernel Interface
- *
- * Copyright (C) 2013, LG Electronics, Kyungsik Lee <kyungsik.lee@lge.com>
- * Copyright (C) 2016, Sven Schmidt <4sschmid@informatik.uni-hamburg.de>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This file is based on the original header file
- * for LZ4 - Fast LZ compression algorithm.
- *
- * LZ4 - Fast LZ compression algorithm
- * Copyright (C) 2011-2016, Yann Collet.
- * BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *	* Redistributions of source code must retain the above copyright
- *	  notice, this list of conditions and the following disclaimer.
- *	* Redistributions in binary form must reproduce the above
- * copyright notice, this list of conditions and the following disclaimer
- * in the documentation and/or other materials provided with the
- * distribution.
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- * You can contact the author at :
- *	- LZ4 homepage : http://www.lz4.org
- *	- LZ4 source repository : https://github.com/lz4/lz4
- */
-
-#ifndef __LZ4_H__
-#define __LZ4_H__
-
-#include <linux/types.h>
-#include <linux/string.h>	 /* memset, memcpy */
-
-/*-************************************************************************
- *	CONSTANTS
- **************************************************************************/
-/*
- * LZ4_MEMORY_USAGE :
- * Memory usage formula : N->2^N Bytes
- * (examples : 10 -> 1KB; 12 -> 4KB ; 16 -> 64KB; 20 -> 1MB; etc.)
- * Increasing memory usage improves compression ratio
- * Reduced memory usage can improve speed, due to cache effect
- * Default value is 14, for 16KB, which nicely fits into Intel x86 L1 cache
- */
-#define LZ4_MEMORY_USAGE 14
-
-#define LZ4_MAX_INPUT_SIZE	0x7E000000 /* 2 113 929 216 bytes */
-#define LZ4_COMPRESSBOUND(isize)	(\
-	(unsigned int)(isize) > (unsigned int)LZ4_MAX_INPUT_SIZE \
-	? 0 \
-	: (isize) + ((isize)/255) + 16)
-
-#define LZ4_ACCELERATION_DEFAULT 1
-#define LZ4_HASHLOG	 (LZ4_MEMORY_USAGE-2)
-#define LZ4_HASHTABLESIZE (1 << LZ4_MEMORY_USAGE)
-#define LZ4_HASH_SIZE_U32 (1 << LZ4_HASHLOG)
-
-#define LZ4HC_MIN_CLEVEL			3
-#define LZ4HC_DEFAULT_CLEVEL			9
-#define LZ4HC_MAX_CLEVEL			16
-
-#define LZ4HC_DICTIONARY_LOGSIZE 16
-#define LZ4HC_MAXD (1<<LZ4HC_DICTIONARY_LOGSIZE)
-#define LZ4HC_MAXD_MASK (LZ4HC_MAXD - 1)
-#define LZ4HC_HASH_LOG (LZ4HC_DICTIONARY_LOGSIZE - 1)
-#define LZ4HC_HASHTABLESIZE (1 << LZ4HC_HASH_LOG)
-#define LZ4HC_HASH_MASK (LZ4HC_HASHTABLESIZE - 1)
-
-/*-************************************************************************
- *	STREAMING CONSTANTS AND STRUCTURES
- **************************************************************************/
-#define LZ4_STREAMSIZE_U64 ((1 << (LZ4_MEMORY_USAGE - 3)) + 4)
-#define LZ4_STREAMSIZE	(LZ4_STREAMSIZE_U64 * sizeof(unsigned long long))
-
-#define LZ4_STREAMHCSIZE        262192
-#define LZ4_STREAMHCSIZE_SIZET (262192 / sizeof(size_t))
-
-#define LZ4_STREAMDECODESIZE_U64	4
-#define LZ4_STREAMDECODESIZE		 (LZ4_STREAMDECODESIZE_U64 * \
-	sizeof(unsigned long long))
+/* SPDX-License-Identifier: BSD-2-Clause */
+// LZ4 compatibility wrapper for Linux kernel
 
-/*
- * LZ4_stream_t - information structure to track an LZ4 stream.
- */
-typedef struct {
-	uint32_t hashTable[LZ4_HASH_SIZE_U32];
-	uint32_t currentOffset;
-	uint32_t initCheck;
-	const uint8_t *dictionary;
-	uint8_t *bufferStart;
-	uint32_t dictSize;
-} LZ4_stream_t_internal;
-typedef union {
-	unsigned long long table[LZ4_STREAMSIZE_U64];
-	LZ4_stream_t_internal internal_donotuse;
-} LZ4_stream_t;
+#ifndef __LINUX_LZ4_H__
+#define __LINUX_LZ4_H__
 
-/*
- * LZ4_streamHC_t - information structure to track an LZ4HC stream.
- */
-typedef struct {
-	unsigned int	 hashTable[LZ4HC_HASHTABLESIZE];
-	unsigned short	 chainTable[LZ4HC_MAXD];
-	/* next block to continue on current prefix */
-	const unsigned char *end;
-	/* All index relative to this position */
-	const unsigned char *base;
-	/* alternate base for extDict */
-	const unsigned char *dictBase;
-	/* below that point, need extDict */
-	unsigned int	 dictLimit;
-	/* below that point, no more dict */
-	unsigned int	 lowLimit;
-	/* index from which to continue dict update */
-	unsigned int	 nextToUpdate;
-	unsigned int	 compressionLevel;
-} LZ4HC_CCtx_internal;
-typedef union {
-	size_t table[LZ4_STREAMHCSIZE_SIZET];
-	LZ4HC_CCtx_internal internal_donotuse;
-} LZ4_streamHC_t;
+#include "../../lib/lz4/lz4.h"
+#include "../../lib/lz4/lz4hc.h"
 
-/*
- * LZ4_streamDecode_t - information structure to track an
- *	LZ4 stream during decompression.
- *
- * init this structure using LZ4_setStreamDecode (or memset()) before first use
- */
-typedef struct {
-	const uint8_t *externalDict;
-	size_t extDictSize;
-	const uint8_t *prefixEnd;
-	size_t prefixSize;
-} LZ4_streamDecode_t_internal;
-typedef union {
-	unsigned long long table[LZ4_STREAMDECODESIZE_U64];
-	LZ4_streamDecode_t_internal internal_donotuse;
-} LZ4_streamDecode_t;
+#define LZ4_MEM_COMPRESS	LZ4_STREAM_MINSIZE
+#define LZ4HC_MEM_COMPRESS	LZ4_STREAMHC_MINSIZE
 
-/*-************************************************************************
- *	SIZE OF STATE
- **************************************************************************/
-#define LZ4_MEM_COMPRESS	LZ4_STREAMSIZE
-#define LZ4HC_MEM_COMPRESS	LZ4_STREAMHCSIZE
-
-/*-************************************************************************
- *	Compression Functions
- **************************************************************************/
-
-/**
- * LZ4_compressBound() - Max. output size in worst case szenarios
- * @isize: Size of the input data
- *
- * Return: Max. size LZ4 may output in a "worst case" szenario
- * (data not compressible)
- */
-static inline int LZ4_compressBound(size_t isize)
-{
-	return LZ4_COMPRESSBOUND(isize);
-}
-
-/**
- * LZ4_compress_default() - Compress data from source to dest
- * @source: source address of the original data
- * @dest: output buffer address of the compressed data
- * @inputSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
- * @maxOutputSize: full or partial size of buffer 'dest'
- *	which must be already allocated
- * @wrkmem: address of the working memory.
- *	This requires 'workmem' of LZ4_MEM_COMPRESS.
- *
- * Compresses 'sourceSize' bytes from buffer 'source'
- * into already allocated 'dest' buffer of size 'maxOutputSize'.
- * Compression is guaranteed to succeed if
- * 'maxOutputSize' >= LZ4_compressBound(inputSize).
- * It also runs faster, so it's a recommended setting.
- * If the function cannot compress 'source' into a more limited 'dest' budget,
- * compression stops *immediately*, and the function result is zero.
- * As a consequence, 'dest' content is not valid.
- *
- * Return: Number of bytes written into buffer 'dest'
- *	(necessarily <= maxOutputSize) or 0 if compression fails
- */
-int LZ4_compress_default(const char *source, char *dest, int inputSize,
-	int maxOutputSize, void *wrkmem);
-
-/**
- * LZ4_compress_fast() - As LZ4_compress_default providing an acceleration param
- * @source: source address of the original data
- * @dest: output buffer address of the compressed data
- * @inputSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
- * @maxOutputSize: full or partial size of buffer 'dest'
- *	which must be already allocated
- * @acceleration: acceleration factor
- * @wrkmem: address of the working memory.
- *	This requires 'workmem' of LZ4_MEM_COMPRESS.
- *
- * Same as LZ4_compress_default(), but allows to select an "acceleration"
- * factor. The larger the acceleration value, the faster the algorithm,
- * but also the lesser the compression. It's a trade-off. It can be fine tuned,
- * with each successive value providing roughly +~3% to speed.
- * An acceleration value of "1" is the same as regular LZ4_compress_default()
- * Values <= 0 will be replaced by LZ4_ACCELERATION_DEFAULT, which is 1.
- *
- * Return: Number of bytes written into buffer 'dest'
- *	(necessarily <= maxOutputSize) or 0 if compression fails
- */
-int LZ4_compress_fast(const char *source, char *dest, int inputSize,
-	int maxOutputSize, int acceleration, void *wrkmem);
-
-/**
- * LZ4_compress_destSize() - Compress as much data as possible
- *	from source to dest
- * @source: source address of the original data
- * @dest: output buffer address of the compressed data
- * @sourceSizePtr: will be modified to indicate how many bytes where read
- *	from 'source' to fill 'dest'. New value is necessarily <= old value.
- * @targetDestSize: Size of buffer 'dest' which must be already allocated
- * @wrkmem: address of the working memory.
- *	This requires 'workmem' of LZ4_MEM_COMPRESS.
- *
- * Reverse the logic, by compressing as much data as possible
- * from 'source' buffer into already allocated buffer 'dest'
- * of size 'targetDestSize'.
- * This function either compresses the entire 'source' content into 'dest'
- * if it's large enough, or fill 'dest' buffer completely with as much data as
- * possible from 'source'.
- *
- * Return: Number of bytes written into 'dest' (necessarily <= targetDestSize)
- *	or 0 if compression fails
- */
-int LZ4_compress_destSize(const char *source, char *dest, int *sourceSizePtr,
-	int targetDestSize, void *wrkmem);
-
-/*-************************************************************************
- *	Decompression Functions
- **************************************************************************/
-
-/**
- * LZ4_decompress_fast() - Decompresses data from 'source' into 'dest'
- * @source: source address of the compressed data
- * @dest: output buffer address of the uncompressed data
- *	which must be already allocated with 'originalSize' bytes
- * @originalSize: is the original and therefore uncompressed size
- *
- * Decompresses data from 'source' into 'dest'.
- * This function fully respect memory boundaries for properly formed
- * compressed data.
- * It is a bit faster than LZ4_decompress_safe().
- * However, it does not provide any protection against intentionally
- * modified data stream (malicious input).
- * Use this function in trusted environment only
- * (data to decode comes from a trusted source).
- *
- * Return: number of bytes read from the source buffer
- *	or a negative result if decompression fails.
- */
-int LZ4_decompress_fast(const char *source, char *dest, int originalSize);
-
-/**
- * LZ4_decompress_safe() - Decompression protected against buffer overflow
- * @source: source address of the compressed data
- * @dest: output buffer address of the uncompressed data
- *	which must be already allocated
- * @compressedSize: is the precise full size of the compressed block
- * @maxDecompressedSize: is the size of 'dest' buffer
- *
- * Decompresses data from 'source' into 'dest'.
- * If the source stream is detected malformed, the function will
- * stop decoding and return a negative result.
- * This function is protected against buffer overflow exploits,
- * including malicious data packets. It never writes outside output buffer,
- * nor reads outside input buffer.
- *
- * Return: number of bytes decompressed into destination buffer
- *	(necessarily <= maxDecompressedSize)
- *	or a negative result in case of error
- */
-int LZ4_decompress_safe(const char *source, char *dest, int compressedSize,
-	int maxDecompressedSize);
-
-/**
- * LZ4_decompress_safe_partial() - Decompress a block of size 'compressedSize'
- *	at position 'source' into buffer 'dest'
- * @source: source address of the compressed data
- * @dest: output buffer address of the decompressed data which must be
- *	already allocated
- * @compressedSize: is the precise full size of the compressed block.
- * @targetOutputSize: the decompression operation will try
- *	to stop as soon as 'targetOutputSize' has been reached
- * @maxDecompressedSize: is the size of destination buffer
- *
- * This function decompresses a compressed block of size 'compressedSize'
- * at position 'source' into destination buffer 'dest'
- * of size 'maxDecompressedSize'.
- * The function tries to stop decompressing operation as soon as
- * 'targetOutputSize' has been reached, reducing decompression time.
- * This function never writes outside of output buffer,
- * and never reads outside of input buffer.
- * It is therefore protected against malicious data packets.
- *
- * Return: the number of bytes decoded in the destination buffer
- *	(necessarily <= maxDecompressedSize)
- *	or a negative result in case of error
- *
- */
-int LZ4_decompress_safe_partial(const char *source, char *dest,
-	int compressedSize, int targetOutputSize, int maxDecompressedSize);
-
-/*-************************************************************************
- *	LZ4 HC Compression
- **************************************************************************/
-
-/**
- * LZ4_compress_HC() - Compress data from `src` into `dst`, using HC algorithm
- * @src: source address of the original data
- * @dst: output buffer address of the compressed data
- * @srcSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
- * @dstCapacity: full or partial size of buffer 'dst',
- *	which must be already allocated
- * @compressionLevel: Recommended values are between 4 and 9, although any
- *	value between 1 and LZ4HC_MAX_CLEVEL will work.
- *	Values >LZ4HC_MAX_CLEVEL behave the same as 16.
- * @wrkmem: address of the working memory.
- *	This requires 'wrkmem' of size LZ4HC_MEM_COMPRESS.
- *
- * Compress data from 'src' into 'dst', using the more powerful
- * but slower "HC" algorithm. Compression is guaranteed to succeed if
- * `dstCapacity >= LZ4_compressBound(srcSize)
- *
- * Return : the number of bytes written into 'dst' or 0 if compression fails.
- */
-int LZ4_compress_HC(const char *src, char *dst, int srcSize, int dstCapacity,
-	int compressionLevel, void *wrkmem);
-
-/**
- * LZ4_resetStreamHC() - Init an allocated 'LZ4_streamHC_t' structure
- * @streamHCPtr: pointer to the 'LZ4_streamHC_t' structure
- * @compressionLevel: Recommended values are between 4 and 9, although any
- *	value between 1 and LZ4HC_MAX_CLEVEL will work.
- *	Values >LZ4HC_MAX_CLEVEL behave the same as 16.
- *
- * An LZ4_streamHC_t structure can be allocated once
- * and re-used multiple times.
- * Use this function to init an allocated `LZ4_streamHC_t` structure
- * and start a new compression.
- */
-void LZ4_resetStreamHC(LZ4_streamHC_t *streamHCPtr, int compressionLevel);
-
-/**
- * LZ4_loadDictHC() - Load a static dictionary into LZ4_streamHC
- * @streamHCPtr: pointer to the LZ4HC_stream_t
- * @dictionary: dictionary to load
- * @dictSize: size of dictionary
- *
- * Use this function to load a static dictionary into LZ4HC_stream.
- * Any previous data will be forgotten, only 'dictionary'
- * will remain in memory.
- * Loading a size of 0 is allowed.
- *
- * Return : dictionary size, in bytes (necessarily <= 64 KB)
- */
-int	LZ4_loadDictHC(LZ4_streamHC_t *streamHCPtr, const char *dictionary,
-	int dictSize);
-
-/**
- * LZ4_compress_HC_continue() - Compress 'src' using data from previously
- *	compressed blocks as a dictionary using the HC algorithm
- * @streamHCPtr: Pointer to the previous 'LZ4_streamHC_t' structure
- * @src: source address of the original data
- * @dst: output buffer address of the compressed data,
- *	which must be already allocated
- * @srcSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
- * @maxDstSize: full or partial size of buffer 'dest'
- *	which must be already allocated
- *
- * These functions compress data in successive blocks of any size, using
- * previous blocks as dictionary. One key assumption is that previous
- * blocks (up to 64 KB) remain read-accessible while
- * compressing next blocks. There is an exception for ring buffers,
- * which can be smaller than 64 KB.
- * Ring buffers scenario is automatically detected and handled by
- * LZ4_compress_HC_continue().
- * Before starting compression, state must be properly initialized,
- * using LZ4_resetStreamHC().
- * A first "fictional block" can then be designated as
- * initial dictionary, using LZ4_loadDictHC() (Optional).
- * Then, use LZ4_compress_HC_continue()
- * to compress each successive block. Previous memory blocks
- * (including initial dictionary when present) must remain accessible
- * and unmodified during compression.
- * 'dst' buffer should be sized to handle worst case scenarios, using
- *  LZ4_compressBound(), to ensure operation success.
- *  If, for any reason, previous data blocks can't be preserved unmodified
- *  in memory during next compression block,
- *  you must save it to a safer memory space, using LZ4_saveDictHC().
- * Return value of LZ4_saveDictHC() is the size of dictionary
- * effectively saved into 'safeBuffer'.
- *
- * Return: Number of bytes written into buffer 'dst'  or 0 if compression fails
- */
-int LZ4_compress_HC_continue(LZ4_streamHC_t *streamHCPtr, const char *src,
-	char *dst, int srcSize, int maxDstSize);
-
-/**
- * LZ4_saveDictHC() - Save static dictionary from LZ4HC_stream
- * @streamHCPtr: pointer to the 'LZ4HC_stream_t' structure
- * @safeBuffer: buffer to save dictionary to, must be already allocated
- * @maxDictSize: size of 'safeBuffer'
- *
- * If previously compressed data block is not guaranteed
- * to remain available at its memory location,
- * save it into a safer place (char *safeBuffer).
- * Note : you don't need to call LZ4_loadDictHC() afterwards,
- * dictionary is immediately usable, you can therefore call
- * LZ4_compress_HC_continue().
- *
- * Return : saved dictionary size in bytes (necessarily <= maxDictSize),
- *	or 0 if error.
- */
-int LZ4_saveDictHC(LZ4_streamHC_t *streamHCPtr, char *safeBuffer,
-	int maxDictSize);
-
-/*-*********************************************
- *	Streaming Compression Functions
- ***********************************************/
-
-/**
- * LZ4_resetStream() - Init an allocated 'LZ4_stream_t' structure
- * @LZ4_stream: pointer to the 'LZ4_stream_t' structure
- *
- * An LZ4_stream_t structure can be allocated once
- * and re-used multiple times.
- * Use this function to init an allocated `LZ4_stream_t` structure
- * and start a new compression.
- */
-void LZ4_resetStream(LZ4_stream_t *LZ4_stream);
-
-/**
- * LZ4_loadDict() - Load a static dictionary into LZ4_stream
- * @streamPtr: pointer to the LZ4_stream_t
- * @dictionary: dictionary to load
- * @dictSize: size of dictionary
- *
- * Use this function to load a static dictionary into LZ4_stream.
- * Any previous data will be forgotten, only 'dictionary'
- * will remain in memory.
- * Loading a size of 0 is allowed.
- *
- * Return : dictionary size, in bytes (necessarily <= 64 KB)
- */
-int LZ4_loadDict(LZ4_stream_t *streamPtr, const char *dictionary,
-	int dictSize);
-
-/**
- * LZ4_saveDict() - Save static dictionary from LZ4_stream
- * @streamPtr: pointer to the 'LZ4_stream_t' structure
- * @safeBuffer: buffer to save dictionary to, must be already allocated
- * @dictSize: size of 'safeBuffer'
- *
- * If previously compressed data block is not guaranteed
- * to remain available at its memory location,
- * save it into a safer place (char *safeBuffer).
- * Note : you don't need to call LZ4_loadDict() afterwards,
- * dictionary is immediately usable, you can therefore call
- * LZ4_compress_fast_continue().
- *
- * Return : saved dictionary size in bytes (necessarily <= dictSize),
- *	or 0 if error.
- */
-int LZ4_saveDict(LZ4_stream_t *streamPtr, char *safeBuffer, int dictSize);
-
-/**
- * LZ4_compress_fast_continue() - Compress 'src' using data from previously
- *	compressed blocks as a dictionary
- * @streamPtr: Pointer to the previous 'LZ4_stream_t' structure
- * @src: source address of the original data
- * @dst: output buffer address of the compressed data,
- *	which must be already allocated
- * @srcSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
- * @maxDstSize: full or partial size of buffer 'dest'
- *	which must be already allocated
- * @acceleration: acceleration factor
- *
- * Compress buffer content 'src', using data from previously compressed blocks
- * as dictionary to improve compression ratio.
- * Important : Previous data blocks are assumed to still
- * be present and unmodified !
- * If maxDstSize >= LZ4_compressBound(srcSize),
- * compression is guaranteed to succeed, and runs faster.
- *
- * Return: Number of bytes written into buffer 'dst'  or 0 if compression fails
- */
-int LZ4_compress_fast_continue(LZ4_stream_t *streamPtr, const char *src,
-	char *dst, int srcSize, int maxDstSize, int acceleration);
-
-/**
- * LZ4_setStreamDecode() - Instruct where to find dictionary
- * @LZ4_streamDecode: the 'LZ4_streamDecode_t' structure
- * @dictionary: dictionary to use
- * @dictSize: size of dictionary
- *
- * Use this function to instruct where to find the dictionary.
- *	Setting a size of 0 is allowed (same effect as reset).
- *
- * Return: 1 if OK, 0 if error
- */
-int LZ4_setStreamDecode(LZ4_streamDecode_t *LZ4_streamDecode,
-	const char *dictionary, int dictSize);
-
-/**
- * LZ4_decompress_safe_continue() - Decompress blocks in streaming mode
- * @LZ4_streamDecode: the 'LZ4_streamDecode_t' structure
- * @source: source address of the compressed data
- * @dest: output buffer address of the uncompressed data
- *	which must be already allocated
- * @compressedSize: is the precise full size of the compressed block
- * @maxDecompressedSize: is the size of 'dest' buffer
- *
- * This decoding function allows decompression of multiple blocks
- * in "streaming" mode.
- * Previously decoded blocks *must* remain available at the memory position
- * where they were decoded (up to 64 KB)
- * In the case of a ring buffers, decoding buffer must be either :
- *    - Exactly same size as encoding buffer, with same update rule
- *      (block boundaries at same positions) In which case,
- *      the decoding & encoding ring buffer can have any size,
- *      including very small ones ( < 64 KB).
- *    - Larger than encoding buffer, by a minimum of maxBlockSize more bytes.
- *      maxBlockSize is implementation dependent.
- *      It's the maximum size you intend to compress into a single block.
- *      In which case, encoding and decoding buffers do not need
- *      to be synchronized, and encoding ring buffer can have any size,
- *      including small ones ( < 64 KB).
- *    - _At least_ 64 KB + 8 bytes + maxBlockSize.
- *      In which case, encoding and decoding buffers do not need to be
- *      synchronized, and encoding ring buffer can have any size,
- *      including larger than decoding buffer. W
- * Whenever these conditions are not possible, save the last 64KB of decoded
- * data into a safe buffer, and indicate where it is saved
- * using LZ4_setStreamDecode()
- *
- * Return: number of bytes decompressed into destination buffer
- *	(necessarily <= maxDecompressedSize)
- *	or a negative result in case of error
- */
-int LZ4_decompress_safe_continue(LZ4_streamDecode_t *LZ4_streamDecode,
-	const char *source, char *dest, int compressedSize,
-	int maxDecompressedSize);
-
-/**
- * LZ4_decompress_fast_continue() - Decompress blocks in streaming mode
- * @LZ4_streamDecode: the 'LZ4_streamDecode_t' structure
- * @source: source address of the compressed data
- * @dest: output buffer address of the uncompressed data
- *	which must be already allocated with 'originalSize' bytes
- * @originalSize: is the original and therefore uncompressed size
- *
- * This decoding function allows decompression of multiple blocks
- * in "streaming" mode.
- * Previously decoded blocks *must* remain available at the memory position
- * where they were decoded (up to 64 KB)
- * In the case of a ring buffers, decoding buffer must be either :
- *    - Exactly same size as encoding buffer, with same update rule
- *      (block boundaries at same positions) In which case,
- *      the decoding & encoding ring buffer can have any size,
- *      including very small ones ( < 64 KB).
- *    - Larger than encoding buffer, by a minimum of maxBlockSize more bytes.
- *      maxBlockSize is implementation dependent.
- *      It's the maximum size you intend to compress into a single block.
- *      In which case, encoding and decoding buffers do not need
- *      to be synchronized, and encoding ring buffer can have any size,
- *      including small ones ( < 64 KB).
- *    - _At least_ 64 KB + 8 bytes + maxBlockSize.
- *      In which case, encoding and decoding buffers do not need to be
- *      synchronized, and encoding ring buffer can have any size,
- *      including larger than decoding buffer. W
- * Whenever these conditions are not possible, save the last 64KB of decoded
- * data into a safe buffer, and indicate where it is saved
- * using LZ4_setStreamDecode()
- *
- * Return: number of bytes decompressed into destination buffer
- *	(necessarily <= maxDecompressedSize)
- *	or a negative result in case of error
- */
-int LZ4_decompress_fast_continue(LZ4_streamDecode_t *LZ4_streamDecode,
-	const char *source, char *dest, int originalSize);
-
-/**
- * LZ4_decompress_safe_usingDict() - Same as LZ4_setStreamDecode()
- *	followed by LZ4_decompress_safe_continue()
- * @source: source address of the compressed data
- * @dest: output buffer address of the uncompressed data
- *	which must be already allocated
- * @compressedSize: is the precise full size of the compressed block
- * @maxDecompressedSize: is the size of 'dest' buffer
- * @dictStart: pointer to the start of the dictionary in memory
- * @dictSize: size of dictionary
- *
- * This decoding function works the same as
- * a combination of LZ4_setStreamDecode() followed by
- * LZ4_decompress_safe_continue()
- * It is stand-alone, and doesn't need an LZ4_streamDecode_t structure.
- *
- * Return: number of bytes decompressed into destination buffer
- *	(necessarily <= maxDecompressedSize)
- *	or a negative result in case of error
- */
-int LZ4_decompress_safe_usingDict(const char *source, char *dest,
-	int compressedSize, int maxDecompressedSize, const char *dictStart,
-	int dictSize);
-
-/**
- * LZ4_decompress_fast_usingDict() - Same as LZ4_setStreamDecode()
- *	followed by LZ4_decompress_fast_continue()
- * @source: source address of the compressed data
- * @dest: output buffer address of the uncompressed data
- *	which must be already allocated with 'originalSize' bytes
- * @originalSize: is the original and therefore uncompressed size
- * @dictStart: pointer to the start of the dictionary in memory
- * @dictSize: size of dictionary
- *
- * This decoding function works the same as
- * a combination of LZ4_setStreamDecode() followed by
- * LZ4_decompress_fast_continue()
- * It is stand-alone, and doesn't need an LZ4_streamDecode_t structure.
- *
- * Return: number of bytes decompressed into destination buffer
- *	(necessarily <= maxDecompressedSize)
- *	or a negative result in case of error
- */
-int LZ4_decompress_fast_usingDict(const char *source, char *dest,
-	int originalSize, const char *dictStart, int dictSize);
+#define LZ4HC_MIN_CLEVEL	LZ4HC_CLEVEL_MIN
+#define LZ4HC_DEFAULT_CLEVEL	LZ4HC_CLEVEL_DEFAULT
+#define LZ4HC_MAX_CLEVEL	LZ4HC_CLEVEL_MAX
 
 #endif
Index: lib/lz4/Makefile
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/lz4/Makefile b/lib/lz4/Makefile
--- a/lib/lz4/Makefile	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ b/lib/lz4/Makefile	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -1,6 +1,7 @@
-# SPDX-License-Identifier: GPL-2.0-only
-ccflags-y += -O3
+ccflags-y += -O3 \
+    -DLZ4_FREESTANDING=1 \
+    -DLZ4_FAST_DEC_LOOP=1
 
-obj-$(CONFIG_LZ4_COMPRESS) += lz4_compress.o
-obj-$(CONFIG_LZ4HC_COMPRESS) += lz4hc_compress.o
-obj-$(CONFIG_LZ4_DECOMPRESS) += lz4_decompress.o
+obj-y += lz4.o lz4hc.o
+
+obj-$(CONFIG_ARM64) += $(addprefix lz4armv8/, lz4accel.o lz4armv8.o)
Index: lib/lz4/lz4.c
===================================================================
diff --git a/lib/lz4/lz4.c b/lib/lz4/lz4.c
new file mode 100644
--- /dev/null	(revision b2497e4243461a835c25469028cd355bfc2e993f)
+++ b/lib/lz4/lz4.c	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -0,0 +1,3484 @@
+/*
+   LZ4 - Fast LZ compression algorithm
+   Copyright (C) 2011-2023, Yann Collet.
+
+   BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)
+
+   Redistribution and use in source and binary forms, with or without
+   modification, are permitted provided that the following conditions are
+   met:
+
+       * Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+       * Redistributions in binary form must reproduce the above
+   copyright notice, this list of conditions and the following disclaimer
+   in the documentation and/or other materials provided with the
+   distribution.
+
+   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+   You can contact the author at :
+    - LZ4 homepage : http://www.lz4.org
+    - LZ4 source repository : https://github.com/lz4/lz4
+*/
+
+/*-************************************
+*  Tuning parameters
+**************************************/
+/*
+ * LZ4_HEAPMODE :
+ * Select how stateless compression functions like `LZ4_compress_default()`
+ * allocate memory for their hash table,
+ * in memory stack (0:default, fastest), or in memory heap (1:requires malloc()).
+ */
+#ifndef LZ4_HEAPMODE
+#define LZ4_HEAPMODE 1
+#endif
+
+/*-************************************
+*  CPU Feature Detection
+**************************************/
+/* LZ4_FORCE_MEMORY_ACCESS
+ * By default, access to unaligned memory is controlled by `memcpy()`, which is safe and portable.
+ * Unfortunately, on some target/compiler combinations, the generated assembly is sub-optimal.
+ * The below switch allow to select different access method for improved performance.
+ * Method 0 (default) : use `memcpy()`. Safe and portable.
+ * Method 1 : `__packed` statement. It depends on compiler extension (ie, not portable).
+ *            This method is safe if your compiler supports it, and *generally* as fast or faster than `memcpy`.
+ * Method 2 : direct access. This method is portable but violate C standard.
+ *            It can generate buggy code on targets which assembly generation depends on alignment.
+ *            But in some circumstances, it's the only known way to get the most performance (ie GCC + ARMv6)
+ * See https://fastcompression.blogspot.fr/2015/08/accessing-unaligned-memory.html for details.
+ * Prefer these methods in priority order (0 > 1 > 2)
+ */
+#ifndef LZ4_FORCE_MEMORY_ACCESS /* can be defined externally */
+#if defined(__GNUC__) &&                                                       \
+	(defined(__ARM_ARCH_6__) || defined(__ARM_ARCH_6J__) ||                \
+	 defined(__ARM_ARCH_6K__) || defined(__ARM_ARCH_6Z__) ||               \
+	 defined(__ARM_ARCH_6ZK__) || defined(__ARM_ARCH_6T2__))
+#define LZ4_FORCE_MEMORY_ACCESS 2
+#elif (defined(__INTEL_COMPILER) && !defined(_WIN32)) || defined(__GNUC__) ||  \
+	defined(_MSC_VER)
+#define LZ4_FORCE_MEMORY_ACCESS 1
+#endif
+#endif
+
+/*
+ * LZ4_FORCE_SW_BITCOUNT
+ * Define this parameter if your target system or compiler does not support hardware bit count
+ */
+#if defined(_MSC_VER) &&                                                       \
+	defined(_WIN32_WCE) /* Visual Studio for WinCE doesn't support Hardware bit count */
+#undef LZ4_FORCE_SW_BITCOUNT /* avoid double def */
+#define LZ4_FORCE_SW_BITCOUNT
+#endif
+
+/*-************************************
+*  Dependency
+**************************************/
+/*
+ * LZ4_SRC_INCLUDED:
+ * Amalgamation flag, whether lz4.c is included
+ */
+#ifndef LZ4_SRC_INCLUDED
+#define LZ4_SRC_INCLUDED 1
+#endif
+
+#ifndef LZ4_DISABLE_DEPRECATE_WARNINGS
+#define LZ4_DISABLE_DEPRECATE_WARNINGS /* due to LZ4_decompress_safe_withPrefix64k */
+#endif
+
+#ifndef LZ4_STATIC_LINKING_ONLY
+#define LZ4_STATIC_LINKING_ONLY
+#endif
+#include "lz4.h"
+/* see also "memory routines" below */
+
+/*-************************************
+*  Compiler Options
+**************************************/
+#if defined(_MSC_VER) && (_MSC_VER >= 1400) /* Visual Studio 2005+ */
+#include <intrin.h> /* only present in VS2005+ */
+#pragma warning(                                                               \
+	disable : 4127) /* disable: C4127: conditional expression is constant */
+#pragma warning(                                                               \
+	disable : 6237) /* disable: C6237: conditional expression is always 0 */
+#pragma warning(                                                               \
+	disable : 6239) /* disable: C6239: (<non-zero constant> && <expression>) always evaluates to the result of <expression> */
+#pragma warning(                                                               \
+	disable : 6240) /* disable: C6240: (<expression> && <non-zero constant>) always evaluates to the result of <expression> */
+#pragma warning(                                                               \
+	disable : 6326) /* disable: C6326: Potential comparison of a constant with another constant */
+#endif /* _MSC_VER */
+
+#ifndef LZ4_FORCE_INLINE
+#if defined(_MSC_VER) && !defined(__clang__) /* MSVC */
+#define LZ4_FORCE_INLINE static __forceinline
+#else
+#if defined(__cplusplus) ||                                                    \
+	defined(__STDC_VERSION__) && __STDC_VERSION__ >= 199901L /* C99 */
+#if defined(__GNUC__) || defined(__clang__)
+#define LZ4_FORCE_INLINE static inline __attribute__((always_inline))
+#else
+#define LZ4_FORCE_INLINE static inline
+#endif
+#else
+#define LZ4_FORCE_INLINE static
+#endif /* __STDC_VERSION__ */
+#endif /* _MSC_VER */
+#endif /* LZ4_FORCE_INLINE */
+
+/* LZ4_FORCE_O2 and LZ4_FORCE_INLINE
+ * gcc on ppc64le generates an unrolled SIMDized loop for LZ4_wildCopy8,
+ * together with a simple 8-byte copy loop as a fall-back path.
+ * However, this optimization hurts the decompression speed by >30%,
+ * because the execution does not go to the optimized loop
+ * for typical compressible data, and all of the preamble checks
+ * before going to the fall-back path become useless overhead.
+ * This optimization happens only with the -O3 flag, and -O2 generates
+ * a simple 8-byte copy loop.
+ * With gcc on ppc64le, all of the LZ4_decompress_* and LZ4_wildCopy8
+ * functions are annotated with __attribute__((optimize("O2"))),
+ * and also LZ4_wildCopy8 is forcibly inlined, so that the O2 attribute
+ * of LZ4_wildCopy8 does not affect the compression speed.
+ */
+#if defined(__PPC64__) && defined(__LITTLE_ENDIAN__) && defined(__GNUC__) &&   \
+	!defined(__clang__)
+#define LZ4_FORCE_O2 __attribute__((optimize("O2")))
+#undef LZ4_FORCE_INLINE
+#define LZ4_FORCE_INLINE                                                       \
+	static __inline __attribute__((optimize("O2"), always_inline))
+#else
+#define LZ4_FORCE_O2
+#endif
+
+#if (defined(__GNUC__) && (__GNUC__ >= 3)) ||                                  \
+	(defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 800)) ||            \
+	defined(__clang__)
+#define expect(expr, value) (__builtin_expect((expr), (value)))
+#else
+#define expect(expr, value) (expr)
+#endif
+
+/* Should the alignment test prove unreliable, for some reason,
+ * it can be disabled by setting LZ4_ALIGN_TEST to 0 */
+#ifndef LZ4_ALIGN_TEST /* can be externally provided */
+#define LZ4_ALIGN_TEST 1
+#endif
+
+/*-************************************
+*  Memory routines
+**************************************/
+
+#if !LZ4_FREESTANDING
+#include <linux/string.h> /* memset, memcpy */
+#endif
+#if !defined(LZ4_memset)
+#define LZ4_memset(p, v, s) memset((p), (v), (s))
+#endif
+#define MEM_INIT(p, v, s) LZ4_memset((p), (v), (s))
+
+/*-************************************
+*  Common Constants
+**************************************/
+#define MINMATCH 4
+
+#define WILDCOPYLENGTH 8
+#define LASTLITERALS 5 /* see ../doc/lz4_Block_format.md#parsing-restrictions */
+#define MFLIMIT 12 /* see ../doc/lz4_Block_format.md#parsing-restrictions */
+#define MATCH_SAFEGUARD_DISTANCE                                               \
+	((2 * WILDCOPYLENGTH) -                                                \
+	 MINMATCH) /* ensure it's possible to write 2 x wildcopyLength without overflowing output buffer */
+#define FASTLOOP_SAFE_DISTANCE 64
+static const int LZ4_minLength = (MFLIMIT + 1);
+
+#define KB *(1 << 10)
+#define MB *(1 << 20)
+#define GB *(1U << 30)
+
+#define LZ4_DISTANCE_ABSOLUTE_MAX 65535
+#if (LZ4_DISTANCE_MAX >                                                        \
+     LZ4_DISTANCE_ABSOLUTE_MAX) /* max supported by LZ4 format */
+#error "LZ4_DISTANCE_MAX is too big : must be <= 65535"
+#endif
+
+#define ML_BITS 4
+#define ML_MASK ((1U << ML_BITS) - 1)
+#define RUN_BITS (8 - ML_BITS)
+#define RUN_MASK ((1U << RUN_BITS) - 1)
+
+/*-************************************
+*  Error detection
+**************************************/
+#if defined(LZ4_DEBUG) && (LZ4_DEBUG >= 1)
+#include <assert.h>
+#else
+#ifndef assert
+#define assert(condition) ((void)0)
+#endif
+#endif
+
+#define LZ4_STATIC_ASSERT(c)                                                   \
+	{                                                                      \
+		enum { LZ4_static_assert = 1 / (int)(!!(c)) };                 \
+	} /* use after variable declarations */
+
+#if defined(LZ4_DEBUG) && (LZ4_DEBUG >= 2)
+#include <stdio.h>
+static int g_debuglog_enable = 1;
+#define DEBUGLOG(l, ...)                                                       \
+	{                                                                      \
+		if ((g_debuglog_enable) && (l <= LZ4_DEBUG)) {                 \
+			fprintf(stderr, __FILE__ " %i: ", __LINE__);           \
+			fprintf(stderr, __VA_ARGS__);                          \
+			fprintf(stderr, " \n");                                \
+		}                                                              \
+	}
+#else
+#define DEBUGLOG(l, ...)                                                       \
+	{                                                                      \
+	} /* disabled */
+#endif
+
+static int LZ4_isAligned(const void *ptr, size_t alignment)
+{
+	return ((size_t)ptr & (alignment - 1)) == 0;
+}
+
+/*-************************************
+*  Types
+**************************************/
+#include <linux/types.h>
+typedef uint8_t BYTE;
+typedef uint16_t U16;
+typedef uint32_t U32;
+typedef int32_t S32;
+typedef uint64_t U64;
+typedef uintptr_t uptrval;
+
+#if defined(__x86_64__)
+typedef U64 reg_t; /* 64-bits in x32 mode */
+#else
+typedef size_t reg_t; /* 32-bits in x32 mode */
+#endif
+
+typedef enum {
+	notLimited = 0,
+	limitedOutput = 1,
+	fillOutput = 2
+} limitedOutput_directive;
+
+static unsigned LZ4_isLittleEndian(void)
+{
+	const union {
+		U32 u;
+		BYTE c[4];
+	} one = { 1 }; /* don't use static : performance detrimental */
+	return one.c[0];
+}
+
+#if defined(__GNUC__) || defined(__INTEL_COMPILER)
+#define LZ4_PACK(__Declaration__) __Declaration__ __attribute__((__packed__))
+#elif defined(_MSC_VER)
+#define LZ4_PACK(__Declaration__)                                              \
+	__pragma(pack(push, 1)) __Declaration__ __pragma(pack(pop))
+#endif
+
+#if defined(LZ4_FORCE_MEMORY_ACCESS) && (LZ4_FORCE_MEMORY_ACCESS == 2)
+/* lie to the compiler about data alignment; use with caution */
+
+static U16 LZ4_read16(const void *memPtr)
+{
+	return *(const U16 *)memPtr;
+}
+static U32 LZ4_read32(const void *memPtr)
+{
+	return *(const U32 *)memPtr;
+}
+static reg_t LZ4_read_ARCH(const void *memPtr)
+{
+	return *(const reg_t *)memPtr;
+}
+
+static void LZ4_write16(void *memPtr, U16 value)
+{
+	*(U16 *)memPtr = value;
+}
+static void LZ4_write32(void *memPtr, U32 value)
+{
+	*(U32 *)memPtr = value;
+}
+
+#elif defined(LZ4_FORCE_MEMORY_ACCESS) && (LZ4_FORCE_MEMORY_ACCESS == 1)
+
+/* __pack instructions are safer, but compiler specific, hence potentially problematic for some compilers */
+/* currently only defined for gcc and icc */
+LZ4_PACK(typedef struct { U16 u16; }) LZ4_unalign16;
+LZ4_PACK(typedef struct { U32 u32; }) LZ4_unalign32;
+LZ4_PACK(typedef struct { reg_t uArch; }) LZ4_unalignST;
+
+static U16 LZ4_read16(const void *ptr)
+{
+	return ((const LZ4_unalign16 *)ptr)->u16;
+}
+static U32 LZ4_read32(const void *ptr)
+{
+	return ((const LZ4_unalign32 *)ptr)->u32;
+}
+static reg_t LZ4_read_ARCH(const void *ptr)
+{
+	return ((const LZ4_unalignST *)ptr)->uArch;
+}
+
+static void LZ4_write16(void *memPtr, U16 value)
+{
+	((LZ4_unalign16 *)memPtr)->u16 = value;
+}
+static void LZ4_write32(void *memPtr, U32 value)
+{
+	((LZ4_unalign32 *)memPtr)->u32 = value;
+}
+
+#else /* safe and portable access using memcpy() */
+
+static U16 LZ4_read16(const void *memPtr)
+{
+	U16 val;
+	LZ4_memcpy(&val, memPtr, sizeof(val));
+	return val;
+}
+
+static U32 LZ4_read32(const void *memPtr)
+{
+	U32 val;
+	LZ4_memcpy(&val, memPtr, sizeof(val));
+	return val;
+}
+
+static reg_t LZ4_read_ARCH(const void *memPtr)
+{
+	reg_t val;
+	LZ4_memcpy(&val, memPtr, sizeof(val));
+	return val;
+}
+
+static void LZ4_write16(void *memPtr, U16 value)
+{
+	LZ4_memcpy(memPtr, &value, sizeof(value));
+}
+
+static void LZ4_write32(void *memPtr, U32 value)
+{
+	LZ4_memcpy(memPtr, &value, sizeof(value));
+}
+
+#endif /* LZ4_FORCE_MEMORY_ACCESS */
+
+static U16 LZ4_readLE16(const void *memPtr)
+{
+	if (LZ4_isLittleEndian()) {
+		return LZ4_read16(memPtr);
+	} else {
+		const BYTE *p = (const BYTE *)memPtr;
+		return (U16)((U16)p[0] | (p[1] << 8));
+	}
+}
+
+#ifdef LZ4_STATIC_LINKING_ONLY_ENDIANNESS_INDEPENDENT_OUTPUT
+static U32 LZ4_readLE32(const void *memPtr)
+{
+	if (LZ4_isLittleEndian()) {
+		return LZ4_read32(memPtr);
+	} else {
+		const BYTE *p = (const BYTE *)memPtr;
+		return (U32)p[0] | (p[1] << 8) | (p[2] << 16) | (p[3] << 24);
+	}
+}
+#endif
+
+static void LZ4_writeLE16(void *memPtr, U16 value)
+{
+	if (LZ4_isLittleEndian()) {
+		LZ4_write16(memPtr, value);
+	} else {
+		BYTE *p = (BYTE *)memPtr;
+		p[0] = (BYTE)value;
+		p[1] = (BYTE)(value >> 8);
+	}
+}
+
+/* customized variant of memcpy, which can overwrite up to 8 bytes beyond dstEnd */
+LZ4_FORCE_INLINE
+void LZ4_wildCopy8(void *dstPtr, const void *srcPtr, void *dstEnd)
+{
+	BYTE *d = (BYTE *)dstPtr;
+	const BYTE *s = (const BYTE *)srcPtr;
+	BYTE *const e = (BYTE *)dstEnd;
+
+	do {
+		LZ4_memcpy(d, s, 8);
+		d += 8;
+		s += 8;
+	} while (d < e);
+}
+
+static const unsigned inc32table[8] = { 0, 1, 2, 1, 0, 4, 4, 4 };
+static const int dec64table[8] = { 0, 0, 0, -1, -4, 1, 2, 3 };
+
+#ifndef LZ4_FAST_DEC_LOOP
+#if defined __i386__ || defined _M_IX86 || defined __x86_64__ || defined _M_X64
+#define LZ4_FAST_DEC_LOOP 1
+#elif defined(__aarch64__) && defined(__APPLE__)
+#define LZ4_FAST_DEC_LOOP 1
+#elif defined(__aarch64__) && !defined(__clang__)
+/* On non-Apple aarch64, we disable this optimization for clang because
+      * on certain mobile chipsets, performance is reduced with clang. For
+      * more information refer to https://github.com/lz4/lz4/pull/707 */
+#define LZ4_FAST_DEC_LOOP 1
+#else
+#define LZ4_FAST_DEC_LOOP 0
+#endif
+#endif
+
+#if LZ4_FAST_DEC_LOOP
+
+LZ4_FORCE_INLINE void LZ4_memcpy_using_offset_base(BYTE *dstPtr,
+						   const BYTE *srcPtr,
+						   BYTE *dstEnd,
+						   const size_t offset)
+{
+	assert(srcPtr + offset == dstPtr);
+	if (offset < 8) {
+		LZ4_write32(dstPtr,
+			    0); /* silence an msan warning when offset==0 */
+		dstPtr[0] = srcPtr[0];
+		dstPtr[1] = srcPtr[1];
+		dstPtr[2] = srcPtr[2];
+		dstPtr[3] = srcPtr[3];
+		srcPtr += inc32table[offset];
+		LZ4_memcpy(dstPtr + 4, srcPtr, 4);
+		srcPtr -= dec64table[offset];
+		dstPtr += 8;
+	} else {
+		LZ4_memcpy(dstPtr, srcPtr, 8);
+		dstPtr += 8;
+		srcPtr += 8;
+	}
+
+	LZ4_wildCopy8(dstPtr, srcPtr, dstEnd);
+}
+
+/* customized variant of memcpy, which can overwrite up to 32 bytes beyond dstEnd
+ * this version copies two times 16 bytes (instead of one time 32 bytes)
+ * because it must be compatible with offsets >= 16. */
+LZ4_FORCE_INLINE void LZ4_wildCopy32(void *dstPtr, const void *srcPtr,
+				     void *dstEnd)
+{
+	BYTE *d = (BYTE *)dstPtr;
+	const BYTE *s = (const BYTE *)srcPtr;
+	BYTE *const e = (BYTE *)dstEnd;
+
+	do {
+		LZ4_memcpy(d, s, 16);
+		LZ4_memcpy(d + 16, s + 16, 16);
+		d += 32;
+		s += 32;
+	} while (d < e);
+}
+
+/* LZ4_memcpy_using_offset()  presumes :
+ * - dstEnd >= dstPtr + MINMATCH
+ * - there is at least 12 bytes available to write after dstEnd */
+LZ4_FORCE_INLINE void LZ4_memcpy_using_offset(BYTE *dstPtr, const BYTE *srcPtr,
+					      BYTE *dstEnd, const size_t offset)
+{
+	BYTE v[8];
+
+	assert(dstEnd >= dstPtr + MINMATCH);
+
+	switch (offset) {
+	case 1:
+		MEM_INIT(v, *srcPtr, 8);
+		break;
+	case 2:
+		LZ4_memcpy(v, srcPtr, 2);
+		LZ4_memcpy(&v[2], srcPtr, 2);
+#if defined(_MSC_VER) && (_MSC_VER <= 1937) /* MSVC 2022 ver 17.7 or earlier */
+#pragma warning(push)
+#pragma warning(                                                               \
+	disable : 6385) /* warning C6385: Reading invalid data from 'v'. */
+#endif
+		LZ4_memcpy(&v[4], v, 4);
+#if defined(_MSC_VER) && (_MSC_VER <= 1937) /* MSVC 2022 ver 17.7 or earlier */
+#pragma warning(pop)
+#endif
+		break;
+	case 4:
+		LZ4_memcpy(v, srcPtr, 4);
+		LZ4_memcpy(&v[4], srcPtr, 4);
+		break;
+	default:
+		LZ4_memcpy_using_offset_base(dstPtr, srcPtr, dstEnd, offset);
+		return;
+	}
+
+	LZ4_memcpy(dstPtr, v, 8);
+	dstPtr += 8;
+	while (dstPtr < dstEnd) {
+		LZ4_memcpy(dstPtr, v, 8);
+		dstPtr += 8;
+	}
+}
+#endif
+
+/*-************************************
+*  Common functions
+**************************************/
+static unsigned LZ4_NbCommonBytes(reg_t val)
+{
+	assert(val != 0);
+	if (LZ4_isLittleEndian()) {
+		if (sizeof(val) == 8) {
+#if defined(_MSC_VER) && (_MSC_VER >= 1800) &&                                 \
+	(defined(_M_AMD64) && !defined(_M_ARM64EC)) &&                         \
+	!defined(LZ4_FORCE_SW_BITCOUNT)
+/*-*************************************************************************************************
+* ARM64EC is a Microsoft-designed ARM64 ABI compatible with AMD64 applications on ARM64 Windows 11.
+* The ARM64EC ABI does not support AVX/AVX2/AVX512 instructions, nor their relevant intrinsics
+* including _tzcnt_u64. Therefore, we need to neuter the _tzcnt_u64 code path for ARM64EC.
+****************************************************************************************************/
+#if defined(__clang__) && (__clang_major__ < 10)
+			/* Avoid undefined clang-cl intrinsics issue.
+             * See https://github.com/lz4/lz4/pull/1017 for details. */
+			return (unsigned)__builtin_ia32_tzcnt_u64(val) >> 3;
+#else
+			/* x64 CPUS without BMI support interpret `TZCNT` as `REP BSF` */
+			return (unsigned)_tzcnt_u64(val) >> 3;
+#endif
+#elif defined(_MSC_VER) && defined(_WIN64) && !defined(LZ4_FORCE_SW_BITCOUNT)
+			unsigned long r = 0;
+			_BitScanForward64(&r, (U64)val);
+			return (unsigned)r >> 3;
+#elif (defined(__clang__) ||                                                   \
+       (defined(__GNUC__) &&                                                   \
+	((__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 4))))) &&    \
+	!defined(LZ4_FORCE_SW_BITCOUNT)
+			return (unsigned)__builtin_ctzll((U64)val) >> 3;
+#else
+			const U64 m = 0x0101010101010101ULL;
+			val ^= val - 1;
+			return (unsigned)(((U64)((val & (m - 1)) * m)) >> 56);
+#endif
+		} else /* 32 bits */ {
+#if defined(_MSC_VER) && (_MSC_VER >= 1400) && !defined(LZ4_FORCE_SW_BITCOUNT)
+			unsigned long r;
+			_BitScanForward(&r, (U32)val);
+			return (unsigned)r >> 3;
+#elif (defined(__clang__) ||                                                   \
+       (defined(__GNUC__) &&                                                   \
+	((__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 4))))) &&    \
+	!defined(__TINYC__) && !defined(LZ4_FORCE_SW_BITCOUNT)
+			return (unsigned)__builtin_ctz((U32)val) >> 3;
+#else
+			const U32 m = 0x01010101;
+			return (unsigned)((((val - 1) ^ val) & (m - 1)) * m) >>
+			       24;
+#endif
+		}
+	} else /* Big Endian CPU */ {
+		if (sizeof(val) == 8) {
+#if (defined(__clang__) ||                                                     \
+     (defined(__GNUC__) &&                                                     \
+      ((__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 4))))) &&      \
+	!defined(__TINYC__) && !defined(LZ4_FORCE_SW_BITCOUNT)
+			return (unsigned)__builtin_clzll((U64)val) >> 3;
+#else
+#if 1
+			/* this method is probably faster,
+             * but adds a 128 bytes lookup table */
+			static const unsigned char ctz7_tab[128] = {
+				7, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				6, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+				4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+			};
+			U64 const mask = 0x0101010101010101ULL;
+			U64 const t = (((val >> 8) - mask) | val) & mask;
+			return ctz7_tab[(t * 0x0080402010080402ULL) >> 57];
+#else
+			/* this method doesn't consume memory space like the previous one,
+             * but it contains several branches,
+             * that may end up slowing execution */
+			static const U32 by32 =
+				sizeof(val) *
+				4; /* 32 on 64 bits (goal), 16 on 32 bits.
+            Just to avoid some static analyzer complaining about shift by 32 on 32-bits target.
+            Note that this code path is never triggered in 32-bits mode. */
+			unsigned r;
+			if (!(val >> by32)) {
+				r = 4;
+			} else {
+				r = 0;
+				val >>= by32;
+			}
+			if (!(val >> 16)) {
+				r += 2;
+				val >>= 8;
+			} else {
+				val >>= 24;
+			}
+			r += (!val);
+			return r;
+#endif
+#endif
+		} else /* 32 bits */ {
+#if (defined(__clang__) ||                                                     \
+     (defined(__GNUC__) &&                                                     \
+      ((__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 4))))) &&      \
+	!defined(LZ4_FORCE_SW_BITCOUNT)
+			return (unsigned)__builtin_clz((U32)val) >> 3;
+#else
+			val >>= 8;
+			val = ((((val + 0x00FFFF00) | 0x00FFFFFF) + val) |
+			       (val + 0x00FF0000)) >>
+			      24;
+			return (unsigned)val ^ 3;
+#endif
+		}
+	}
+}
+
+#define STEPSIZE sizeof(reg_t)
+LZ4_FORCE_INLINE
+unsigned LZ4_count(const BYTE *pIn, const BYTE *pMatch, const BYTE *pInLimit)
+{
+	const BYTE *const pStart = pIn;
+
+	if (likely(pIn < pInLimit - (STEPSIZE - 1))) {
+		reg_t const diff = LZ4_read_ARCH(pMatch) ^ LZ4_read_ARCH(pIn);
+		if (!diff) {
+			pIn += STEPSIZE;
+			pMatch += STEPSIZE;
+		} else {
+			return LZ4_NbCommonBytes(diff);
+		}
+	}
+
+	while (likely(pIn < pInLimit - (STEPSIZE - 1))) {
+		reg_t const diff = LZ4_read_ARCH(pMatch) ^ LZ4_read_ARCH(pIn);
+		if (!diff) {
+			pIn += STEPSIZE;
+			pMatch += STEPSIZE;
+			continue;
+		}
+		pIn += LZ4_NbCommonBytes(diff);
+		return (unsigned)(pIn - pStart);
+	}
+
+	if ((STEPSIZE == 8) && (pIn < (pInLimit - 3)) &&
+	    (LZ4_read32(pMatch) == LZ4_read32(pIn))) {
+		pIn += 4;
+		pMatch += 4;
+	}
+	if ((pIn < (pInLimit - 1)) && (LZ4_read16(pMatch) == LZ4_read16(pIn))) {
+		pIn += 2;
+		pMatch += 2;
+	}
+	if ((pIn < pInLimit) && (*pMatch == *pIn))
+		pIn++;
+	return (unsigned)(pIn - pStart);
+}
+
+#ifndef LZ4_COMMONDEFS_ONLY
+/*-************************************
+*  Local Constants
+**************************************/
+static const int LZ4_64Klimit = ((64 KB) + (MFLIMIT - 1));
+static const U32 LZ4_skipTrigger =
+	6; /* Increase this value ==> compression run slower on incompressible data */
+
+/*-************************************
+*  Local Structures and types
+**************************************/
+typedef enum { clearedTable = 0, byPtr, byU32, byU16 } tableType_t;
+
+/**
+ * This enum distinguishes several different modes of accessing previous
+ * content in the stream.
+ *
+ * - noDict        : There is no preceding content.
+ * - withPrefix64k : Table entries up to ctx->dictSize before the current blob
+ *                   blob being compressed are valid and refer to the preceding
+ *                   content (of length ctx->dictSize), which is available
+ *                   contiguously preceding in memory the content currently
+ *                   being compressed.
+ * - usingExtDict  : Like withPrefix64k, but the preceding content is somewhere
+ *                   else in memory, starting at ctx->dictionary with length
+ *                   ctx->dictSize.
+ * - usingDictCtx  : Everything concerning the preceding content is
+ *                   in a separate context, pointed to by ctx->dictCtx.
+ *                   ctx->dictionary, ctx->dictSize, and table entries
+ *                   in the current context that refer to positions
+ *                   preceding the beginning of the current compression are
+ *                   ignored. Instead, ctx->dictCtx->dictionary and ctx->dictCtx
+ *                   ->dictSize describe the location and size of the preceding
+ *                   content, and matches are found by looking in the ctx
+ *                   ->dictCtx->hashTable.
+ */
+typedef enum {
+	noDict = 0,
+	withPrefix64k,
+	usingExtDict,
+	usingDictCtx
+} dict_directive;
+typedef enum { noDictIssue = 0, dictSmall } dictIssue_directive;
+
+/*-************************************
+*  Local Utils
+**************************************/
+int LZ4_versionNumber(void)
+{
+	return LZ4_VERSION_NUMBER;
+}
+const char *LZ4_versionString(void)
+{
+	return LZ4_VERSION_STRING;
+}
+int LZ4_compressBound(int isize)
+{
+	return LZ4_COMPRESSBOUND(isize);
+}
+int LZ4_sizeofState(void)
+{
+	return sizeof(LZ4_stream_t);
+}
+
+/*-****************************************
+*  Internal Definitions, used only in Tests
+*******************************************/
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+int LZ4_compress_forceExtDict(LZ4_stream_t *LZ4_dict, const char *source,
+			      char *dest, int srcSize);
+
+int LZ4_decompress_safe_forceExtDict(const char *source, char *dest,
+				     int compressedSize, int maxOutputSize,
+				     const void *dictStart, size_t dictSize);
+int LZ4_decompress_safe_partial_forceExtDict(const char *source, char *dest,
+					     int compressedSize,
+					     int targetOutputSize,
+					     int dstCapacity,
+					     const void *dictStart,
+					     size_t dictSize);
+#if defined(__cplusplus)
+}
+#endif
+
+/*-******************************
+*  Compression functions
+********************************/
+LZ4_FORCE_INLINE U32 LZ4_hash4(U32 sequence, tableType_t const tableType)
+{
+	if (tableType == byU16)
+		return ((sequence * 2654435761U) >>
+			((MINMATCH * 8) - (LZ4_HASHLOG + 1)));
+	else
+		return ((sequence * 2654435761U) >>
+			((MINMATCH * 8) - LZ4_HASHLOG));
+}
+
+LZ4_FORCE_INLINE U32 LZ4_hash5(U64 sequence, tableType_t const tableType)
+{
+	const U32 hashLog =
+		(tableType == byU16) ? LZ4_HASHLOG + 1 : LZ4_HASHLOG;
+	if (LZ4_isLittleEndian()) {
+		const U64 prime5bytes = 889523592379ULL;
+		return (U32)(((sequence << 24) * prime5bytes) >>
+			     (64 - hashLog));
+	} else {
+		const U64 prime8bytes = 11400714785074694791ULL;
+		return (U32)(((sequence >> 24) * prime8bytes) >>
+			     (64 - hashLog));
+	}
+}
+
+LZ4_FORCE_INLINE U32 LZ4_hashPosition(const void *const p,
+				      tableType_t const tableType)
+{
+	if ((sizeof(reg_t) == 8) && (tableType != byU16))
+		return LZ4_hash5(LZ4_read_ARCH(p), tableType);
+
+#ifdef LZ4_STATIC_LINKING_ONLY_ENDIANNESS_INDEPENDENT_OUTPUT
+	return LZ4_hash4(LZ4_readLE32(p), tableType);
+#else
+	return LZ4_hash4(LZ4_read32(p), tableType);
+#endif
+}
+
+LZ4_FORCE_INLINE void LZ4_clearHash(U32 h, void *tableBase,
+				    tableType_t const tableType)
+{
+	switch (tableType) {
+	default: /* fallthrough */
+	case clearedTable: { /* illegal! */
+		assert(0);
+		return;
+	}
+	case byPtr: {
+		const BYTE **hashTable = (const BYTE **)tableBase;
+		hashTable[h] = NULL;
+		return;
+	}
+	case byU32: {
+		U32 *hashTable = (U32 *)tableBase;
+		hashTable[h] = 0;
+		return;
+	}
+	case byU16: {
+		U16 *hashTable = (U16 *)tableBase;
+		hashTable[h] = 0;
+		return;
+	}
+	}
+}
+
+LZ4_FORCE_INLINE void LZ4_putIndexOnHash(U32 idx, U32 h, void *tableBase,
+					 tableType_t const tableType)
+{
+	switch (tableType) {
+	default: /* fallthrough */
+	case clearedTable: /* fallthrough */
+	case byPtr: { /* illegal! */
+		assert(0);
+		return;
+	}
+	case byU32: {
+		U32 *hashTable = (U32 *)tableBase;
+		hashTable[h] = idx;
+		return;
+	}
+	case byU16: {
+		U16 *hashTable = (U16 *)tableBase;
+		assert(idx < 65536);
+		hashTable[h] = (U16)idx;
+		return;
+	}
+	}
+}
+
+/* LZ4_putPosition*() : only used in byPtr mode */
+LZ4_FORCE_INLINE void LZ4_putPositionOnHash(const BYTE *p, U32 h,
+					    void *tableBase,
+					    tableType_t const tableType)
+{
+	const BYTE **const hashTable = (const BYTE **)tableBase;
+	assert(tableType == byPtr);
+	(void)tableType;
+	hashTable[h] = p;
+}
+
+LZ4_FORCE_INLINE void LZ4_putPosition(const BYTE *p, void *tableBase,
+				      tableType_t tableType)
+{
+	U32 const h = LZ4_hashPosition(p, tableType);
+	LZ4_putPositionOnHash(p, h, tableBase, tableType);
+}
+
+/* LZ4_getIndexOnHash() :
+ * Index of match position registered in hash table.
+ * hash position must be calculated by using base+index, or dictBase+index.
+ * Assumption 1 : only valid if tableType == byU32 or byU16.
+ * Assumption 2 : h is presumed valid (within limits of hash table)
+ */
+LZ4_FORCE_INLINE U32 LZ4_getIndexOnHash(U32 h, const void *tableBase,
+					tableType_t tableType)
+{
+	LZ4_STATIC_ASSERT(LZ4_MEMORY_USAGE > 2);
+	if (tableType == byU32) {
+		const U32 *const hashTable = (const U32 *)tableBase;
+		assert(h < (1U << (LZ4_MEMORY_USAGE - 2)));
+		return hashTable[h];
+	}
+	if (tableType == byU16) {
+		const U16 *const hashTable = (const U16 *)tableBase;
+		assert(h < (1U << (LZ4_MEMORY_USAGE - 1)));
+		return hashTable[h];
+	}
+	assert(0);
+	return 0; /* forbidden case */
+}
+
+static const BYTE *LZ4_getPositionOnHash(U32 h, const void *tableBase,
+					 tableType_t tableType)
+{
+	assert(tableType == byPtr);
+	(void)tableType;
+	{
+		const BYTE *const *hashTable = (const BYTE *const *)tableBase;
+		return hashTable[h];
+	}
+}
+
+LZ4_FORCE_INLINE const BYTE *
+LZ4_getPosition(const BYTE *p, const void *tableBase, tableType_t tableType)
+{
+	U32 const h = LZ4_hashPosition(p, tableType);
+	return LZ4_getPositionOnHash(h, tableBase, tableType);
+}
+
+LZ4_FORCE_INLINE void LZ4_prepareTable(LZ4_stream_t_internal *const cctx,
+				       const int inputSize,
+				       const tableType_t tableType)
+{
+	/* If the table hasn't been used, it's guaranteed to be zeroed out, and is
+     * therefore safe to use no matter what mode we're in. Otherwise, we figure
+     * out if it's safe to leave as is or whether it needs to be reset.
+     */
+	if ((tableType_t)cctx->tableType != clearedTable) {
+		assert(inputSize >= 0);
+		if ((tableType_t)cctx->tableType != tableType ||
+		    ((tableType == byU16) &&
+		     cctx->currentOffset + (unsigned)inputSize >= 0xFFFFU) ||
+		    ((tableType == byU32) && cctx->currentOffset > 1 GB) ||
+		    tableType == byPtr || inputSize >= 4 KB) {
+			DEBUGLOG(4, "LZ4_prepareTable: Resetting table in %p",
+				 cctx);
+			MEM_INIT(cctx->hashTable, 0, LZ4_HASHTABLESIZE);
+			cctx->currentOffset = 0;
+			cctx->tableType = (U32)clearedTable;
+		} else {
+			DEBUGLOG(
+				4,
+				"LZ4_prepareTable: Re-use hash table (no reset)");
+		}
+	}
+
+	/* Adding a gap, so all previous entries are > LZ4_DISTANCE_MAX back,
+     * is faster than compressing without a gap.
+     * However, compressing with currentOffset == 0 is faster still,
+     * so we preserve that case.
+     */
+	if (cctx->currentOffset != 0 && tableType == byU32) {
+		DEBUGLOG(5, "LZ4_prepareTable: adding 64KB to currentOffset");
+		cctx->currentOffset += 64 KB;
+	}
+
+	/* Finally, clear history */
+	cctx->dictCtx = NULL;
+	cctx->dictionary = NULL;
+	cctx->dictSize = 0;
+}
+
+/** LZ4_compress_generic_validated() :
+ *  inlined, to ensure branches are decided at compilation time.
+ *  The following conditions are presumed already validated:
+ *  - source != NULL
+ *  - inputSize > 0
+ */
+LZ4_FORCE_INLINE int LZ4_compress_generic_validated(
+	LZ4_stream_t_internal *const cctx, const char *const source,
+	char *const dest, const int inputSize,
+	int *inputConsumed, /* only written when outputDirective == fillOutput */
+	const int maxOutputSize, const limitedOutput_directive outputDirective,
+	const tableType_t tableType, const dict_directive dictDirective,
+	const dictIssue_directive dictIssue, const int acceleration)
+{
+	int result;
+	const BYTE *ip = (const BYTE *)source;
+
+	U32 const startIndex = cctx->currentOffset;
+	const BYTE *base = (const BYTE *)source - startIndex;
+	const BYTE *lowLimit;
+
+	const LZ4_stream_t_internal *dictCtx =
+		(const LZ4_stream_t_internal *)cctx->dictCtx;
+	const BYTE *const dictionary = dictDirective == usingDictCtx ?
+					       dictCtx->dictionary :
+					       cctx->dictionary;
+	const U32 dictSize = dictDirective == usingDictCtx ? dictCtx->dictSize :
+							     cctx->dictSize;
+	const U32 dictDelta =
+		(dictDirective == usingDictCtx) ?
+			startIndex - dictCtx->currentOffset :
+			0; /* make indexes in dictCtx comparable with indexes in current context */
+
+	int const maybe_extMem = (dictDirective == usingExtDict) ||
+				 (dictDirective == usingDictCtx);
+	U32 const prefixIdxLimit =
+		startIndex -
+		dictSize; /* used when dictDirective == dictSmall */
+	const BYTE *const dictEnd =
+		dictionary ? dictionary + dictSize : dictionary;
+	const BYTE *anchor = (const BYTE *)source;
+	const BYTE *const iend = ip + inputSize;
+	const BYTE *const mflimitPlusOne = iend - MFLIMIT + 1;
+	const BYTE *const matchlimit = iend - LASTLITERALS;
+
+	/* the dictCtx currentOffset is indexed on the start of the dictionary,
+     * while a dictionary in the current context precedes the currentOffset */
+	const BYTE *dictBase =
+		(dictionary == NULL) ?
+			NULL :
+		(dictDirective == usingDictCtx) ?
+			dictionary + dictSize - dictCtx->currentOffset :
+			dictionary + dictSize - startIndex;
+
+	BYTE *op = (BYTE *)dest;
+	BYTE *const olimit = op + maxOutputSize;
+
+	U32 offset = 0;
+	U32 forwardH;
+
+	DEBUGLOG(5, "LZ4_compress_generic_validated: srcSize=%i, tableType=%u",
+		 inputSize, tableType);
+	assert(ip != NULL);
+	if (tableType == byU16)
+		assert(inputSize <
+		       LZ4_64Klimit); /* Size too large (not within 64K limit) */
+	if (tableType == byPtr)
+		assert(dictDirective ==
+		       noDict); /* only supported use case with byPtr */
+	/* If init conditions are not met, we don't have to mark stream
+     * as having dirty context, since no action was taken yet */
+	if (outputDirective == fillOutput && maxOutputSize < 1) {
+		return 0;
+	} /* Impossible to store anything */
+	assert(acceleration >= 1);
+
+	lowLimit = (const BYTE *)source -
+		   (dictDirective == withPrefix64k ? dictSize : 0);
+
+	/* Update context state */
+	if (dictDirective == usingDictCtx) {
+		/* Subsequent linked blocks can't use the dictionary. */
+		/* Instead, they use the block we just compressed. */
+		cctx->dictCtx = NULL;
+		cctx->dictSize = (U32)inputSize;
+	} else {
+		cctx->dictSize += (U32)inputSize;
+	}
+	cctx->currentOffset += (U32)inputSize;
+	cctx->tableType = (U32)tableType;
+
+	if (inputSize < LZ4_minLength)
+		goto _last_literals; /* Input too small, no compression (all literals) */
+
+	/* First Byte */
+	{
+		U32 const h = LZ4_hashPosition(ip, tableType);
+		if (tableType == byPtr) {
+			LZ4_putPositionOnHash(ip, h, cctx->hashTable, byPtr);
+		} else {
+			LZ4_putIndexOnHash(startIndex, h, cctx->hashTable,
+					   tableType);
+		}
+	}
+	ip++;
+	forwardH = LZ4_hashPosition(ip, tableType);
+
+	/* Main Loop */
+	for (;;) {
+		const BYTE *match;
+		BYTE *token;
+		const BYTE *filledIp;
+
+		/* Find a match */
+		if (tableType == byPtr) {
+			const BYTE *forwardIp = ip;
+			int step = 1;
+			int searchMatchNb = acceleration << LZ4_skipTrigger;
+			do {
+				U32 const h = forwardH;
+				ip = forwardIp;
+				forwardIp += step;
+				step = (searchMatchNb++ >> LZ4_skipTrigger);
+
+				if (unlikely(forwardIp > mflimitPlusOne))
+					goto _last_literals;
+				assert(ip < mflimitPlusOne);
+
+				match = LZ4_getPositionOnHash(
+					h, cctx->hashTable, tableType);
+				forwardH =
+					LZ4_hashPosition(forwardIp, tableType);
+				LZ4_putPositionOnHash(ip, h, cctx->hashTable,
+						      tableType);
+
+			} while ((match + LZ4_DISTANCE_MAX < ip) ||
+				 (LZ4_read32(match) != LZ4_read32(ip)));
+
+		} else { /* byU32, byU16 */
+
+			const BYTE *forwardIp = ip;
+			int step = 1;
+			int searchMatchNb = acceleration << LZ4_skipTrigger;
+			do {
+				U32 const h = forwardH;
+				U32 const currentPos = (U32)(forwardIp - base);
+				U32 matchIndex = LZ4_getIndexOnHash(
+					h, cctx->hashTable, tableType);
+				assert(matchIndex <= currentPos);
+				assert(forwardIp - base <
+				       (ptrdiff_t)(2 GB - 1));
+				ip = forwardIp;
+				forwardIp += step;
+				step = (searchMatchNb++ >> LZ4_skipTrigger);
+
+				if (unlikely(forwardIp > mflimitPlusOne))
+					goto _last_literals;
+				assert(ip < mflimitPlusOne);
+
+				if (dictDirective == usingDictCtx) {
+					if (matchIndex < startIndex) {
+						/* there was no match, try the dictionary */
+						assert(tableType == byU32);
+						matchIndex = LZ4_getIndexOnHash(
+							h, dictCtx->hashTable,
+							byU32);
+						match = dictBase + matchIndex;
+						matchIndex +=
+							dictDelta; /* make dictCtx index comparable with current context */
+						lowLimit = dictionary;
+					} else {
+						match = base + matchIndex;
+						lowLimit = (const BYTE *)source;
+					}
+				} else if (dictDirective == usingExtDict) {
+					if (matchIndex < startIndex) {
+						DEBUGLOG(
+							7,
+							"extDict candidate: matchIndex=%5u  <  startIndex=%5u",
+							matchIndex, startIndex);
+						assert(startIndex -
+							       matchIndex >=
+						       MINMATCH);
+						assert(dictBase);
+						match = dictBase + matchIndex;
+						lowLimit = dictionary;
+					} else {
+						match = base + matchIndex;
+						lowLimit = (const BYTE *)source;
+					}
+				} else { /* single continuous memory segment */
+					match = base + matchIndex;
+				}
+				forwardH =
+					LZ4_hashPosition(forwardIp, tableType);
+				LZ4_putIndexOnHash(currentPos, h,
+						   cctx->hashTable, tableType);
+
+				DEBUGLOG(7,
+					 "candidate at pos=%u  (offset=%u \n",
+					 matchIndex, currentPos - matchIndex);
+				if ((dictIssue == dictSmall) &&
+				    (matchIndex < prefixIdxLimit)) {
+					continue;
+				} /* match outside of valid area */
+				assert(matchIndex < currentPos);
+				if (((tableType != byU16) ||
+				     (LZ4_DISTANCE_MAX <
+				      LZ4_DISTANCE_ABSOLUTE_MAX)) &&
+				    (matchIndex + LZ4_DISTANCE_MAX <
+				     currentPos)) {
+					continue;
+				} /* too far */
+				assert((currentPos - matchIndex) <=
+				       LZ4_DISTANCE_MAX); /* match now expected within distance */
+
+				if (LZ4_read32(match) == LZ4_read32(ip)) {
+					if (maybe_extMem)
+						offset =
+							currentPos - matchIndex;
+					break; /* match found */
+				}
+
+			} while (1);
+		}
+
+		/* Catch up */
+		filledIp = ip;
+		assert(ip >
+		       anchor); /* this is always true as ip has been advanced before entering the main loop */
+		if ((match > lowLimit) && unlikely(ip[-1] == match[-1])) {
+			do {
+				ip--;
+				match--;
+			} while (((ip > anchor) & (match > lowLimit)) &&
+				 (unlikely(ip[-1] == match[-1])));
+		}
+
+		/* Encode Literals */
+		{
+			unsigned const litLength = (unsigned)(ip - anchor);
+			token = op++;
+			if ((outputDirective ==
+			     limitedOutput) && /* Check output buffer overflow */
+			    (unlikely(op + litLength + (2 + 1 + LASTLITERALS) +
+					      (litLength / 255) >
+				      olimit))) {
+				return 0; /* cannot compress within `dst` budget. Stored indexes in hash table are nonetheless fine */
+			}
+			if ((outputDirective == fillOutput) &&
+			    (unlikely(
+				    op + (litLength + 240) / 255 /* litlen */ +
+					    litLength /* literals */ +
+					    2 /* offset */ + 1 /* token */ +
+					    MFLIMIT -
+					    MINMATCH /* min last literals so last match is <= end - MFLIMIT */
+				    > olimit))) {
+				op--;
+				goto _last_literals;
+			}
+			if (litLength >= RUN_MASK) {
+				unsigned len = litLength - RUN_MASK;
+				*token = (RUN_MASK << ML_BITS);
+				for (; len >= 255; len -= 255)
+					*op++ = 255;
+				*op++ = (BYTE)len;
+			} else
+				*token = (BYTE)(litLength << ML_BITS);
+
+			/* Copy Literals */
+			LZ4_wildCopy8(op, anchor, op + litLength);
+			op += litLength;
+			DEBUGLOG(6, "seq.start:%i, literals=%u, match.start:%i",
+				 (int)(anchor - (const BYTE *)source),
+				 litLength, (int)(ip - (const BYTE *)source));
+		}
+
+	_next_match:
+		/* at this stage, the following variables must be correctly set :
+         * - ip : at start of LZ operation
+         * - match : at start of previous pattern occurrence; can be within current prefix, or within extDict
+         * - offset : if maybe_ext_memSegment==1 (constant)
+         * - lowLimit : must be == dictionary to mean "match is within extDict"; must be == source otherwise
+         * - token and *token : position to write 4-bits for match length; higher 4-bits for literal length supposed already written
+         */
+
+		if ((outputDirective == fillOutput) &&
+		    (op + 2 /* offset */ + 1 /* token */ + MFLIMIT -
+			     MINMATCH /* min last literals so last match is <= end - MFLIMIT */
+		     > olimit)) {
+			/* the match was too close to the end, rewind and go to last literals */
+			op = token;
+			goto _last_literals;
+		}
+
+		/* Encode Offset */
+		if (maybe_extMem) { /* static test */
+			DEBUGLOG(6,
+				 "             with offset=%u  (ext if > %i)",
+				 offset, (int)(ip - (const BYTE *)source));
+			assert(offset <= LZ4_DISTANCE_MAX && offset > 0);
+			LZ4_writeLE16(op, (U16)offset);
+			op += 2;
+		} else {
+			DEBUGLOG(6,
+				 "             with offset=%u  (same segment)",
+				 (U32)(ip - match));
+			assert(ip - match <= LZ4_DISTANCE_MAX);
+			LZ4_writeLE16(op, (U16)(ip - match));
+			op += 2;
+		}
+
+		/* Encode MatchLength */
+		{
+			unsigned matchCode;
+
+			if ((dictDirective == usingExtDict ||
+			     dictDirective == usingDictCtx) &&
+			    (lowLimit ==
+			     dictionary) /* match within extDict */) {
+				const BYTE *limit = ip + (dictEnd - match);
+				assert(dictEnd > match);
+				if (limit > matchlimit)
+					limit = matchlimit;
+				matchCode = LZ4_count(ip + MINMATCH,
+						      match + MINMATCH, limit);
+				ip += (size_t)matchCode + MINMATCH;
+				if (ip == limit) {
+					unsigned const more =
+						LZ4_count(limit,
+							  (const BYTE *)source,
+							  matchlimit);
+					matchCode += more;
+					ip += more;
+				}
+				DEBUGLOG(
+					6,
+					"             with matchLength=%u starting in extDict",
+					matchCode + MINMATCH);
+			} else {
+				matchCode =
+					LZ4_count(ip + MINMATCH,
+						  match + MINMATCH, matchlimit);
+				ip += (size_t)matchCode + MINMATCH;
+				DEBUGLOG(6, "             with matchLength=%u",
+					 matchCode + MINMATCH);
+			}
+
+			if ((outputDirective) && /* Check output buffer overflow */
+			    (unlikely(op + (1 + LASTLITERALS) +
+					      (matchCode + 240) / 255 >
+				      olimit))) {
+				if (outputDirective == fillOutput) {
+					/* Match description too long : reduce it */
+					U32 newMatchCode =
+						15 /* in token */ -
+						1 /* to avoid needing a zero byte */ +
+						((U32)(olimit - op) - 1 -
+						 LASTLITERALS) *
+							255;
+					ip -= matchCode - newMatchCode;
+					assert(newMatchCode < matchCode);
+					matchCode = newMatchCode;
+					if (unlikely(ip <= filledIp)) {
+						/* We have already filled up to filledIp so if ip ends up less than filledIp
+                         * we have positions in the hash table beyond the current position. This is
+                         * a problem if we reuse the hash table. So we have to remove these positions
+                         * from the hash table.
+                         */
+						const BYTE *ptr;
+						DEBUGLOG(
+							5,
+							"Clearing %u positions",
+							(U32)(filledIp - ip));
+						for (ptr = ip; ptr <= filledIp;
+						     ++ptr) {
+							U32 const h =
+								LZ4_hashPosition(
+									ptr,
+									tableType);
+							LZ4_clearHash(
+								h,
+								cctx->hashTable,
+								tableType);
+						}
+					}
+				} else {
+					assert(outputDirective ==
+					       limitedOutput);
+					return 0; /* cannot compress within `dst` budget. Stored indexes in hash table are nonetheless fine */
+				}
+			}
+			if (matchCode >= ML_MASK) {
+				*token += ML_MASK;
+				matchCode -= ML_MASK;
+				LZ4_write32(op, 0xFFFFFFFF);
+				while (matchCode >= 4 * 255) {
+					op += 4;
+					LZ4_write32(op, 0xFFFFFFFF);
+					matchCode -= 4 * 255;
+				}
+				op += matchCode / 255;
+				*op++ = (BYTE)(matchCode % 255);
+			} else
+				*token += (BYTE)(matchCode);
+		}
+		/* Ensure we have enough space for the last literals. */
+		assert(!(outputDirective == fillOutput &&
+			 op + 1 + LASTLITERALS > olimit));
+
+		anchor = ip;
+
+		/* Test end of chunk */
+		if (ip >= mflimitPlusOne)
+			break;
+
+		/* Fill table */
+		{
+			U32 const h = LZ4_hashPosition(ip - 2, tableType);
+			if (tableType == byPtr) {
+				LZ4_putPositionOnHash(ip - 2, h,
+						      cctx->hashTable, byPtr);
+			} else {
+				U32 const idx = (U32)((ip - 2) - base);
+				LZ4_putIndexOnHash(idx, h, cctx->hashTable,
+						   tableType);
+			}
+		}
+
+		/* Test next position */
+		if (tableType == byPtr) {
+			match = LZ4_getPosition(ip, cctx->hashTable, tableType);
+			LZ4_putPosition(ip, cctx->hashTable, tableType);
+			if ((match + LZ4_DISTANCE_MAX >= ip) &&
+			    (LZ4_read32(match) == LZ4_read32(ip))) {
+				token = op++;
+				*token = 0;
+				goto _next_match;
+			}
+
+		} else { /* byU32, byU16 */
+
+			U32 const h = LZ4_hashPosition(ip, tableType);
+			U32 const currentPos = (U32)(ip - base);
+			U32 matchIndex = LZ4_getIndexOnHash(h, cctx->hashTable,
+							    tableType);
+			assert(matchIndex < currentPos);
+			if (dictDirective == usingDictCtx) {
+				if (matchIndex < startIndex) {
+					/* there was no match, try the dictionary */
+					assert(tableType == byU32);
+					matchIndex = LZ4_getIndexOnHash(
+						h, dictCtx->hashTable, byU32);
+					match = dictBase + matchIndex;
+					lowLimit =
+						dictionary; /* required for match length counter */
+					matchIndex += dictDelta;
+				} else {
+					match = base + matchIndex;
+					lowLimit = (const BYTE *)
+						source; /* required for match length counter */
+				}
+			} else if (dictDirective == usingExtDict) {
+				if (matchIndex < startIndex) {
+					assert(dictBase);
+					match = dictBase + matchIndex;
+					lowLimit =
+						dictionary; /* required for match length counter */
+				} else {
+					match = base + matchIndex;
+					lowLimit = (const BYTE *)
+						source; /* required for match length counter */
+				}
+			} else { /* single memory segment */
+				match = base + matchIndex;
+			}
+			LZ4_putIndexOnHash(currentPos, h, cctx->hashTable,
+					   tableType);
+			assert(matchIndex < currentPos);
+			if (((dictIssue == dictSmall) ?
+				     (matchIndex >= prefixIdxLimit) :
+				     1) &&
+			    (((tableType == byU16) &&
+			      (LZ4_DISTANCE_MAX == LZ4_DISTANCE_ABSOLUTE_MAX)) ?
+				     1 :
+				     (matchIndex + LZ4_DISTANCE_MAX >=
+				      currentPos)) &&
+			    (LZ4_read32(match) == LZ4_read32(ip))) {
+				token = op++;
+				*token = 0;
+				if (maybe_extMem)
+					offset = currentPos - matchIndex;
+				DEBUGLOG(
+					6,
+					"seq.start:%i, literals=%u, match.start:%i",
+					(int)(anchor - (const BYTE *)source), 0,
+					(int)(ip - (const BYTE *)source));
+				goto _next_match;
+			}
+		}
+
+		/* Prepare next loop */
+		forwardH = LZ4_hashPosition(++ip, tableType);
+	}
+
+_last_literals:
+	/* Encode Last Literals */
+	{
+		size_t lastRun = (size_t)(iend - anchor);
+		if ((outputDirective) && /* Check output buffer overflow */
+		    (op + lastRun + 1 + ((lastRun + 255 - RUN_MASK) / 255) >
+		     olimit)) {
+			if (outputDirective == fillOutput) {
+				/* adapt lastRun to fill 'dst' */
+				assert(olimit >= op);
+				lastRun = (size_t)(olimit - op) - 1 /*token*/;
+				lastRun -= (lastRun + 256 - RUN_MASK) /
+					   256; /*additional length tokens*/
+			} else {
+				assert(outputDirective == limitedOutput);
+				return 0; /* cannot compress within `dst` budget. Stored indexes in hash table are nonetheless fine */
+			}
+		}
+		DEBUGLOG(6, "Final literal run : %i literals", (int)lastRun);
+		if (lastRun >= RUN_MASK) {
+			size_t accumulator = lastRun - RUN_MASK;
+			*op++ = RUN_MASK << ML_BITS;
+			for (; accumulator >= 255; accumulator -= 255)
+				*op++ = 255;
+			*op++ = (BYTE)accumulator;
+		} else {
+			*op++ = (BYTE)(lastRun << ML_BITS);
+		}
+		LZ4_memcpy(op, anchor, lastRun);
+		ip = anchor + lastRun;
+		op += lastRun;
+	}
+
+	if (outputDirective == fillOutput) {
+		*inputConsumed = (int)(((const char *)ip) - source);
+	}
+	result = (int)(((char *)op) - dest);
+	assert(result > 0);
+	DEBUGLOG(5, "LZ4_compress_generic: compressed %i bytes into %i bytes",
+		 inputSize, result);
+	return result;
+}
+
+/** LZ4_compress_generic() :
+ *  inlined, to ensure branches are decided at compilation time;
+ *  takes care of src == (NULL, 0)
+ *  and forward the rest to LZ4_compress_generic_validated */
+LZ4_FORCE_INLINE int LZ4_compress_generic(
+	LZ4_stream_t_internal *const cctx, const char *const src,
+	char *const dst, const int srcSize,
+	int *inputConsumed, /* only written when outputDirective == fillOutput */
+	const int dstCapacity, const limitedOutput_directive outputDirective,
+	const tableType_t tableType, const dict_directive dictDirective,
+	const dictIssue_directive dictIssue, const int acceleration)
+{
+	DEBUGLOG(5, "LZ4_compress_generic: srcSize=%i, dstCapacity=%i", srcSize,
+		 dstCapacity);
+
+	if ((U32)srcSize > (U32)LZ4_MAX_INPUT_SIZE) {
+		return 0;
+	} /* Unsupported srcSize, too large (or negative) */
+	if (srcSize == 0) { /* src == NULL supported if srcSize == 0 */
+		if (outputDirective != notLimited && dstCapacity <= 0)
+			return 0; /* no output, can't write anything */
+		DEBUGLOG(5, "Generating an empty block");
+		assert(outputDirective == notLimited || dstCapacity >= 1);
+		assert(dst != NULL);
+		dst[0] = 0;
+		if (outputDirective == fillOutput) {
+			assert(inputConsumed != NULL);
+			*inputConsumed = 0;
+		}
+		return 1;
+	}
+	assert(src != NULL);
+
+	return LZ4_compress_generic_validated(
+		cctx, src, dst, srcSize,
+		inputConsumed, /* only written into if outputDirective == fillOutput */
+		dstCapacity, outputDirective, tableType, dictDirective,
+		dictIssue, acceleration);
+}
+
+int LZ4_compress_fast_extState(void *state, const char *source, char *dest,
+			       int inputSize, int maxOutputSize,
+			       int acceleration)
+{
+	LZ4_stream_t_internal *const ctx =
+		&LZ4_initStream(state, sizeof(LZ4_stream_t))->internal_donotuse;
+	assert(ctx != NULL);
+	if (acceleration < 1)
+		acceleration = LZ4_ACCELERATION_DEFAULT;
+	if (acceleration > LZ4_ACCELERATION_MAX)
+		acceleration = LZ4_ACCELERATION_MAX;
+	if (maxOutputSize >= LZ4_compressBound(inputSize)) {
+		if (inputSize < LZ4_64Klimit) {
+			return LZ4_compress_generic(ctx, source, dest,
+						    inputSize, NULL, 0,
+						    notLimited, byU16, noDict,
+						    noDictIssue, acceleration);
+		} else {
+			const tableType_t tableType =
+				((sizeof(void *) == 4) &&
+				 ((uptrval)source > LZ4_DISTANCE_MAX)) ?
+					byPtr :
+					byU32;
+			return LZ4_compress_generic(ctx, source, dest,
+						    inputSize, NULL, 0,
+						    notLimited, tableType,
+						    noDict, noDictIssue,
+						    acceleration);
+		}
+	} else {
+		if (inputSize < LZ4_64Klimit) {
+			return LZ4_compress_generic(
+				ctx, source, dest, inputSize, NULL,
+				maxOutputSize, limitedOutput, byU16, noDict,
+				noDictIssue, acceleration);
+		} else {
+			const tableType_t tableType =
+				((sizeof(void *) == 4) &&
+				 ((uptrval)source > LZ4_DISTANCE_MAX)) ?
+					byPtr :
+					byU32;
+			return LZ4_compress_generic(
+				ctx, source, dest, inputSize, NULL,
+				maxOutputSize, limitedOutput, tableType, noDict,
+				noDictIssue, acceleration);
+		}
+	}
+}
+
+/**
+ * LZ4_compress_fast_extState_fastReset() :
+ * A variant of LZ4_compress_fast_extState().
+ *
+ * Using this variant avoids an expensive initialization step. It is only safe
+ * to call if the state buffer is known to be correctly initialized already
+ * (see comment in lz4.h on LZ4_resetStream_fast() for a definition of
+ * "correctly initialized").
+ */
+int LZ4_compress_fast_extState_fastReset(void *state, const char *src,
+					 char *dst, int srcSize,
+					 int dstCapacity, int acceleration)
+{
+	LZ4_stream_t_internal *const ctx =
+		&((LZ4_stream_t *)state)->internal_donotuse;
+	if (acceleration < 1)
+		acceleration = LZ4_ACCELERATION_DEFAULT;
+	if (acceleration > LZ4_ACCELERATION_MAX)
+		acceleration = LZ4_ACCELERATION_MAX;
+	assert(ctx != NULL);
+
+	if (dstCapacity >= LZ4_compressBound(srcSize)) {
+		if (srcSize < LZ4_64Klimit) {
+			const tableType_t tableType = byU16;
+			LZ4_prepareTable(ctx, srcSize, tableType);
+			if (ctx->currentOffset) {
+				return LZ4_compress_generic(
+					ctx, src, dst, srcSize, NULL, 0,
+					notLimited, tableType, noDict,
+					dictSmall, acceleration);
+			} else {
+				return LZ4_compress_generic(
+					ctx, src, dst, srcSize, NULL, 0,
+					notLimited, tableType, noDict,
+					noDictIssue, acceleration);
+			}
+		} else {
+			const tableType_t tableType =
+				((sizeof(void *) == 4) &&
+				 ((uptrval)src > LZ4_DISTANCE_MAX)) ?
+					byPtr :
+					byU32;
+			LZ4_prepareTable(ctx, srcSize, tableType);
+			return LZ4_compress_generic(ctx, src, dst, srcSize,
+						    NULL, 0, notLimited,
+						    tableType, noDict,
+						    noDictIssue, acceleration);
+		}
+	} else {
+		if (srcSize < LZ4_64Klimit) {
+			const tableType_t tableType = byU16;
+			LZ4_prepareTable(ctx, srcSize, tableType);
+			if (ctx->currentOffset) {
+				return LZ4_compress_generic(
+					ctx, src, dst, srcSize, NULL,
+					dstCapacity, limitedOutput, tableType,
+					noDict, dictSmall, acceleration);
+			} else {
+				return LZ4_compress_generic(
+					ctx, src, dst, srcSize, NULL,
+					dstCapacity, limitedOutput, tableType,
+					noDict, noDictIssue, acceleration);
+			}
+		} else {
+			const tableType_t tableType =
+				((sizeof(void *) == 4) &&
+				 ((uptrval)src > LZ4_DISTANCE_MAX)) ?
+					byPtr :
+					byU32;
+			LZ4_prepareTable(ctx, srcSize, tableType);
+			return LZ4_compress_generic(ctx, src, dst, srcSize,
+						    NULL, dstCapacity,
+						    limitedOutput, tableType,
+						    noDict, noDictIssue,
+						    acceleration);
+		}
+	}
+}
+
+int LZ4_compress_fast(const char* src, char* dest, int srcSize, int dstCapacity,
+		      int acceleration, void *wrkmem)
+{
+    return LZ4_compress_fast_extState(wrkmem, src, dest, srcSize, dstCapacity, acceleration);
+}
+EXPORT_SYMBOL(LZ4_compress_fast);
+
+int LZ4_compress_default(const char *src, char *dst, int srcSize,
+			 int dstCapacity, void *wrkmem)
+{
+	return LZ4_compress_fast_extState(wrkmem, src, dst, srcSize,
+					  dstCapacity, 1);
+}
+EXPORT_SYMBOL(LZ4_compress_default);
+
+/* Note!: This function leaves the stream in an unclean/broken state!
+ * It is not safe to subsequently use the same state with a _fastReset() or
+ * _continue() call without resetting it. */
+static int LZ4_compress_destSize_extState_internal(LZ4_stream_t *state,
+						   const char *src, char *dst,
+						   int *srcSizePtr,
+						   int targetDstSize,
+						   int acceleration)
+{
+	void *const s = LZ4_initStream(state, sizeof(*state));
+	assert(s != NULL);
+	(void)s;
+
+	if (targetDstSize >=
+	    LZ4_compressBound(
+		    *srcSizePtr)) { /* compression success is guaranteed */
+		return LZ4_compress_fast_extState(state, src, dst, *srcSizePtr,
+						  targetDstSize, acceleration);
+	} else {
+		if (*srcSizePtr < LZ4_64Klimit) {
+			return LZ4_compress_generic(&state->internal_donotuse,
+						    src, dst, *srcSizePtr,
+						    srcSizePtr, targetDstSize,
+						    fillOutput, byU16, noDict,
+						    noDictIssue, acceleration);
+		} else {
+			tableType_t const addrMode =
+				((sizeof(void *) == 4) &&
+				 ((uptrval)src > LZ4_DISTANCE_MAX)) ?
+					byPtr :
+					byU32;
+			return LZ4_compress_generic(&state->internal_donotuse,
+						    src, dst, *srcSizePtr,
+						    srcSizePtr, targetDstSize,
+						    fillOutput, addrMode,
+						    noDict, noDictIssue,
+						    acceleration);
+		}
+	}
+}
+
+int LZ4_compress_destSize_extState(void *state, const char *src, char *dst,
+				   int *srcSizePtr, int targetDstSize,
+				   int acceleration)
+{
+	int const r = LZ4_compress_destSize_extState_internal(
+		(LZ4_stream_t *)state, src, dst, srcSizePtr, targetDstSize,
+		acceleration);
+	/* clean the state on exit */
+	LZ4_initStream(state, sizeof(LZ4_stream_t));
+	return r;
+}
+
+int LZ4_compress_destSize(const char *src, char *dst, int *srcSizePtr,
+			  int targetDstSize, void *wrkmem)
+{
+	return LZ4_compress_destSize_extState_internal(
+		wrkmem, src, dst, srcSizePtr, targetDstSize, 1);
+}
+EXPORT_SYMBOL(LZ4_compress_destSize);
+
+/*-******************************
+*  Streaming functions
+********************************/
+
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+LZ4_stream_t *LZ4_createStream(void)
+{
+	LZ4_stream_t *const lz4s = (LZ4_stream_t *)ALLOC(sizeof(LZ4_stream_t));
+	LZ4_STATIC_ASSERT(sizeof(LZ4_stream_t) >=
+			  sizeof(LZ4_stream_t_internal));
+	DEBUGLOG(4, "LZ4_createStream %p", lz4s);
+	if (lz4s == NULL)
+		return NULL;
+	LZ4_initStream(lz4s, sizeof(*lz4s));
+	return lz4s;
+}
+#endif
+
+static size_t LZ4_stream_t_alignment(void)
+{
+#if LZ4_ALIGN_TEST
+	typedef struct {
+		char c;
+		LZ4_stream_t t;
+	} t_a;
+	return sizeof(t_a) - sizeof(LZ4_stream_t);
+#else
+	return 1; /* effectively disabled */
+#endif
+}
+
+LZ4_stream_t *LZ4_initStream(void *buffer, size_t size)
+{
+	DEBUGLOG(5, "LZ4_initStream");
+	if (buffer == NULL) {
+		return NULL;
+	}
+	if (size < sizeof(LZ4_stream_t)) {
+		return NULL;
+	}
+	if (!LZ4_isAligned(buffer, LZ4_stream_t_alignment()))
+		return NULL;
+	MEM_INIT(buffer, 0, sizeof(LZ4_stream_t_internal));
+	return (LZ4_stream_t *)buffer;
+}
+
+/* resetStream is now deprecated,
+ * prefer initStream() which is more general */
+void LZ4_resetStream(LZ4_stream_t *LZ4_stream)
+{
+	DEBUGLOG(5, "LZ4_resetStream (ctx:%p)", LZ4_stream);
+	MEM_INIT(LZ4_stream, 0, sizeof(LZ4_stream_t_internal));
+}
+
+void LZ4_resetStream_fast(LZ4_stream_t *ctx)
+{
+	LZ4_prepareTable(&(ctx->internal_donotuse), 0, byU32);
+}
+
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+int LZ4_freeStream(LZ4_stream_t *LZ4_stream)
+{
+	if (!LZ4_stream)
+		return 0; /* support free on NULL */
+	DEBUGLOG(5, "LZ4_freeStream %p", LZ4_stream);
+	FREEMEM(LZ4_stream);
+	return (0);
+}
+#endif
+
+typedef enum { _ld_fast, _ld_slow } LoadDict_mode_e;
+#define HASH_UNIT sizeof(reg_t)
+int LZ4_loadDict_internal(LZ4_stream_t *LZ4_dict, const char *dictionary,
+			  int dictSize, LoadDict_mode_e _ld)
+{
+	LZ4_stream_t_internal *const dict = &LZ4_dict->internal_donotuse;
+	const tableType_t tableType = byU32;
+	const BYTE *p = (const BYTE *)dictionary;
+	const BYTE *const dictEnd = p + dictSize;
+	U32 idx32;
+
+	DEBUGLOG(4, "LZ4_loadDict (%i bytes from %p into %p)", dictSize,
+		 dictionary, LZ4_dict);
+
+	/* It's necessary to reset the context,
+     * and not just continue it with prepareTable()
+     * to avoid any risk of generating overflowing matchIndex
+     * when compressing using this dictionary */
+	LZ4_resetStream(LZ4_dict);
+
+	/* We always increment the offset by 64 KB, since, if the dict is longer,
+     * we truncate it to the last 64k, and if it's shorter, we still want to
+     * advance by a whole window length so we can provide the guarantee that
+     * there are only valid offsets in the window, which allows an optimization
+     * in LZ4_compress_fast_continue() where it uses noDictIssue even when the
+     * dictionary isn't a full 64k. */
+	dict->currentOffset += 64 KB;
+
+	if (dictSize < (int)HASH_UNIT) {
+		return 0;
+	}
+
+	if ((dictEnd - p) > 64 KB)
+		p = dictEnd - 64 KB;
+	dict->dictionary = p;
+	dict->dictSize = (U32)(dictEnd - p);
+	dict->tableType = (U32)tableType;
+	idx32 = dict->currentOffset - dict->dictSize;
+
+	while (p <= dictEnd - HASH_UNIT) {
+		U32 const h = LZ4_hashPosition(p, tableType);
+		/* Note: overwriting => favors positions end of dictionary */
+		LZ4_putIndexOnHash(idx32, h, dict->hashTable, tableType);
+		p += 3;
+		idx32 += 3;
+	}
+
+	if (_ld == _ld_slow) {
+		/* Fill hash table with additional references, to improve compression capability */
+		p = dict->dictionary;
+		idx32 = dict->currentOffset - dict->dictSize;
+		while (p <= dictEnd - HASH_UNIT) {
+			U32 const h = LZ4_hashPosition(p, tableType);
+			U32 const limit = dict->currentOffset - 64 KB;
+			if (LZ4_getIndexOnHash(h, dict->hashTable, tableType) <=
+			    limit) {
+				/* Note: not overwriting => favors positions beginning of dictionary */
+				LZ4_putIndexOnHash(idx32, h, dict->hashTable,
+						   tableType);
+			}
+			p++;
+			idx32++;
+		}
+	}
+
+	return (int)dict->dictSize;
+}
+
+int LZ4_loadDict(LZ4_stream_t *LZ4_dict, const char *dictionary, int dictSize)
+{
+	return LZ4_loadDict_internal(LZ4_dict, dictionary, dictSize, _ld_fast);
+}
+EXPORT_SYMBOL(LZ4_loadDict);
+
+int LZ4_loadDictSlow(LZ4_stream_t *LZ4_dict, const char *dictionary,
+		     int dictSize)
+{
+	return LZ4_loadDict_internal(LZ4_dict, dictionary, dictSize, _ld_slow);
+}
+
+void LZ4_attach_dictionary(LZ4_stream_t *workingStream,
+			   const LZ4_stream_t *dictionaryStream)
+{
+	const LZ4_stream_t_internal *dictCtx =
+		(dictionaryStream == NULL) ?
+			NULL :
+			&(dictionaryStream->internal_donotuse);
+
+	DEBUGLOG(4, "LZ4_attach_dictionary (%p, %p, size %u)", workingStream,
+		 dictionaryStream, dictCtx != NULL ? dictCtx->dictSize : 0);
+
+	if (dictCtx != NULL) {
+		/* If the current offset is zero, we will never look in the
+         * external dictionary context, since there is no value a table
+         * entry can take that indicate a miss. In that case, we need
+         * to bump the offset to something non-zero.
+         */
+		if (workingStream->internal_donotuse.currentOffset == 0) {
+			workingStream->internal_donotuse.currentOffset = 64 KB;
+		}
+
+		/* Don't actually attach an empty dictionary.
+         */
+		if (dictCtx->dictSize == 0) {
+			dictCtx = NULL;
+		}
+	}
+	workingStream->internal_donotuse.dictCtx = dictCtx;
+}
+
+static void LZ4_renormDictT(LZ4_stream_t_internal *LZ4_dict, int nextSize)
+{
+	assert(nextSize >= 0);
+	if (LZ4_dict->currentOffset + (unsigned)nextSize >
+	    0x80000000) { /* potential ptrdiff_t overflow (32-bits mode) */
+		/* rescale hash table */
+		U32 const delta = LZ4_dict->currentOffset - 64 KB;
+		const BYTE *dictEnd = LZ4_dict->dictionary + LZ4_dict->dictSize;
+		int i;
+		DEBUGLOG(4, "LZ4_renormDictT");
+		for (i = 0; i < LZ4_HASH_SIZE_U32; i++) {
+			if (LZ4_dict->hashTable[i] < delta)
+				LZ4_dict->hashTable[i] = 0;
+			else
+				LZ4_dict->hashTable[i] -= delta;
+		}
+		LZ4_dict->currentOffset = 64 KB;
+		if (LZ4_dict->dictSize > 64 KB)
+			LZ4_dict->dictSize = 64 KB;
+		LZ4_dict->dictionary = dictEnd - LZ4_dict->dictSize;
+	}
+}
+
+int LZ4_compress_fast_continue(LZ4_stream_t *LZ4_stream, const char *source,
+			       char *dest, int inputSize, int maxOutputSize,
+			       int acceleration)
+{
+	const tableType_t tableType = byU32;
+	LZ4_stream_t_internal *const streamPtr = &LZ4_stream->internal_donotuse;
+	const char *dictEnd = streamPtr->dictSize ?
+				      (const char *)streamPtr->dictionary +
+					      streamPtr->dictSize :
+				      NULL;
+
+	DEBUGLOG(5, "LZ4_compress_fast_continue (inputSize=%i, dictSize=%u)",
+		 inputSize, streamPtr->dictSize);
+
+	LZ4_renormDictT(streamPtr, inputSize); /* fix index overflow */
+	if (acceleration < 1)
+		acceleration = LZ4_ACCELERATION_DEFAULT;
+	if (acceleration > LZ4_ACCELERATION_MAX)
+		acceleration = LZ4_ACCELERATION_MAX;
+
+	/* invalidate tiny dictionaries */
+	if ((streamPtr->dictSize <
+	     4) /* tiny dictionary : not enough for a hash */
+	    && (dictEnd != source) /* prefix mode */
+	    &&
+	    (inputSize >
+	     0) /* tolerance : don't lose history, in case next invocation would use prefix mode */
+	    && (streamPtr->dictCtx == NULL) /* usingDictCtx */
+	) {
+		DEBUGLOG(
+			5,
+			"LZ4_compress_fast_continue: dictSize(%u) at addr:%p is too small",
+			streamPtr->dictSize, streamPtr->dictionary);
+		/* remove dictionary existence from history, to employ faster prefix mode */
+		streamPtr->dictSize = 0;
+		streamPtr->dictionary = (const BYTE *)source;
+		dictEnd = source;
+	}
+
+	/* Check overlapping input/dictionary space */
+	{
+		const char *const sourceEnd = source + inputSize;
+		if ((sourceEnd > (const char *)streamPtr->dictionary) &&
+		    (sourceEnd < dictEnd)) {
+			streamPtr->dictSize = (U32)(dictEnd - sourceEnd);
+			if (streamPtr->dictSize > 64 KB)
+				streamPtr->dictSize = 64 KB;
+			if (streamPtr->dictSize < 4)
+				streamPtr->dictSize = 0;
+			streamPtr->dictionary =
+				(const BYTE *)dictEnd - streamPtr->dictSize;
+		}
+	}
+
+	/* prefix mode : source data follows dictionary */
+	if (dictEnd == source) {
+		if ((streamPtr->dictSize < 64 KB) &&
+		    (streamPtr->dictSize < streamPtr->currentOffset))
+			return LZ4_compress_generic(
+				streamPtr, source, dest, inputSize, NULL,
+				maxOutputSize, limitedOutput, tableType,
+				withPrefix64k, dictSmall, acceleration);
+		else
+			return LZ4_compress_generic(
+				streamPtr, source, dest, inputSize, NULL,
+				maxOutputSize, limitedOutput, tableType,
+				withPrefix64k, noDictIssue, acceleration);
+	}
+
+	/* external dictionary mode */
+	{
+		int result;
+		if (streamPtr->dictCtx) {
+			/* We depend here on the fact that dictCtx'es (produced by
+             * LZ4_loadDict) guarantee that their tables contain no references
+             * to offsets between dictCtx->currentOffset - 64 KB and
+             * dictCtx->currentOffset - dictCtx->dictSize. This makes it safe
+             * to use noDictIssue even when the dict isn't a full 64 KB.
+             */
+			if (inputSize > 4 KB) {
+				/* For compressing large blobs, it is faster to pay the setup
+                 * cost to copy the dictionary's tables into the active context,
+                 * so that the compression loop is only looking into one table.
+                 */
+				LZ4_memcpy(streamPtr, streamPtr->dictCtx,
+					   sizeof(*streamPtr));
+				result = LZ4_compress_generic(
+					streamPtr, source, dest, inputSize,
+					NULL, maxOutputSize, limitedOutput,
+					tableType, usingExtDict, noDictIssue,
+					acceleration);
+			} else {
+				result = LZ4_compress_generic(
+					streamPtr, source, dest, inputSize,
+					NULL, maxOutputSize, limitedOutput,
+					tableType, usingDictCtx, noDictIssue,
+					acceleration);
+			}
+		} else { /* small data <= 4 KB */
+			if ((streamPtr->dictSize < 64 KB) &&
+			    (streamPtr->dictSize < streamPtr->currentOffset)) {
+				result = LZ4_compress_generic(
+					streamPtr, source, dest, inputSize,
+					NULL, maxOutputSize, limitedOutput,
+					tableType, usingExtDict, dictSmall,
+					acceleration);
+			} else {
+				result = LZ4_compress_generic(
+					streamPtr, source, dest, inputSize,
+					NULL, maxOutputSize, limitedOutput,
+					tableType, usingExtDict, noDictIssue,
+					acceleration);
+			}
+		}
+		streamPtr->dictionary = (const BYTE *)source;
+		streamPtr->dictSize = (U32)inputSize;
+		return result;
+	}
+}
+
+/* Hidden debug function, to force-test external dictionary mode */
+int LZ4_compress_forceExtDict(LZ4_stream_t *LZ4_dict, const char *source,
+			      char *dest, int srcSize)
+{
+	LZ4_stream_t_internal *const streamPtr = &LZ4_dict->internal_donotuse;
+	int result;
+
+	LZ4_renormDictT(streamPtr, srcSize);
+
+	if ((streamPtr->dictSize < 64 KB) &&
+	    (streamPtr->dictSize < streamPtr->currentOffset)) {
+		result = LZ4_compress_generic(streamPtr, source, dest, srcSize,
+					      NULL, 0, notLimited, byU32,
+					      usingExtDict, dictSmall, 1);
+	} else {
+		result = LZ4_compress_generic(streamPtr, source, dest, srcSize,
+					      NULL, 0, notLimited, byU32,
+					      usingExtDict, noDictIssue, 1);
+	}
+
+	streamPtr->dictionary = (const BYTE *)source;
+	streamPtr->dictSize = (U32)srcSize;
+
+	return result;
+}
+
+/*! LZ4_saveDict() :
+ *  If previously compressed data block is not guaranteed to remain available at its memory location,
+ *  save it into a safer place (char* safeBuffer).
+ *  Note : no need to call LZ4_loadDict() afterwards, dictionary is immediately usable,
+ *         one can therefore call LZ4_compress_fast_continue() right after.
+ * @return : saved dictionary size in bytes (necessarily <= dictSize), or 0 if error.
+ */
+int LZ4_saveDict(LZ4_stream_t *LZ4_dict, char *safeBuffer, int dictSize)
+{
+	LZ4_stream_t_internal *const dict = &LZ4_dict->internal_donotuse;
+
+	DEBUGLOG(5, "LZ4_saveDict : dictSize=%i, safeBuffer=%p", dictSize,
+		 safeBuffer);
+
+	if ((U32)dictSize > 64 KB) {
+		dictSize = 64 KB;
+	} /* useless to define a dictionary > 64 KB */
+	if ((U32)dictSize > dict->dictSize) {
+		dictSize = (int)dict->dictSize;
+	}
+
+	if (safeBuffer == NULL)
+		assert(dictSize == 0);
+	if (dictSize > 0) {
+		const BYTE *const previousDictEnd =
+			dict->dictionary + dict->dictSize;
+		assert(dict->dictionary);
+		LZ4_memmove(safeBuffer, previousDictEnd - dictSize,
+			    (size_t)dictSize);
+	}
+
+	dict->dictionary = (const BYTE *)safeBuffer;
+	dict->dictSize = (U32)dictSize;
+
+	return dictSize;
+}
+EXPORT_SYMBOL(LZ4_saveDict);
+
+/*-*******************************
+ *  Decompression functions
+ ********************************/
+
+typedef enum { decode_full_block = 0, partial_decode = 1 } earlyEnd_directive;
+
+#undef MIN
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+
+/* variant for decompress_unsafe()
+ * does not know end of input
+ * presumes input is well formed
+ * note : will consume at least one byte */
+static size_t read_long_length_no_check(const BYTE **pp)
+{
+	size_t b, l = 0;
+	do {
+		b = **pp;
+		(*pp)++;
+		l += b;
+	} while (b == 255);
+	DEBUGLOG(6,
+		 "read_long_length_no_check: +length=%zu using %zu input bytes",
+		 l, l / 255 + 1)
+	return l;
+}
+
+/* core decoder variant for LZ4_decompress_fast*()
+ * for legacy support only : these entry points are deprecated.
+ * - Presumes input is correctly formed (no defense vs malformed inputs)
+ * - Does not know input size (presume input buffer is "large enough")
+ * - Decompress a full block (only)
+ * @return : nb of bytes read from input.
+ * Note : this variant is not optimized for speed, just for maintenance.
+ *        the goal is to remove support of decompress_fast*() variants by v2.0
+**/
+LZ4_FORCE_INLINE int LZ4_decompress_unsafe_generic(
+	const BYTE *const istart, BYTE *const ostart, int decompressedSize,
+
+	size_t prefixSize,
+	const BYTE *const dictStart, /* only if dict==usingExtDict */
+	const size_t dictSize /* note: =0 if dictStart==NULL */
+)
+{
+	const BYTE *ip = istart;
+	BYTE *op = (BYTE *)ostart;
+	BYTE *const oend = ostart + decompressedSize;
+	const BYTE *const prefixStart = ostart - prefixSize;
+
+	DEBUGLOG(5, "LZ4_decompress_unsafe_generic");
+	if (dictStart == NULL)
+		assert(dictSize == 0);
+
+	while (1) {
+		/* start new sequence */
+		unsigned token = *ip++;
+
+		/* literals */
+		{
+			size_t ll = token >> ML_BITS;
+			if (ll == 15) {
+				/* long literal length */
+				ll += read_long_length_no_check(&ip);
+			}
+			if ((size_t)(oend - op) < ll)
+				return -1; /* output buffer overflow */
+			LZ4_memmove(op, ip,
+				    ll); /* support in-place decompression */
+			op += ll;
+			ip += ll;
+			if ((size_t)(oend - op) < MFLIMIT) {
+				if (op == oend)
+					break; /* end of block */
+				DEBUGLOG(
+					5,
+					"invalid: literals end at distance %zi from end of block",
+					oend - op);
+				/* incorrect end of block :
+                 * last match must start at least MFLIMIT==12 bytes before end of output block */
+				return -1;
+			}
+		}
+
+		/* match */
+		{
+			size_t ml = token & 15;
+			size_t const offset = LZ4_readLE16(ip);
+			ip += 2;
+
+			if (ml == 15) {
+				/* long literal length */
+				ml += read_long_length_no_check(&ip);
+			}
+			ml += MINMATCH;
+
+			if ((size_t)(oend - op) < ml)
+				return -1; /* output buffer overflow */
+
+			{
+				const BYTE *match = op - offset;
+
+				/* out of range */
+				if (offset >
+				    (size_t)(op - prefixStart) + dictSize) {
+					DEBUGLOG(6, "offset out of range");
+					return -1;
+				}
+
+				/* check special case : extDict */
+				if (offset > (size_t)(op - prefixStart)) {
+					/* extDict scenario */
+					const BYTE *const dictEnd =
+						dictStart + dictSize;
+					const BYTE *extMatch =
+						dictEnd -
+						(offset -
+						 (size_t)(op - prefixStart));
+					size_t const extml =
+						(size_t)(dictEnd - extMatch);
+					if (extml > ml) {
+						/* match entirely within extDict */
+						LZ4_memmove(op, extMatch, ml);
+						op += ml;
+						ml = 0;
+					} else {
+						/* match split between extDict & prefix */
+						LZ4_memmove(op, extMatch,
+							    extml);
+						op += extml;
+						ml -= extml;
+					}
+					match = prefixStart;
+				}
+
+				/* match copy - slow variant, supporting overlap copy */
+				{
+					size_t u;
+					for (u = 0; u < ml; u++) {
+						op[u] = match[u];
+					}
+				}
+			}
+			op += ml;
+			if ((size_t)(oend - op) < LASTLITERALS) {
+				DEBUGLOG(
+					5,
+					"invalid: match ends at distance %zi from end of block",
+					oend - op);
+				/* incorrect end of block :
+                 * last match must stop at least LASTLITERALS==5 bytes before end of output block */
+				return -1;
+			}
+		} /* match */
+	} /* main loop */
+	return (int)(ip - istart);
+}
+
+/* Read the variable-length literal or match length.
+ *
+ * @ip : input pointer
+ * @ilimit : position after which if length is not decoded, the input is necessarily corrupted.
+ * @initial_check - check ip >= ipmax before start of loop.  Returns initial_error if so.
+ * @error (output) - error code.  Must be set to 0 before call.
+**/
+typedef size_t Rvl_t;
+static const Rvl_t rvl_error = (Rvl_t)(-1);
+LZ4_FORCE_INLINE Rvl_t read_variable_length(const BYTE **ip, const BYTE *ilimit,
+					    int initial_check)
+{
+	Rvl_t s, length = 0;
+	assert(ip != NULL);
+	assert(*ip != NULL);
+	assert(ilimit != NULL);
+	if (initial_check &&
+	    unlikely((*ip) >= ilimit)) { /* read limit reached */
+		return rvl_error;
+	}
+	s = **ip;
+	(*ip)++;
+	length += s;
+	if (unlikely((*ip) > ilimit)) { /* read limit reached */
+		return rvl_error;
+	}
+	/* accumulator overflow detection (32-bit mode only) */
+	if ((sizeof(length) < 8) && unlikely(length > ((Rvl_t)(-1) / 2))) {
+		return rvl_error;
+	}
+	if (likely(s != 255))
+		return length;
+	do {
+		s = **ip;
+		(*ip)++;
+		length += s;
+		if (unlikely((*ip) > ilimit)) { /* read limit reached */
+			return rvl_error;
+		}
+		/* accumulator overflow detection (32-bit mode only) */
+		if ((sizeof(length) < 8) &&
+		    unlikely(length > ((Rvl_t)(-1) / 2))) {
+			return rvl_error;
+		}
+	} while (s == 255);
+
+	return length;
+}
+
+/*! LZ4_decompress_generic() :
+ *  This generic decompression function covers all use cases.
+ *  It shall be instantiated several times, using different sets of directives.
+ *  Note that it is important for performance that this function really get inlined,
+ *  in order to remove useless branches during compilation optimization.
+ */
+LZ4_FORCE_INLINE int __LZ4_decompress_generic(
+	const char *const src, char *const dst, const BYTE *ip, BYTE *op,
+	int srcSize,
+	int outputSize, /* If endOnInput==endOnInputSize, this value is `dstCapacity` */
+
+	earlyEnd_directive partialDecoding, /* full, partial */
+	dict_directive dict, /* noDict, withPrefix64k, usingExtDict */
+	const BYTE *const lowPrefix, /* always <= dst, == dst when no prefix */
+	const BYTE *const dictStart, /* only if dict==usingExtDict */
+	const size_t dictSize /* note : = 0 if noDict */
+)
+{
+	if ((src == NULL) || (outputSize < 0)) {
+		return -1;
+	}
+
+	{
+		const BYTE *const iend = src + srcSize;
+
+		BYTE *const oend = dst + outputSize;
+		BYTE *cpy;
+
+		const BYTE *const dictEnd =
+			(dictStart == NULL) ? NULL : dictStart + dictSize;
+
+		const int checkOffset = (dictSize < (int)(64 KB));
+
+		/* Set up the "end" pointers for the shortcut. */
+		const BYTE *const shortiend =
+			iend - 14 /*maxLL*/ - 2 /*offset*/;
+		const BYTE *const shortoend =
+			oend - 14 /*maxLL*/ - 18 /*maxML*/;
+
+		const BYTE *match;
+		size_t offset;
+		unsigned token;
+		size_t length;
+
+		DEBUGLOG(5, "LZ4_decompress_generic (srcSize:%i, dstSize:%i)",
+			 srcSize, outputSize);
+
+		/* Special cases */
+		assert(lowPrefix <= op);
+		if (unlikely(outputSize == 0)) {
+			/* Empty output buffer */
+			if (partialDecoding)
+				return 0;
+			return ((srcSize == 1) && (*ip == 0)) ? 0 : -1;
+		}
+		if (unlikely(srcSize == 0)) {
+			return -1;
+		}
+
+		/* LZ4_FAST_DEC_LOOP:
+     * designed for modern OoO performance cpus,
+     * where copying reliably 32-bytes is preferable to an unpredictable branch.
+     * note : fast loop may show a regression for some client arm chips. */
+#if LZ4_FAST_DEC_LOOP
+		if ((oend - op) < FASTLOOP_SAFE_DISTANCE) {
+			DEBUGLOG(6, "move to safe decode loop");
+			goto safe_decode;
+		}
+
+		/* Fast loop : decode sequences as long as output < oend-FASTLOOP_SAFE_DISTANCE */
+		DEBUGLOG(6, "using fast decode loop");
+		while (1) {
+			/* Main fastloop assertion: We can always wildcopy FASTLOOP_SAFE_DISTANCE */
+			assert(oend - op >= FASTLOOP_SAFE_DISTANCE);
+			assert(ip < iend);
+			token = *ip++;
+			length = token >> ML_BITS; /* literal length */
+			DEBUGLOG(7, "blockPos%6u: litLength token = %u",
+				 (unsigned)(op - (BYTE *)dst),
+				 (unsigned)length);
+
+			/* decode literal length */
+			if (length == RUN_MASK) {
+				size_t const addl = read_variable_length(
+					&ip, iend - RUN_MASK, 1);
+				if (addl == rvl_error) {
+					DEBUGLOG(
+						6,
+						"error reading long literal length");
+					goto _output_error;
+				}
+				length += addl;
+				if (unlikely((uptrval)(op) + length <
+					     (uptrval)(op))) {
+					goto _output_error;
+				} /* overflow detection */
+				if (unlikely((uptrval)(ip) + length <
+					     (uptrval)(ip))) {
+					goto _output_error;
+				} /* overflow detection */
+
+				/* copy literals */
+				LZ4_STATIC_ASSERT(MFLIMIT >= WILDCOPYLENGTH);
+				if ((op + length > oend - 32) ||
+				    (ip + length > iend - 32)) {
+					goto safe_literal_copy;
+				}
+				LZ4_wildCopy32(op, ip, op + length);
+				ip += length;
+				op += length;
+			} else if (ip <=
+				   iend - (16 +
+					   1 /*max lit + offset + nextToken*/)) {
+				/* We don't need to check oend, since we check it once for each loop below */
+				DEBUGLOG(7,
+					 "copy %u bytes in a 16-bytes stripe",
+					 (unsigned)length);
+				/* Literals can only be <= 14, but hope compilers optimize better when copy by a register size */
+				LZ4_memcpy(op, ip, 16);
+				ip += length;
+				op += length;
+			} else {
+				goto safe_literal_copy;
+			}
+
+			/* get offset */
+			offset = LZ4_readLE16(ip);
+			ip += 2;
+			DEBUGLOG(6, "blockPos%6u: offset = %u",
+				 (unsigned)(op - (BYTE *)dst),
+				 (unsigned)offset);
+			match = op - offset;
+			assert(match <= op); /* overflow check */
+
+			/* get matchlength */
+			length = token & ML_MASK;
+			DEBUGLOG(7, "  match length token = %u (len==%u)",
+				 (unsigned)length, (unsigned)length + MINMATCH);
+
+			if (length == ML_MASK) {
+				size_t const addl = read_variable_length(
+					&ip, iend - LASTLITERALS + 1, 0);
+				if (addl == rvl_error) {
+					DEBUGLOG(
+						5,
+						"error reading long match length");
+					goto _output_error;
+				}
+				length += addl;
+				length += MINMATCH;
+				DEBUGLOG(7, "  long match length == %u",
+					 (unsigned)length);
+				if (unlikely((uptrval)(op) + length <
+					     (uptrval)op)) {
+					goto _output_error;
+				} /* overflow detection */
+				if (op + length >=
+				    oend - FASTLOOP_SAFE_DISTANCE) {
+					goto safe_match_copy;
+				}
+			} else {
+				length += MINMATCH;
+				if (op + length >=
+				    oend - FASTLOOP_SAFE_DISTANCE) {
+					DEBUGLOG(
+						7,
+						"moving to safe_match_copy (ml==%u)",
+						(unsigned)length);
+					goto safe_match_copy;
+				}
+
+				/* Fastpath check: skip LZ4_wildCopy32 when true */
+				if ((dict == withPrefix64k) ||
+				    (match >= lowPrefix)) {
+					if (offset >= 8) {
+						assert(match >= lowPrefix);
+						assert(match <= op);
+						assert(op + 18 <= oend);
+
+						LZ4_memcpy(op, match, 8);
+						LZ4_memcpy(op + 8, match + 8,
+							   8);
+						LZ4_memcpy(op + 16, match + 16,
+							   2);
+						op += length;
+						continue;
+					}
+				}
+			}
+
+			if (checkOffset &&
+			    (unlikely(match + dictSize < lowPrefix))) {
+				DEBUGLOG(
+					5,
+					"Error : pos=%zi, offset=%zi => outside buffers",
+					op - lowPrefix, op - match);
+				goto _output_error;
+			}
+			/* match starting within external dictionary */
+			if ((dict == usingExtDict) && (match < lowPrefix)) {
+				assert(dictEnd != NULL);
+				if (unlikely(op + length >
+					     oend - LASTLITERALS)) {
+					if (partialDecoding) {
+						DEBUGLOG(
+							7,
+							"partialDecoding: dictionary match, close to dstEnd");
+						length = MIN(
+							length,
+							(size_t)(oend - op));
+					} else {
+						DEBUGLOG(
+							6,
+							"end-of-block condition violated")
+						goto _output_error;
+					}
+				}
+
+				if (length <= (size_t)(lowPrefix - match)) {
+					/* match fits entirely within external dictionary : just copy */
+					LZ4_memmove(op,
+						    dictEnd -
+							    (lowPrefix - match),
+						    length);
+					op += length;
+				} else {
+					/* match stretches into both external dictionary and current block */
+					size_t const copySize =
+						(size_t)(lowPrefix - match);
+					size_t const restSize =
+						length - copySize;
+					LZ4_memcpy(op, dictEnd - copySize,
+						   copySize);
+					op += copySize;
+					if (restSize >
+					    (size_t)(op -
+						     lowPrefix)) { /* overlap copy */
+						BYTE *const endOfMatch =
+							op + restSize;
+						const BYTE *copyFrom =
+							lowPrefix;
+						while (op < endOfMatch) {
+							*op++ = *copyFrom++;
+						}
+					} else {
+						LZ4_memcpy(op, lowPrefix,
+							   restSize);
+						op += restSize;
+					}
+				}
+				continue;
+			}
+
+			/* copy match within block */
+			cpy = op + length;
+
+			assert((op <= oend) && (oend - op >= 32));
+			if (unlikely(offset < 16)) {
+				LZ4_memcpy_using_offset(op, match, cpy, offset);
+			} else {
+				LZ4_wildCopy32(op, match, cpy);
+			}
+
+			op = cpy; /* wildcopy correction */
+		}
+	safe_decode:
+#endif
+
+		/* Main Loop : decode remaining sequences where output < FASTLOOP_SAFE_DISTANCE */
+		DEBUGLOG(6, "using safe decode loop");
+		while (1) {
+			assert(ip < iend);
+			token = *ip++;
+			length = token >> ML_BITS; /* literal length */
+			DEBUGLOG(7, "blockPos%6u: litLength token = %u",
+				 (unsigned)(op - (BYTE *)dst),
+				 (unsigned)length);
+
+			/* A two-stage shortcut for the most common case:
+             * 1) If the literal length is 0..14, and there is enough space,
+             * enter the shortcut and copy 16 bytes on behalf of the literals
+             * (in the fast mode, only 8 bytes can be safely copied this way).
+             * 2) Further if the match length is 4..18, copy 18 bytes in a similar
+             * manner; but we ensure that there's enough space in the output for
+             * those 18 bytes earlier, upon entering the shortcut (in other words,
+             * there is a combined check for both stages).
+             */
+			if ((length != RUN_MASK)
+			    /* strictly "less than" on input, to re-enter the loop with at least one byte */
+			    && likely((ip < shortiend) & (op <= shortoend))) {
+				/* Copy the literals */
+				LZ4_memcpy(op, ip, 16);
+				op += length;
+				ip += length;
+
+				/* The second stage: prepare for match copying, decode full info.
+                 * If it doesn't work out, the info won't be wasted. */
+				length = token & ML_MASK; /* match length */
+				DEBUGLOG(
+					7,
+					"blockPos%6u: matchLength token = %u (len=%u)",
+					(unsigned)(op - (BYTE *)dst),
+					(unsigned)length, (unsigned)length + 4);
+				offset = LZ4_readLE16(ip);
+				ip += 2;
+				match = op - offset;
+				assert(match <= op); /* check overflow */
+
+				/* Do not deal with overlapping matches. */
+				if ((length != ML_MASK) && (offset >= 8) &&
+				    (dict == withPrefix64k ||
+				     match >= lowPrefix)) {
+					/* Copy the match. */
+					LZ4_memcpy(op + 0, match + 0, 8);
+					LZ4_memcpy(op + 8, match + 8, 8);
+					LZ4_memcpy(op + 16, match + 16, 2);
+					op += length + MINMATCH;
+					/* Both stages worked, load the next token. */
+					continue;
+				}
+
+				/* The second stage didn't work out, but the info is ready.
+                 * Propel it right to the point of match copying. */
+				goto _copy_match;
+			}
+
+			/* decode literal length */
+			if (length == RUN_MASK) {
+				size_t const addl = read_variable_length(
+					&ip, iend - RUN_MASK, 1);
+				if (addl == rvl_error) {
+					goto _output_error;
+				}
+				length += addl;
+				if (unlikely((uptrval)(op) + length <
+					     (uptrval)(op))) {
+					goto _output_error;
+				} /* overflow detection */
+				if (unlikely((uptrval)(ip) + length <
+					     (uptrval)(ip))) {
+					goto _output_error;
+				} /* overflow detection */
+			}
+
+#if LZ4_FAST_DEC_LOOP
+		safe_literal_copy:
+#endif
+			/* copy literals */
+			cpy = op + length;
+
+			LZ4_STATIC_ASSERT(MFLIMIT >= WILDCOPYLENGTH);
+			if ((cpy > oend - MFLIMIT) ||
+			    (ip + length > iend - (2 + 1 + LASTLITERALS))) {
+				/* We've either hit the input parsing restriction or the output parsing restriction.
+                 * In the normal scenario, decoding a full block, it must be the last sequence,
+                 * otherwise it's an error (invalid input or dimensions).
+                 * In partialDecoding scenario, it's necessary to ensure there is no buffer overflow.
+                 */
+				if (partialDecoding) {
+					/* Since we are partial decoding we may be in this block because of the output parsing
+                     * restriction, which is not valid since the output buffer is allowed to be undersized.
+                     */
+					DEBUGLOG(
+						7,
+						"partialDecoding: copying literals, close to input or output end")
+					DEBUGLOG(
+						7,
+						"partialDecoding: literal length = %u",
+						(unsigned)length);
+					DEBUGLOG(
+						7,
+						"partialDecoding: remaining space in dstBuffer : %i",
+						(int)(oend - op));
+					DEBUGLOG(
+						7,
+						"partialDecoding: remaining space in srcBuffer : %i",
+						(int)(iend - ip));
+					/* Finishing in the middle of a literals segment,
+                     * due to lack of input.
+                     */
+					if (ip + length > iend) {
+						length = (size_t)(iend - ip);
+						cpy = op + length;
+					}
+					/* Finishing in the middle of a literals segment,
+                     * due to lack of output space.
+                     */
+					if (cpy > oend) {
+						cpy = oend;
+						assert(op <= oend);
+						length = (size_t)(oend - op);
+					}
+				} else {
+					/* We must be on the last sequence (or invalid) because of the parsing limitations
+                      * so check that we exactly consume the input and don't overrun the output buffer.
+                      */
+					if ((ip + length != iend) ||
+					    (cpy > oend)) {
+						DEBUGLOG(
+							5,
+							"should have been last run of literals")
+						DEBUGLOG(
+							5,
+							"ip(%p) + length(%i) = %p != iend (%p)",
+							ip, (int)length,
+							ip + length, iend);
+						DEBUGLOG(
+							5,
+							"or cpy(%p) > (oend-MFLIMIT)(%p)",
+							cpy, oend - MFLIMIT);
+						DEBUGLOG(
+							5,
+							"after writing %u bytes / %i bytes available",
+							(unsigned)(op -
+								   (BYTE *)dst),
+							outputSize);
+						goto _output_error;
+					}
+				}
+				LZ4_memmove(
+					op, ip,
+					length); /* supports overlapping memory regions, for in-place decompression scenarios */
+				ip += length;
+				op += length;
+				/* Necessarily EOF when !partialDecoding.
+                 * When partialDecoding, it is EOF if we've either
+                 * filled the output buffer or
+                 * can't proceed with reading an offset for following match.
+                 */
+				if (!partialDecoding || (cpy == oend) ||
+				    (ip >= (iend - 2))) {
+					break;
+				}
+			} else {
+				LZ4_wildCopy8(
+					op, ip,
+					cpy); /* can overwrite up to 8 bytes beyond cpy */
+				ip += length;
+				op = cpy;
+			}
+
+			/* get offset */
+			offset = LZ4_readLE16(ip);
+			ip += 2;
+			match = op - offset;
+
+			/* get matchlength */
+			length = token & ML_MASK;
+			DEBUGLOG(7, "blockPos%6u: matchLength token = %u",
+				 (unsigned)(op - (BYTE *)dst),
+				 (unsigned)length);
+
+		_copy_match:
+			if (length == ML_MASK) {
+				size_t const addl = read_variable_length(
+					&ip, iend - LASTLITERALS + 1, 0);
+				if (addl == rvl_error) {
+					goto _output_error;
+				}
+				length += addl;
+				if (unlikely((uptrval)(op) + length <
+					     (uptrval)op))
+					goto _output_error; /* overflow detection */
+			}
+			length += MINMATCH;
+
+#if LZ4_FAST_DEC_LOOP
+		safe_match_copy:
+#endif
+			if ((checkOffset) &&
+			    (unlikely(match + dictSize < lowPrefix)))
+				goto _output_error; /* Error : offset outside buffers */
+			/* match starting within external dictionary */
+			if ((dict == usingExtDict) && (match < lowPrefix)) {
+				assert(dictEnd != NULL);
+				if (unlikely(op + length >
+					     oend - LASTLITERALS)) {
+					if (partialDecoding)
+						length = MIN(
+							length,
+							(size_t)(oend - op));
+					else
+						goto _output_error; /* doesn't respect parsing restriction */
+				}
+
+				if (length <= (size_t)(lowPrefix - match)) {
+					/* match fits entirely within external dictionary : just copy */
+					LZ4_memmove(op,
+						    dictEnd -
+							    (lowPrefix - match),
+						    length);
+					op += length;
+				} else {
+					/* match stretches into both external dictionary and current block */
+					size_t const copySize =
+						(size_t)(lowPrefix - match);
+					size_t const restSize =
+						length - copySize;
+					LZ4_memcpy(op, dictEnd - copySize,
+						   copySize);
+					op += copySize;
+					if (restSize >
+					    (size_t)(op -
+						     lowPrefix)) { /* overlap copy */
+						BYTE *const endOfMatch =
+							op + restSize;
+						const BYTE *copyFrom =
+							lowPrefix;
+						while (op < endOfMatch)
+							*op++ = *copyFrom++;
+					} else {
+						LZ4_memcpy(op, lowPrefix,
+							   restSize);
+						op += restSize;
+					}
+				}
+				continue;
+			}
+			assert(match >= lowPrefix);
+
+			/* copy match within block */
+			cpy = op + length;
+
+			/* partialDecoding : may end anywhere within the block */
+			assert(op <= oend);
+			if (partialDecoding &&
+			    (cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
+				size_t const mlen =
+					MIN(length, (size_t)(oend - op));
+				const BYTE *const matchEnd = match + mlen;
+				BYTE *const copyEnd = op + mlen;
+				if (matchEnd > op) { /* overlap copy */
+					while (op < copyEnd) {
+						*op++ = *match++;
+					}
+				} else {
+					LZ4_memcpy(op, match, mlen);
+				}
+				op = copyEnd;
+				if (op == oend) {
+					break;
+				}
+				continue;
+			}
+
+			if (unlikely(offset < 8)) {
+				LZ4_write32(
+					op,
+					0); /* silence msan warning when offset==0 */
+				op[0] = match[0];
+				op[1] = match[1];
+				op[2] = match[2];
+				op[3] = match[3];
+				match += inc32table[offset];
+				LZ4_memcpy(op + 4, match, 4);
+				match -= dec64table[offset];
+			} else {
+				LZ4_memcpy(op, match, 8);
+				match += 8;
+			}
+			op += 8;
+
+			if (unlikely(cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
+				BYTE *const oCopyLimit =
+					oend - (WILDCOPYLENGTH - 1);
+				if (cpy > oend - LASTLITERALS) {
+					goto _output_error;
+				} /* Error : last LASTLITERALS bytes must be literals (uncompressed) */
+				if (op < oCopyLimit) {
+					LZ4_wildCopy8(op, match, oCopyLimit);
+					match += oCopyLimit - op;
+					op = oCopyLimit;
+				}
+				while (op < cpy) {
+					*op++ = *match++;
+				}
+			} else {
+				LZ4_memcpy(op, match, 8);
+				if (length > 16) {
+					LZ4_wildCopy8(op + 8, match + 8, cpy);
+				}
+			}
+			op = cpy; /* wildcopy correction */
+		}
+
+		/* end of decoding */
+		DEBUGLOG(5, "decoded %i bytes", (int)(((char *)op) - dst));
+		return (int)(((char *)op) -
+			     dst); /* Nb of output bytes decoded */
+
+		/* Overflow error detected */
+	_output_error:
+		return (int)(-(((const char *)ip) - src)) - 1;
+	}
+}
+
+/*===== Instantiate the API decoding functions. =====*/
+
+LZ4_FORCE_INLINE int
+LZ4_decompress_generic(const char *const src, char *const dst, int srcSize,
+		       /*
+			* If endOnInput == endOnInputSize,
+			* this value is `dstCapacity`
+			*/
+		       int outputSize,
+		       /* full, partial */
+		       earlyEnd_directive partialDecoding,
+		       /* noDict, withPrefix64k, usingExtDict */
+		       dict_directive dict,
+		       /* always <= dst, == dst when no prefix */
+		       const BYTE *const lowPrefix,
+		       /* only if dict == usingExtDict */
+		       const BYTE *const dictStart,
+		       /* note : = 0 if noDict */
+		       const size_t dictSize)
+{
+	return __LZ4_decompress_generic(src, dst, (const BYTE *)src,
+					(BYTE *)dst, srcSize, outputSize,
+					partialDecoding, dict, lowPrefix,
+					dictStart, dictSize);
+}
+
+LZ4_FORCE_O2
+int LZ4_decompress_safe(const char *source, char *dest, int compressedSize,
+			int maxDecompressedSize)
+{
+	return LZ4_decompress_generic(source, dest, compressedSize,
+				      maxDecompressedSize, decode_full_block,
+				      noDict, (BYTE *)dest, NULL, 0);
+}
+EXPORT_SYMBOL(LZ4_decompress_safe);
+
+LZ4_FORCE_O2
+int LZ4_decompress_safe_partial(const char *src, char *dst, int compressedSize,
+				int targetOutputSize, int dstCapacity)
+{
+	dstCapacity = MIN(targetOutputSize, dstCapacity);
+	return LZ4_decompress_generic(src, dst, compressedSize, dstCapacity,
+				      partial_decode, noDict, (BYTE *)dst, NULL,
+				      0);
+}
+EXPORT_SYMBOL(LZ4_decompress_safe_partial);
+
+LZ4_FORCE_O2
+int LZ4_decompress_fast(const char *source, char *dest, int originalSize)
+{
+	DEBUGLOG(5, "LZ4_decompress_fast");
+	return LZ4_decompress_unsafe_generic((const BYTE *)source, (BYTE *)dest,
+					     originalSize, 0, NULL, 0);
+}
+EXPORT_SYMBOL(LZ4_decompress_fast);
+
+/*===== Instantiate a few more decoding cases, used more than once. =====*/
+
+LZ4_FORCE_O2 /* Exported, an obsolete API function. */
+	int
+	LZ4_decompress_safe_withPrefix64k(const char *source, char *dest,
+					  int compressedSize, int maxOutputSize)
+{
+	return LZ4_decompress_generic(source, dest, compressedSize,
+				      maxOutputSize, decode_full_block,
+				      withPrefix64k, (BYTE *)dest - 64 KB, NULL,
+				      0);
+}
+
+LZ4_FORCE_O2
+static int LZ4_decompress_safe_partial_withPrefix64k(const char *source,
+						     char *dest,
+						     int compressedSize,
+						     int targetOutputSize,
+						     int dstCapacity)
+{
+	dstCapacity = MIN(targetOutputSize, dstCapacity);
+	return LZ4_decompress_generic(source, dest, compressedSize, dstCapacity,
+				      partial_decode, withPrefix64k,
+				      (BYTE *)dest - 64 KB, NULL, 0);
+}
+
+/* Another obsolete API function, paired with the previous one. */
+int LZ4_decompress_fast_withPrefix64k(const char *source, char *dest,
+				      int originalSize)
+{
+	return LZ4_decompress_unsafe_generic((const BYTE *)source, (BYTE *)dest,
+					     originalSize, 64 KB, NULL, 0);
+}
+
+LZ4_FORCE_O2
+static int LZ4_decompress_safe_withSmallPrefix(const char *source, char *dest,
+					       int compressedSize,
+					       int maxOutputSize,
+					       size_t prefixSize)
+{
+	return LZ4_decompress_generic(source, dest, compressedSize,
+				      maxOutputSize, decode_full_block, noDict,
+				      (BYTE *)dest - prefixSize, NULL, 0);
+}
+
+LZ4_FORCE_O2
+static int LZ4_decompress_safe_partial_withSmallPrefix(
+	const char *source, char *dest, int compressedSize,
+	int targetOutputSize, int dstCapacity, size_t prefixSize)
+{
+	dstCapacity = MIN(targetOutputSize, dstCapacity);
+	return LZ4_decompress_generic(source, dest, compressedSize, dstCapacity,
+				      partial_decode, noDict,
+				      (BYTE *)dest - prefixSize, NULL, 0);
+}
+
+LZ4_FORCE_O2
+int LZ4_decompress_safe_forceExtDict(const char *source, char *dest,
+				     int compressedSize, int maxOutputSize,
+				     const void *dictStart, size_t dictSize)
+{
+	DEBUGLOG(5, "LZ4_decompress_safe_forceExtDict");
+	return LZ4_decompress_generic(source, dest, compressedSize,
+				      maxOutputSize, decode_full_block,
+				      usingExtDict, (BYTE *)dest,
+				      (const BYTE *)dictStart, dictSize);
+}
+
+LZ4_FORCE_O2
+int LZ4_decompress_safe_partial_forceExtDict(const char *source, char *dest,
+					     int compressedSize,
+					     int targetOutputSize,
+					     int dstCapacity,
+					     const void *dictStart,
+					     size_t dictSize)
+{
+	dstCapacity = MIN(targetOutputSize, dstCapacity);
+	return LZ4_decompress_generic(source, dest, compressedSize, dstCapacity,
+				      partial_decode, usingExtDict,
+				      (BYTE *)dest, (const BYTE *)dictStart,
+				      dictSize);
+}
+
+LZ4_FORCE_O2
+static int LZ4_decompress_fast_extDict(const char *source, char *dest,
+				       int originalSize, const void *dictStart,
+				       size_t dictSize)
+{
+	return LZ4_decompress_unsafe_generic((const BYTE *)source, (BYTE *)dest,
+					     originalSize, 0,
+					     (const BYTE *)dictStart, dictSize);
+}
+
+/* The "double dictionary" mode, for use with e.g. ring buffers: the first part
+ * of the dictionary is passed as prefix, and the second via dictStart + dictSize.
+ * These routines are used only once, in LZ4_decompress_*_continue().
+ */
+LZ4_FORCE_INLINE
+int LZ4_decompress_safe_doubleDict(const char *source, char *dest,
+				   int compressedSize, int maxOutputSize,
+				   size_t prefixSize, const void *dictStart,
+				   size_t dictSize)
+{
+	return LZ4_decompress_generic(source, dest, compressedSize,
+				      maxOutputSize, decode_full_block,
+				      usingExtDict, (BYTE *)dest - prefixSize,
+				      (const BYTE *)dictStart, dictSize);
+}
+
+/*===== streaming decompression functions =====*/
+
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+LZ4_streamDecode_t *LZ4_createStreamDecode(void)
+{
+	LZ4_STATIC_ASSERT(sizeof(LZ4_streamDecode_t) >=
+			  sizeof(LZ4_streamDecode_t_internal));
+	return (LZ4_streamDecode_t *)ALLOC_AND_ZERO(sizeof(LZ4_streamDecode_t));
+}
+
+int LZ4_freeStreamDecode(LZ4_streamDecode_t *LZ4_stream)
+{
+	if (LZ4_stream == NULL) {
+		return 0;
+	} /* support free on NULL */
+	FREEMEM(LZ4_stream);
+	return 0;
+}
+#endif
+
+/*! LZ4_setStreamDecode() :
+ *  Use this function to instruct where to find the dictionary.
+ *  This function is not necessary if previous data is still available where it was decoded.
+ *  Loading a size of 0 is allowed (same effect as no dictionary).
+ * @return : 1 if OK, 0 if error
+ */
+int LZ4_setStreamDecode(LZ4_streamDecode_t *LZ4_streamDecode,
+			const char *dictionary, int dictSize)
+{
+	LZ4_streamDecode_t_internal *lz4sd =
+		&LZ4_streamDecode->internal_donotuse;
+	lz4sd->prefixSize = (size_t)dictSize;
+	if (dictSize) {
+		assert(dictionary != NULL);
+		lz4sd->prefixEnd = (const BYTE *)dictionary + dictSize;
+	} else {
+		lz4sd->prefixEnd = (const BYTE *)dictionary;
+	}
+	lz4sd->externalDict = NULL;
+	lz4sd->extDictSize = 0;
+	return 1;
+}
+EXPORT_SYMBOL(LZ4_setStreamDecode);
+
+/*! LZ4_decoderRingBufferSize() :
+ *  when setting a ring buffer for streaming decompression (optional scenario),
+ *  provides the minimum size of this ring buffer
+ *  to be compatible with any source respecting maxBlockSize condition.
+ *  Note : in a ring buffer scenario,
+ *  blocks are presumed decompressed next to each other.
+ *  When not enough space remains for next block (remainingSize < maxBlockSize),
+ *  decoding resumes from beginning of ring buffer.
+ * @return : minimum ring buffer size,
+ *           or 0 if there is an error (invalid maxBlockSize).
+ */
+int LZ4_decoderRingBufferSize(int maxBlockSize)
+{
+	if (maxBlockSize < 0)
+		return 0;
+	if (maxBlockSize > LZ4_MAX_INPUT_SIZE)
+		return 0;
+	if (maxBlockSize < 16)
+		maxBlockSize = 16;
+	return LZ4_DECODER_RING_BUFFER_SIZE(maxBlockSize);
+}
+
+/*
+*_continue() :
+    These decoding functions allow decompression of multiple blocks in "streaming" mode.
+    Previously decoded blocks must still be available at the memory position where they were decoded.
+    If it's not possible, save the relevant part of decoded data into a safe buffer,
+    and indicate where it stands using LZ4_setStreamDecode()
+*/
+LZ4_FORCE_O2
+int LZ4_decompress_safe_continue(LZ4_streamDecode_t *LZ4_streamDecode,
+				 const char *source, char *dest,
+				 int compressedSize, int maxOutputSize)
+{
+	LZ4_streamDecode_t_internal *lz4sd =
+		&LZ4_streamDecode->internal_donotuse;
+	int result;
+
+	if (lz4sd->prefixSize == 0) {
+		/* The first call, no dictionary yet. */
+		assert(lz4sd->extDictSize == 0);
+#if defined(CONFIG_ARM64) && defined(CONFIG_KERNEL_MODE_NEON)
+		result = LZ4_arm64_decompress_safe(source, dest, compressedSize,
+						   maxOutputSize, false);
+#else
+		result = LZ4_decompress_safe(source, dest, compressedSize,
+					     maxOutputSize);
+#endif
+		if (result <= 0)
+			return result;
+		lz4sd->prefixSize = (size_t)result;
+		lz4sd->prefixEnd = (BYTE *)dest + result;
+	} else if (lz4sd->prefixEnd == (BYTE *)dest) {
+		/* They're rolling the current segment. */
+		if (lz4sd->prefixSize >= 64 KB - 1)
+			result = LZ4_decompress_safe_withPrefix64k(
+				source, dest, compressedSize, maxOutputSize);
+		else if (lz4sd->extDictSize == 0)
+			result = LZ4_decompress_safe_withSmallPrefix(
+				source, dest, compressedSize, maxOutputSize,
+				lz4sd->prefixSize);
+		else
+			result = LZ4_decompress_safe_doubleDict(
+				source, dest, compressedSize, maxOutputSize,
+				lz4sd->prefixSize, lz4sd->externalDict,
+				lz4sd->extDictSize);
+		if (result <= 0)
+			return result;
+		lz4sd->prefixSize += (size_t)result;
+		lz4sd->prefixEnd += result;
+	} else {
+		/* The buffer wraps around, or they're switching to another buffer. */
+		lz4sd->extDictSize = lz4sd->prefixSize;
+		lz4sd->externalDict = lz4sd->prefixEnd - lz4sd->extDictSize;
+		result = LZ4_decompress_safe_forceExtDict(
+			source, dest, compressedSize, maxOutputSize,
+			lz4sd->externalDict, lz4sd->extDictSize);
+		if (result <= 0)
+			return result;
+		lz4sd->prefixSize = (size_t)result;
+		lz4sd->prefixEnd = (BYTE *)dest + result;
+	}
+
+	return result;
+}
+EXPORT_SYMBOL(LZ4_decompress_safe_continue);
+
+LZ4_FORCE_O2 ssize_t LZ4_arm64_decompress_safe_partial(const void *source,
+						       void *dest,
+						       size_t inputSize,
+						       size_t outputSize,
+						       bool dip)
+{
+	uint8_t *dstPtr = dest;
+	const uint8_t *srcPtr = source;
+	ssize_t ret;
+
+#ifdef __ARCH_HAS_LZ4_ACCELERATOR
+	/* Go fast if we can, keeping away from the end of buffers */
+	if (outputSize > LZ4_FAST_MARGIN && inputSize > LZ4_FAST_MARGIN &&
+	    lz4_decompress_accel_enable()) {
+		ret = lz4_decompress_asm(
+			&dstPtr, dest, dest + outputSize - LZ4_FAST_MARGIN,
+			&srcPtr, source + inputSize - LZ4_FAST_MARGIN, dip);
+		if (ret)
+			return -EIO;
+	}
+#endif
+	/* Finish in safe */
+	return __LZ4_decompress_generic(source, dest, srcPtr, dstPtr, inputSize,
+					outputSize, partial_decode, noDict,
+					(BYTE *)dest, NULL, 0);
+}
+EXPORT_SYMBOL(LZ4_arm64_decompress_safe_partial);
+
+LZ4_FORCE_O2 ssize_t LZ4_arm64_decompress_safe(const void *source, void *dest,
+					       size_t inputSize,
+					       size_t outputSize, bool dip)
+{
+	uint8_t *dstPtr = dest;
+	const uint8_t *srcPtr = source;
+	ssize_t ret;
+
+#ifdef __ARCH_HAS_LZ4_ACCELERATOR
+	/* Go fast if we can, keeping away from the end of buffers */
+	if (outputSize > LZ4_FAST_MARGIN && inputSize > LZ4_FAST_MARGIN &&
+	    lz4_decompress_accel_enable()) {
+		ret = lz4_decompress_asm(
+			&dstPtr, dest, dest + outputSize - LZ4_FAST_MARGIN,
+			&srcPtr, source + inputSize - LZ4_FAST_MARGIN, dip);
+		if (ret)
+			return -EIO;
+	}
+#endif
+	/* Finish in safe */
+	return __LZ4_decompress_generic(source, dest, srcPtr, dstPtr, inputSize,
+					outputSize, decode_full_block, noDict,
+					(BYTE *)dest, NULL, 0);
+}
+EXPORT_SYMBOL(LZ4_arm64_decompress_safe);
+
+LZ4_FORCE_O2 int
+LZ4_decompress_fast_continue(LZ4_streamDecode_t *LZ4_streamDecode,
+			     const char *source, char *dest, int originalSize)
+{
+	LZ4_streamDecode_t_internal *const lz4sd =
+		(assert(LZ4_streamDecode != NULL),
+		 &LZ4_streamDecode->internal_donotuse);
+	int result;
+
+	DEBUGLOG(5, "LZ4_decompress_fast_continue (toDecodeSize=%i)",
+		 originalSize);
+	assert(originalSize >= 0);
+
+	if (lz4sd->prefixSize == 0) {
+		DEBUGLOG(5, "first invocation : no prefix nor extDict");
+		assert(lz4sd->extDictSize == 0);
+		result = LZ4_decompress_fast(source, dest, originalSize);
+		if (result <= 0)
+			return result;
+		lz4sd->prefixSize = (size_t)originalSize;
+		lz4sd->prefixEnd = (BYTE *)dest + originalSize;
+	} else if (lz4sd->prefixEnd == (BYTE *)dest) {
+		DEBUGLOG(5, "continue using existing prefix");
+		result = LZ4_decompress_unsafe_generic(
+			(const BYTE *)source, (BYTE *)dest, originalSize,
+			lz4sd->prefixSize, lz4sd->externalDict,
+			lz4sd->extDictSize);
+		if (result <= 0)
+			return result;
+		lz4sd->prefixSize += (size_t)originalSize;
+		lz4sd->prefixEnd += originalSize;
+	} else {
+		DEBUGLOG(5, "prefix becomes extDict");
+		lz4sd->extDictSize = lz4sd->prefixSize;
+		lz4sd->externalDict = lz4sd->prefixEnd - lz4sd->extDictSize;
+		result = LZ4_decompress_fast_extDict(source, dest, originalSize,
+						     lz4sd->externalDict,
+						     lz4sd->extDictSize);
+		if (result <= 0)
+			return result;
+		lz4sd->prefixSize = (size_t)originalSize;
+		lz4sd->prefixEnd = (BYTE *)dest + originalSize;
+	}
+
+	return result;
+}
+EXPORT_SYMBOL(LZ4_decompress_fast_continue);
+
+/*
+Advanced decoding functions :
+*_usingDict() :
+    These decoding functions work the same as "_continue" ones,
+    the dictionary must be explicitly provided within parameters
+*/
+
+int LZ4_decompress_safe_usingDict(const char *source, char *dest,
+				  int compressedSize, int maxOutputSize,
+				  const char *dictStart, int dictSize)
+{
+	if (dictSize == 0)
+		return LZ4_decompress_safe(source, dest, compressedSize,
+					   maxOutputSize);
+	if (dictStart + dictSize == dest) {
+		if (dictSize >= 64 KB - 1) {
+			return LZ4_decompress_safe_withPrefix64k(
+				source, dest, compressedSize, maxOutputSize);
+		}
+		assert(dictSize >= 0);
+		return LZ4_decompress_safe_withSmallPrefix(source, dest,
+							   compressedSize,
+							   maxOutputSize,
+							   (size_t)dictSize);
+	}
+	assert(dictSize >= 0);
+	return LZ4_decompress_safe_forceExtDict(source, dest, compressedSize,
+						maxOutputSize, dictStart,
+						(size_t)dictSize);
+}
+EXPORT_SYMBOL(LZ4_decompress_safe_usingDict);
+
+int LZ4_decompress_safe_partial_usingDict(const char *source, char *dest,
+					  int compressedSize,
+					  int targetOutputSize, int dstCapacity,
+					  const char *dictStart, int dictSize)
+{
+	if (dictSize == 0)
+		return LZ4_decompress_safe_partial(source, dest, compressedSize,
+						   targetOutputSize,
+						   dstCapacity);
+	if (dictStart + dictSize == dest) {
+		if (dictSize >= 64 KB - 1) {
+			return LZ4_decompress_safe_partial_withPrefix64k(
+				source, dest, compressedSize, targetOutputSize,
+				dstCapacity);
+		}
+		assert(dictSize >= 0);
+		return LZ4_decompress_safe_partial_withSmallPrefix(
+			source, dest, compressedSize, targetOutputSize,
+			dstCapacity, (size_t)dictSize);
+	}
+	assert(dictSize >= 0);
+	return LZ4_decompress_safe_partial_forceExtDict(
+		source, dest, compressedSize, targetOutputSize, dstCapacity,
+		dictStart, (size_t)dictSize);
+}
+
+int LZ4_decompress_fast_usingDict(const char *source, char *dest,
+				  int originalSize, const char *dictStart,
+				  int dictSize)
+{
+	if (dictSize == 0 || dictStart + dictSize == dest)
+		return LZ4_decompress_unsafe_generic((const BYTE *)source,
+						     (BYTE *)dest, originalSize,
+						     (size_t)dictSize, NULL, 0);
+	assert(dictSize >= 0);
+	return LZ4_decompress_fast_extDict(source, dest, originalSize,
+					   dictStart, (size_t)dictSize);
+}
+EXPORT_SYMBOL(LZ4_decompress_fast_usingDict);
+
+/*
+These decompression functions are deprecated and should no longer be used.
+They are only provided here for compatibility with older user programs.
+- LZ4_uncompress is totally equivalent to LZ4_decompress_fast
+- LZ4_uncompress_unknownOutputSize is totally equivalent to LZ4_decompress_safe
+*/
+int LZ4_uncompress(const char *source, char *dest, int outputSize)
+{
+	return LZ4_decompress_fast(source, dest, outputSize);
+}
+int LZ4_uncompress_unknownOutputSize(const char *source, char *dest, int isize,
+				     int maxOutputSize)
+{
+	return LZ4_decompress_safe(source, dest, isize, maxOutputSize);
+}
+
+/* Obsolete Streaming functions */
+
+int LZ4_sizeofStreamState(void)
+{
+	return sizeof(LZ4_stream_t);
+}
+
+int LZ4_resetStreamState(void *state, char *inputBuffer)
+{
+	(void)inputBuffer;
+	LZ4_resetStream((LZ4_stream_t *)state);
+	return 0;
+}
+
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+void *LZ4_create(char *inputBuffer)
+{
+	(void)inputBuffer;
+	return LZ4_createStream();
+}
+#endif
+
+char *LZ4_slideInputBuffer(void *state)
+{
+	/* avoid const char * -> char * conversion warning */
+	return (char *)(uptrval)((LZ4_stream_t *)state)
+		->internal_donotuse.dictionary;
+}
+
+#endif /* LZ4_COMMONDEFS_ONLY */
Index: lib/lz4/lz4.h
===================================================================
diff --git a/lib/lz4/lz4.h b/lib/lz4/lz4.h
new file mode 100644
--- /dev/null	(revision b2497e4243461a835c25469028cd355bfc2e993f)
+++ b/lib/lz4/lz4.h	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -0,0 +1,984 @@
+/*
+ *  LZ4 - Fast LZ compression algorithm
+ *  Header File
+ *  Copyright (C) 2011-2023, Yann Collet.
+
+   BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)
+
+   Redistribution and use in source and binary forms, with or without
+   modification, are permitted provided that the following conditions are
+   met:
+
+       * Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+       * Redistributions in binary form must reproduce the above
+   copyright notice, this list of conditions and the following disclaimer
+   in the documentation and/or other materials provided with the
+   distribution.
+
+   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+   You can contact the author at :
+    - LZ4 homepage : http://www.lz4.org
+    - LZ4 source repository : https://github.com/lz4/lz4
+*/
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#ifndef LZ4_H_2983827168210
+#define LZ4_H_2983827168210
+
+/**
+  Introduction
+
+  LZ4 is lossless compression algorithm, providing compression speed >500 MB/s per core,
+  scalable with multi-cores CPU. It features an extremely fast decoder, with speed in
+  multiple GB/s per core, typically reaching RAM speed limits on multi-core systems.
+
+  The LZ4 compression library provides in-memory compression and decompression functions.
+  It gives full buffer control to user.
+  Compression can be done in:
+    - a single step (described as Simple Functions)
+    - a single step, reusing a context (described in Advanced Functions)
+    - unbounded multiple steps (described as Streaming compression)
+
+  lz4.h generates and decodes LZ4-compressed blocks (doc/lz4_Block_format.md).
+  Decompressing such a compressed block requires additional metadata.
+  Exact metadata depends on exact decompression function.
+  For the typical case of LZ4_decompress_safe(),
+  metadata includes block's compressed size, and maximum bound of decompressed size.
+  Each application is free to encode and pass such metadata in whichever way it wants.
+
+  lz4.h only handle blocks, it can not generate Frames.
+
+  Blocks are different from Frames (doc/lz4_Frame_format.md).
+  Frames bundle both blocks and metadata in a specified manner.
+  Embedding metadata is required for compressed data to be self-contained and portable.
+  Frame format is delivered through a companion API, declared in lz4frame.h.
+  The `lz4` CLI can only manage frames.
+*/
+
+#include <linux/compiler.h>
+#include <linux/export.h>
+#include <linux/string.h>
+
+#include "lz4armv8/lz4accel.h"
+
+#define LZ4_FORCE_INLINE static inline __attribute__((always_inline))
+
+/*^***************************************************************
+*  Export parameters
+*****************************************************************/
+/*
+*  LZ4_DLL_EXPORT :
+*  Enable exporting of functions when building a Windows DLL
+*  LZ4LIB_VISIBILITY :
+*  Control library symbols visibility.
+*/
+#ifndef LZ4LIB_VISIBILITY
+#if defined(__GNUC__) && (__GNUC__ >= 4)
+#define LZ4LIB_VISIBILITY __attribute__((visibility("default")))
+#else
+#define LZ4LIB_VISIBILITY
+#endif
+#endif
+#if defined(LZ4_DLL_EXPORT) && (LZ4_DLL_EXPORT == 1)
+#define LZ4LIB_API __declspec(dllexport) LZ4LIB_VISIBILITY
+#elif defined(LZ4_DLL_IMPORT) && (LZ4_DLL_IMPORT == 1)
+#define LZ4LIB_API                                                             \
+	__declspec(dllimport)                                                  \
+	LZ4LIB_VISIBILITY /* It isn't required but allows to generate better code, saving a function pointer load from the IAT and an indirect jump.*/
+#else
+#define LZ4LIB_API LZ4LIB_VISIBILITY
+#endif
+
+/*-************************************
+*  Reading and writing into memory
+**************************************/
+
+/**
+ * LZ4 relies on memcpy with a constant size being inlined. In freestanding
+ * environments, the compiler can't assume the implementation of memcpy() is
+ * standard compliant, so it can't apply its specialized memcpy() inlining
+ * logic. When possible, use __builtin_memcpy() to tell the compiler to analyze
+ * memcpy() as if it were standard compliant, so it can inline it in freestanding
+ * environments. This is needed when decompressing the Linux Kernel, for example.
+ */
+#define LZ4_memcpy(dst, src, size) __builtin_memcpy(dst, src, size)
+#define LZ4_memset(dst, src, size) __builtin_memset(dst, src, size)
+#define LZ4_memmove(dst, src, size) __builtin_memmove(dst, src, size)
+
+/*! LZ4_FREESTANDING :
+ *  When this macro is set to 1, it enables "freestanding mode" that is
+ *  suitable for typical freestanding environment which doesn't support
+ *  standard C library.
+ *
+ *  - LZ4_FREESTANDING is a compile-time switch.
+ *  - It requires the following macros to be defined:
+ *    LZ4_memcpy, LZ4_memmove, LZ4_memset.
+ *  - It only enables LZ4/HC functions which don't use heap.
+ *    All LZ4F_* functions are not supported.
+ *  - See tests/freestanding.c to check its basic setup.
+ */
+#if defined(LZ4_FREESTANDING) && (LZ4_FREESTANDING == 1)
+#define LZ4_HEAPMODE 1
+#define LZ4HC_HEAPMODE 1
+#define LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION 1
+#if !defined(LZ4_memcpy)
+#error "LZ4_FREESTANDING requires macro 'LZ4_memcpy'."
+#endif
+#if !defined(LZ4_memset)
+#error "LZ4_FREESTANDING requires macro 'LZ4_memset'."
+#endif
+#if !defined(LZ4_memmove)
+#error "LZ4_FREESTANDING requires macro 'LZ4_memmove'."
+#endif
+#elif !defined(LZ4_FREESTANDING)
+#define LZ4_FREESTANDING 0
+#endif
+
+/*------   Version   ------*/
+#define LZ4_VERSION_MAJOR 1 /* for breaking interface changes  */
+#define LZ4_VERSION_MINOR 10 /* for new (non-breaking) interface capabilities */
+#define LZ4_VERSION_RELEASE 0 /* for tweaks, bug-fixes, or development */
+
+#define LZ4_VERSION_NUMBER                                                     \
+	(LZ4_VERSION_MAJOR * 100 * 100 + LZ4_VERSION_MINOR * 100 +             \
+	 LZ4_VERSION_RELEASE)
+
+#define LZ4_LIB_VERSION LZ4_VERSION_MAJOR.LZ4_VERSION_MINOR.LZ4_VERSION_RELEASE
+#define LZ4_QUOTE(str) #str
+#define LZ4_EXPAND_AND_QUOTE(str) LZ4_QUOTE(str)
+#define LZ4_VERSION_STRING                                                     \
+	LZ4_EXPAND_AND_QUOTE(LZ4_LIB_VERSION) /* requires v1.7.3+ */
+
+LZ4LIB_API int LZ4_versionNumber(
+	void); /**< library version number; useful to check dll version; requires v1.3.0+ */
+LZ4LIB_API const char *LZ4_versionString(
+	void); /**< library version string; useful to check dll version; requires v1.7.5+ */
+
+/*-************************************
+*  Tuning memory usage
+**************************************/
+/*!
+ * LZ4_MEMORY_USAGE :
+ * Can be selected at compile time, by setting LZ4_MEMORY_USAGE.
+ * Memory usage formula : N->2^N Bytes (examples : 10 -> 1KB; 12 -> 4KB ; 16 -> 64KB; 20 -> 1MB)
+ * Increasing memory usage improves compression ratio, generally at the cost of speed.
+ * Reduced memory usage may improve speed at the cost of ratio, thanks to better cache locality.
+ * Default value is 14, for 16KB, which nicely fits into most L1 caches.
+ */
+#ifndef LZ4_MEMORY_USAGE
+#define LZ4_MEMORY_USAGE LZ4_MEMORY_USAGE_DEFAULT
+#endif
+
+/* These are absolute limits, they should not be changed by users */
+#define LZ4_MEMORY_USAGE_MIN 10
+#define LZ4_MEMORY_USAGE_DEFAULT 14
+#define LZ4_MEMORY_USAGE_MAX 20
+
+#if (LZ4_MEMORY_USAGE < LZ4_MEMORY_USAGE_MIN)
+#error "LZ4_MEMORY_USAGE is too small !"
+#endif
+
+#if (LZ4_MEMORY_USAGE > LZ4_MEMORY_USAGE_MAX)
+#error "LZ4_MEMORY_USAGE is too large !"
+#endif
+
+/*
+ * LZ4_ACCELERATION_DEFAULT :
+ * Select "acceleration" for LZ4_compress_fast() when parameter value <= 0
+ */
+#define LZ4_ACCELERATION_DEFAULT 1
+/*
+ * LZ4_ACCELERATION_MAX :
+ * Any "acceleration" value higher than this threshold
+ * get treated as LZ4_ACCELERATION_MAX instead (fix #876)
+ */
+#define LZ4_ACCELERATION_MAX 65537
+
+/*-************************************
+*  Simple Functions
+**************************************/
+/*! LZ4_compress_default() :
+ *  Compresses 'srcSize' bytes from buffer 'src'
+ *  into already allocated 'dst' buffer of size 'dstCapacity'.
+ *  Compression is guaranteed to succeed if 'dstCapacity' >= LZ4_compressBound(srcSize).
+ *  It also runs faster, so it's a recommended setting.
+ *  If the function cannot compress 'src' into a more limited 'dst' budget,
+ *  compression stops *immediately*, and the function result is zero.
+ *  In which case, 'dst' content is undefined (invalid).
+ *      srcSize : max supported value is LZ4_MAX_INPUT_SIZE.
+ *      dstCapacity : size of buffer 'dst' (which must be already allocated)
+ *     @return  : the number of bytes written into buffer 'dst' (necessarily <= dstCapacity)
+ *                or 0 if compression fails
+ * Note : This function is protected against buffer overflow scenarios (never writes outside 'dst' buffer, nor read outside 'source' buffer).
+ */
+LZ4LIB_API int LZ4_compress_default(const char *src, char *dst, int srcSize,
+				    int dstCapacity, void *wrkmem);
+
+/*! LZ4_decompress_safe() :
+ * @compressedSize : is the exact complete size of the compressed block.
+ * @dstCapacity : is the size of destination buffer (which must be already allocated),
+ *                presumed an upper bound of decompressed size.
+ * @return : the number of bytes decompressed into destination buffer (necessarily <= dstCapacity)
+ *           If destination buffer is not large enough, decoding will stop and output an error code (negative value).
+ *           If the source stream is detected malformed, the function will stop decoding and return a negative result.
+ * Note 1 : This function is protected against malicious data packets :
+ *          it will never writes outside 'dst' buffer, nor read outside 'source' buffer,
+ *          even if the compressed block is maliciously modified to order the decoder to do these actions.
+ *          In such case, the decoder stops immediately, and considers the compressed block malformed.
+ * Note 2 : compressedSize and dstCapacity must be provided to the function, the compressed block does not contain them.
+ *          The implementation is free to send / store / derive this information in whichever way is most beneficial.
+ *          If there is a need for a different format which bundles together both compressed data and its metadata, consider looking at lz4frame.h instead.
+ */
+LZ4LIB_API int LZ4_decompress_safe(const char *src, char *dst,
+				   int compressedSize, int dstCapacity);
+
+/*-************************************
+*  Advanced Functions
+**************************************/
+#define LZ4_MAX_INPUT_SIZE 0x7E000000 /* 2 113 929 216 bytes */
+#define LZ4_COMPRESSBOUND(isize)                                               \
+	((unsigned)(isize) > (unsigned)LZ4_MAX_INPUT_SIZE ?                    \
+		 0 :                                                           \
+		 (isize) + ((isize) / 255) + 16)
+
+/*! LZ4_compressBound() :
+    Provides the maximum size that LZ4 compression may output in a "worst case" scenario (input data not compressible)
+    This function is primarily useful for memory allocation purposes (destination buffer size).
+    Macro LZ4_COMPRESSBOUND() is also provided for compilation-time evaluation (stack memory allocation for example).
+    Note that LZ4_compress_default() compresses faster when dstCapacity is >= LZ4_compressBound(srcSize)
+        inputSize  : max supported value is LZ4_MAX_INPUT_SIZE
+        return : maximum output size in a "worst case" scenario
+              or 0, if input size is incorrect (too large or negative)
+*/
+LZ4LIB_API int LZ4_compressBound(int inputSize);
+
+/*! LZ4_compress_fast() :
+    Same as LZ4_compress_default(), but allows selection of "acceleration" factor.
+    The larger the acceleration value, the faster the algorithm, but also the lesser the compression.
+    It's a trade-off. It can be fine tuned, with each successive value providing roughly +~3% to speed.
+    An acceleration value of "1" is the same as regular LZ4_compress_default()
+    Values <= 0 will be replaced by LZ4_ACCELERATION_DEFAULT (currently == 1, see lz4.c).
+    Values > LZ4_ACCELERATION_MAX will be replaced by LZ4_ACCELERATION_MAX (currently == 65537, see lz4.c).
+*/
+LZ4LIB_API int LZ4_compress_fast(const char *src, char *dst, int srcSize,
+				 int dstCapacity, int acceleration, void *wrkmem);
+
+/*! LZ4_compress_fast_extState() :
+ *  Same as LZ4_compress_fast(), using an externally allocated memory space for its state.
+ *  Use LZ4_sizeofState() to know how much memory must be allocated,
+ *  and allocate it on 8-bytes boundaries (using `malloc()` typically).
+ *  Then, provide this buffer as `void* state` to compression function.
+ */
+LZ4LIB_API int LZ4_sizeofState(void);
+LZ4LIB_API int LZ4_compress_fast_extState(void *state, const char *src,
+					  char *dst, int srcSize,
+					  int dstCapacity, int acceleration);
+
+/*! LZ4_compress_destSize() :
+ *  Reverse the logic : compresses as much data as possible from 'src' buffer
+ *  into already allocated buffer 'dst', of size >= 'dstCapacity'.
+ *  This function either compresses the entire 'src' content into 'dst' if it's large enough,
+ *  or fill 'dst' buffer completely with as much data as possible from 'src'.
+ *  note: acceleration parameter is fixed to "default".
+ *
+ * *srcSizePtr : in+out parameter. Initially contains size of input.
+ *               Will be modified to indicate how many bytes where read from 'src' to fill 'dst'.
+ *               New value is necessarily <= input value.
+ * @return : Nb bytes written into 'dst' (necessarily <= dstCapacity)
+ *           or 0 if compression fails.
+ *
+ * Note : from v1.8.2 to v1.9.1, this function had a bug (fixed in v1.9.2+):
+ *        the produced compressed content could, in specific circumstances,
+ *        require to be decompressed into a destination buffer larger
+ *        by at least 1 byte than the content to decompress.
+ *        If an application uses `LZ4_compress_destSize()`,
+ *        it's highly recommended to update liblz4 to v1.9.2 or better.
+ *        If this can't be done or ensured,
+ *        the receiving decompression function should provide
+ *        a dstCapacity which is > decompressedSize, by at least 1 byte.
+ *        See https://github.com/lz4/lz4/issues/859 for details
+ */
+LZ4LIB_API int LZ4_compress_destSize(const char *src, char *dst,
+				     int *srcSizePtr, int targetDstSize, void *wrkmem);
+
+/*! LZ4_decompress_safe_partial() :
+ *  Decompress an LZ4 compressed block, of size 'srcSize' at position 'src',
+ *  into destination buffer 'dst' of size 'dstCapacity'.
+ *  Up to 'targetOutputSize' bytes will be decoded.
+ *  The function stops decoding on reaching this objective.
+ *  This can be useful to boost performance
+ *  whenever only the beginning of a block is required.
+ *
+ * @return : the number of bytes decoded in `dst` (necessarily <= targetOutputSize)
+ *           If source stream is detected malformed, function returns a negative result.
+ *
+ *  Note 1 : @return can be < targetOutputSize, if compressed block contains less data.
+ *
+ *  Note 2 : targetOutputSize must be <= dstCapacity
+ *
+ *  Note 3 : this function effectively stops decoding on reaching targetOutputSize,
+ *           so dstCapacity is kind of redundant.
+ *           This is because in older versions of this function,
+ *           decoding operation would still write complete sequences.
+ *           Therefore, there was no guarantee that it would stop writing at exactly targetOutputSize,
+ *           it could write more bytes, though only up to dstCapacity.
+ *           Some "margin" used to be required for this operation to work properly.
+ *           Thankfully, this is no longer necessary.
+ *           The function nonetheless keeps the same signature, in an effort to preserve API compatibility.
+ *
+ *  Note 4 : If srcSize is the exact size of the block,
+ *           then targetOutputSize can be any value,
+ *           including larger than the block's decompressed size.
+ *           The function will, at most, generate block's decompressed size.
+ *
+ *  Note 5 : If srcSize is _larger_ than block's compressed size,
+ *           then targetOutputSize **MUST** be <= block's decompressed size.
+ *           Otherwise, *silent corruption will occur*.
+ */
+LZ4LIB_API int LZ4_decompress_safe_partial(const char *src, char *dst,
+					   int srcSize, int targetOutputSize,
+					   int dstCapacity);
+
+/*-*********************************************
+*  Streaming Compression Functions
+***********************************************/
+typedef union LZ4_stream_u LZ4_stream_t; /* incomplete type (defined later) */
+
+/*!
+ Note about RC_INVOKED
+
+ - RC_INVOKED is predefined symbol of rc.exe (the resource compiler which is part of MSVC/Visual Studio).
+   https://docs.microsoft.com/en-us/windows/win32/menurc/predefined-macros
+
+ - Since rc.exe is a legacy compiler, it truncates long symbol (> 30 chars)
+   and reports warning "RC4011: identifier truncated".
+
+ - To eliminate the warning, we surround long preprocessor symbol with
+   "#if !defined(RC_INVOKED) ... #endif" block that means
+   "skip this block when rc.exe is trying to read it".
+*/
+#if !defined(                                                                  \
+	RC_INVOKED) /* https://docs.microsoft.com/en-us/windows/win32/menurc/predefined-macros */
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+LZ4LIB_API LZ4_stream_t *LZ4_createStream(void);
+LZ4LIB_API int LZ4_freeStream(LZ4_stream_t *streamPtr);
+#endif /* !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION) */
+#endif
+
+/*! LZ4_resetStream_fast() : v1.9.0+
+ *  Use this to prepare an LZ4_stream_t for a new chain of dependent blocks
+ *  (e.g., LZ4_compress_fast_continue()).
+ *
+ *  An LZ4_stream_t must be initialized once before usage.
+ *  This is automatically done when created by LZ4_createStream().
+ *  However, should the LZ4_stream_t be simply declared on stack (for example),
+ *  it's necessary to initialize it first, using LZ4_initStream().
+ *
+ *  After init, start any new stream with LZ4_resetStream_fast().
+ *  A same LZ4_stream_t can be re-used multiple times consecutively
+ *  and compress multiple streams,
+ *  provided that it starts each new stream with LZ4_resetStream_fast().
+ *
+ *  LZ4_resetStream_fast() is much faster than LZ4_initStream(),
+ *  but is not compatible with memory regions containing garbage data.
+ *
+ *  Note: it's only useful to call LZ4_resetStream_fast()
+ *        in the context of streaming compression.
+ *        The *extState* functions perform their own resets.
+ *        Invoking LZ4_resetStream_fast() before is redundant, and even counterproductive.
+ */
+LZ4LIB_API void LZ4_resetStream_fast(LZ4_stream_t *streamPtr);
+
+/*! LZ4_loadDict() :
+ *  Use this function to reference a static dictionary into LZ4_stream_t.
+ *  The dictionary must remain available during compression.
+ *  LZ4_loadDict() triggers a reset, so any previous data will be forgotten.
+ *  The same dictionary will have to be loaded on decompression side for successful decoding.
+ *  Dictionary are useful for better compression of small data (KB range).
+ *  While LZ4 itself accepts any input as dictionary, dictionary efficiency is also a topic.
+ *  When in doubt, employ the Zstandard's Dictionary Builder.
+ *  Loading a size of 0 is allowed, and is the same as reset.
+ * @return : loaded dictionary size, in bytes (note: only the last 64 KB are loaded)
+ */
+LZ4LIB_API int LZ4_loadDict(LZ4_stream_t *streamPtr, const char *dictionary,
+			    int dictSize);
+
+/*! LZ4_loadDictSlow() : v1.10.0+
+ *  Same as LZ4_loadDict(),
+ *  but uses a bit more cpu to reference the dictionary content more thoroughly.
+ *  This is expected to slightly improve compression ratio.
+ *  The extra-cpu cost is likely worth it if the dictionary is re-used across multiple sessions.
+ * @return : loaded dictionary size, in bytes (note: only the last 64 KB are loaded)
+ */
+LZ4LIB_API int LZ4_loadDictSlow(LZ4_stream_t *streamPtr, const char *dictionary,
+				int dictSize);
+
+/*! LZ4_attach_dictionary() : stable since v1.10.0
+ *
+ *  This allows efficient re-use of a static dictionary multiple times.
+ *
+ *  Rather than re-loading the dictionary buffer into a working context before
+ *  each compression, or copying a pre-loaded dictionary's LZ4_stream_t into a
+ *  working LZ4_stream_t, this function introduces a no-copy setup mechanism,
+ *  in which the working stream references @dictionaryStream in-place.
+ *
+ *  Several assumptions are made about the state of @dictionaryStream.
+ *  Currently, only states which have been prepared by LZ4_loadDict() or
+ *  LZ4_loadDictSlow() should be expected to work.
+ *
+ *  Alternatively, the provided @dictionaryStream may be NULL,
+ *  in which case any existing dictionary stream is unset.
+ *
+ *  If a dictionary is provided, it replaces any pre-existing stream history.
+ *  The dictionary contents are the only history that can be referenced and
+ *  logically immediately precede the data compressed in the first subsequent
+ *  compression call.
+ *
+ *  The dictionary will only remain attached to the working stream through the
+ *  first compression call, at the end of which it is cleared.
+ * @dictionaryStream stream (and source buffer) must remain in-place / accessible / unchanged
+ *  through the completion of the compression session.
+ *
+ *  Note: there is no equivalent LZ4_attach_*() method on the decompression side
+ *  because there is no initialization cost, hence no need to share the cost across multiple sessions.
+ *  To decompress LZ4 blocks using dictionary, attached or not,
+ *  just employ the regular LZ4_setStreamDecode() for streaming,
+ *  or the stateless LZ4_decompress_safe_usingDict() for one-shot decompression.
+ */
+LZ4LIB_API void LZ4_attach_dictionary(LZ4_stream_t *workingStream,
+				      const LZ4_stream_t *dictionaryStream);
+
+/*! LZ4_compress_fast_continue() :
+ *  Compress 'src' content using data from previously compressed blocks, for better compression ratio.
+ * 'dst' buffer must be already allocated.
+ *  If dstCapacity >= LZ4_compressBound(srcSize), compression is guaranteed to succeed, and runs faster.
+ *
+ * @return : size of compressed block
+ *           or 0 if there is an error (typically, cannot fit into 'dst').
+ *
+ *  Note 1 : Each invocation to LZ4_compress_fast_continue() generates a new block.
+ *           Each block has precise boundaries.
+ *           Each block must be decompressed separately, calling LZ4_decompress_*() with relevant metadata.
+ *           It's not possible to append blocks together and expect a single invocation of LZ4_decompress_*() to decompress them together.
+ *
+ *  Note 2 : The previous 64KB of source data is __assumed__ to remain present, unmodified, at same address in memory !
+ *
+ *  Note 3 : When input is structured as a double-buffer, each buffer can have any size, including < 64 KB.
+ *           Make sure that buffers are separated, by at least one byte.
+ *           This construction ensures that each block only depends on previous block.
+ *
+ *  Note 4 : If input buffer is a ring-buffer, it can have any size, including < 64 KB.
+ *
+ *  Note 5 : After an error, the stream status is undefined (invalid), it can only be reset or freed.
+ */
+LZ4LIB_API int LZ4_compress_fast_continue(LZ4_stream_t *streamPtr,
+					  const char *src, char *dst,
+					  int srcSize, int dstCapacity,
+					  int acceleration);
+
+/*! LZ4_saveDict() :
+ *  If last 64KB data cannot be guaranteed to remain available at its current memory location,
+ *  save it into a safer place (char* safeBuffer).
+ *  This is schematically equivalent to a memcpy() followed by LZ4_loadDict(),
+ *  but is much faster, because LZ4_saveDict() doesn't need to rebuild tables.
+ * @return : saved dictionary size in bytes (necessarily <= maxDictSize), or 0 if error.
+ */
+LZ4LIB_API int LZ4_saveDict(LZ4_stream_t *streamPtr, char *safeBuffer,
+			    int maxDictSize);
+
+/*-**********************************************
+*  Streaming Decompression Functions
+*  Bufferless synchronous API
+************************************************/
+typedef union LZ4_streamDecode_u LZ4_streamDecode_t; /* tracking context */
+
+/*! LZ4_createStreamDecode() and LZ4_freeStreamDecode() :
+ *  creation / destruction of streaming decompression tracking context.
+ *  A tracking context can be re-used multiple times.
+ */
+#if !defined(                                                                  \
+	RC_INVOKED) /* https://docs.microsoft.com/en-us/windows/win32/menurc/predefined-macros */
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+LZ4LIB_API LZ4_streamDecode_t *LZ4_createStreamDecode(void);
+LZ4LIB_API int LZ4_freeStreamDecode(LZ4_streamDecode_t *LZ4_stream);
+#endif /* !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION) */
+#endif
+
+/*! LZ4_setStreamDecode() :
+ *  An LZ4_streamDecode_t context can be allocated once and re-used multiple times.
+ *  Use this function to start decompression of a new stream of blocks.
+ *  A dictionary can optionally be set. Use NULL or size 0 for a reset order.
+ *  Dictionary is presumed stable : it must remain accessible and unmodified during next decompression.
+ * @return : 1 if OK, 0 if error
+ */
+LZ4LIB_API int LZ4_setStreamDecode(LZ4_streamDecode_t *LZ4_streamDecode,
+				   const char *dictionary, int dictSize);
+
+/*! LZ4_decoderRingBufferSize() : v1.8.2+
+ *  Note : in a ring buffer scenario (optional),
+ *  blocks are presumed decompressed next to each other
+ *  up to the moment there is not enough remaining space for next block (remainingSize < maxBlockSize),
+ *  at which stage it resumes from beginning of ring buffer.
+ *  When setting such a ring buffer for streaming decompression,
+ *  provides the minimum size of this ring buffer
+ *  to be compatible with any source respecting maxBlockSize condition.
+ * @return : minimum ring buffer size,
+ *           or 0 if there is an error (invalid maxBlockSize).
+ */
+LZ4LIB_API int LZ4_decoderRingBufferSize(int maxBlockSize);
+#define LZ4_DECODER_RING_BUFFER_SIZE(maxBlockSize)                             \
+	(65536 + 14 +                                                          \
+	 (maxBlockSize)) /* for static allocation; maxBlockSize presumed valid */
+
+/*! LZ4_decompress_safe_continue() :
+ *  This decoding function allows decompression of consecutive blocks in "streaming" mode.
+ *  The difference with the usual independent blocks is that
+ *  new blocks are allowed to find references into former blocks.
+ *  A block is an unsplittable entity, and must be presented entirely to the decompression function.
+ *  LZ4_decompress_safe_continue() only accepts one block at a time.
+ *  It's modeled after `LZ4_decompress_safe()` and behaves similarly.
+ *
+ * @LZ4_streamDecode : decompression state, tracking the position in memory of past data
+ * @compressedSize : exact complete size of one compressed block.
+ * @dstCapacity : size of destination buffer (which must be already allocated),
+ *                must be an upper bound of decompressed size.
+ * @return : number of bytes decompressed into destination buffer (necessarily <= dstCapacity)
+ *           If destination buffer is not large enough, decoding will stop and output an error code (negative value).
+ *           If the source stream is detected malformed, the function will stop decoding and return a negative result.
+ *
+ *  The last 64KB of previously decoded data *must* remain available and unmodified
+ *  at the memory position where they were previously decoded.
+ *  If less than 64KB of data has been decoded, all the data must be present.
+ *
+ *  Special : if decompression side sets a ring buffer, it must respect one of the following conditions :
+ *  - Decompression buffer size is _at least_ LZ4_decoderRingBufferSize(maxBlockSize).
+ *    maxBlockSize is the maximum size of any single block. It can have any value > 16 bytes.
+ *    In which case, encoding and decoding buffers do not need to be synchronized.
+ *    Actually, data can be produced by any source compliant with LZ4 format specification, and respecting maxBlockSize.
+ *  - Synchronized mode :
+ *    Decompression buffer size is _exactly_ the same as compression buffer size,
+ *    and follows exactly same update rule (block boundaries at same positions),
+ *    and decoding function is provided with exact decompressed size of each block (exception for last block of the stream),
+ *    _then_ decoding & encoding ring buffer can have any size, including small ones ( < 64 KB).
+ *  - Decompression buffer is larger than encoding buffer, by a minimum of maxBlockSize more bytes.
+ *    In which case, encoding and decoding buffers do not need to be synchronized,
+ *    and encoding ring buffer can have any size, including small ones ( < 64 KB).
+ *
+ *  Whenever these conditions are not possible,
+ *  save the last 64KB of decoded data into a safe buffer where it can't be modified during decompression,
+ *  then indicate where this data is saved using LZ4_setStreamDecode(), before decompressing next block.
+*/
+LZ4LIB_API int
+LZ4_decompress_safe_continue(LZ4_streamDecode_t *LZ4_streamDecode,
+			     const char *src, char *dst, int srcSize,
+			     int dstCapacity);
+
+LZ4LIB_API ssize_t LZ4_arm64_decompress_safe_partial(const void *source,
+						     void *dest,
+						     size_t inputSize,
+						     size_t outputSize,
+						     bool dip);
+
+LZ4LIB_API ssize_t LZ4_arm64_decompress_safe(const void *source, void *dest,
+					     size_t inputSize,
+					     size_t outputSize, bool dip);
+
+/*! LZ4_decompress_safe_usingDict() :
+ *  Works the same as
+ *  a combination of LZ4_setStreamDecode() followed by LZ4_decompress_safe_continue()
+ *  However, it's stateless: it doesn't need any LZ4_streamDecode_t state.
+ *  Dictionary is presumed stable : it must remain accessible and unmodified during decompression.
+ *  Performance tip : Decompression speed can be substantially increased
+ *                    when dst == dictStart + dictSize.
+ */
+LZ4LIB_API int LZ4_decompress_safe_usingDict(const char *src, char *dst,
+					     int srcSize, int dstCapacity,
+					     const char *dictStart,
+					     int dictSize);
+
+/*! LZ4_decompress_safe_partial_usingDict() :
+ *  Behaves the same as LZ4_decompress_safe_partial()
+ *  with the added ability to specify a memory segment for past data.
+ *  Performance tip : Decompression speed can be substantially increased
+ *                    when dst == dictStart + dictSize.
+ */
+LZ4LIB_API int LZ4_decompress_safe_partial_usingDict(
+	const char *src, char *dst, int compressedSize, int targetOutputSize,
+	int maxOutputSize, const char *dictStart, int dictSize);
+
+#endif /* LZ4_H_2983827168210 */
+
+/*^*************************************
+ * !!!!!!   STATIC LINKING ONLY   !!!!!!
+ ***************************************/
+
+/*-****************************************************************************
+ * Experimental section
+ *
+ * Symbols declared in this section must be considered unstable. Their
+ * signatures or semantics may change, or they may be removed altogether in the
+ * future. They are therefore only safe to depend on when the caller is
+ * statically linked against the library.
+ *
+ * To protect against unsafe usage, not only are the declarations guarded,
+ * the definitions are hidden by default
+ * when building LZ4 as a shared/dynamic library.
+ *
+ * In order to access these declarations,
+ * define LZ4_STATIC_LINKING_ONLY in your application
+ * before including LZ4's headers.
+ *
+ * In order to make their implementations accessible dynamically, you must
+ * define LZ4_PUBLISH_STATIC_FUNCTIONS when building the LZ4 library.
+ ******************************************************************************/
+
+#ifdef LZ4_STATIC_LINKING_ONLY
+
+#ifndef LZ4_STATIC_3504398509
+#define LZ4_STATIC_3504398509
+
+#ifdef LZ4_PUBLISH_STATIC_FUNCTIONS
+#define LZ4LIB_STATIC_API LZ4LIB_API
+#else
+#define LZ4LIB_STATIC_API
+#endif
+
+/*! LZ4_compress_fast_extState_fastReset() :
+ *  A variant of LZ4_compress_fast_extState().
+ *
+ *  Using this variant avoids an expensive initialization step.
+ *  It is only safe to call if the state buffer is known to be correctly initialized already
+ *  (see above comment on LZ4_resetStream_fast() for a definition of "correctly initialized").
+ *  From a high level, the difference is that
+ *  this function initializes the provided state with a call to something like LZ4_resetStream_fast()
+ *  while LZ4_compress_fast_extState() starts with a call to LZ4_resetStream().
+ */
+LZ4LIB_STATIC_API int
+LZ4_compress_fast_extState_fastReset(void *state, const char *src, char *dst,
+				     int srcSize, int dstCapacity,
+				     int acceleration);
+
+/*! LZ4_compress_destSize_extState() : introduced in v1.10.0
+ *  Same as LZ4_compress_destSize(), but using an externally allocated state.
+ *  Also: exposes @acceleration
+ */
+int LZ4_compress_destSize_extState(void *state, const char *src, char *dst,
+				   int *srcSizePtr, int targetDstSize,
+				   int acceleration);
+
+/*! In-place compression and decompression
+ *
+ * It's possible to have input and output sharing the same buffer,
+ * for highly constrained memory environments.
+ * In both cases, it requires input to lay at the end of the buffer,
+ * and decompression to start at beginning of the buffer.
+ * Buffer size must feature some margin, hence be larger than final size.
+ *
+ * |<------------------------buffer--------------------------------->|
+ *                             |<-----------compressed data--------->|
+ * |<-----------decompressed size------------------>|
+ *                                                  |<----margin---->|
+ *
+ * This technique is more useful for decompression,
+ * since decompressed size is typically larger,
+ * and margin is short.
+ *
+ * In-place decompression will work inside any buffer
+ * which size is >= LZ4_DECOMPRESS_INPLACE_BUFFER_SIZE(decompressedSize).
+ * This presumes that decompressedSize > compressedSize.
+ * Otherwise, it means compression actually expanded data,
+ * and it would be more efficient to store such data with a flag indicating it's not compressed.
+ * This can happen when data is not compressible (already compressed, or encrypted).
+ *
+ * For in-place compression, margin is larger, as it must be able to cope with both
+ * history preservation, requiring input data to remain unmodified up to LZ4_DISTANCE_MAX,
+ * and data expansion, which can happen when input is not compressible.
+ * As a consequence, buffer size requirements are much higher,
+ * and memory savings offered by in-place compression are more limited.
+ *
+ * There are ways to limit this cost for compression :
+ * - Reduce history size, by modifying LZ4_DISTANCE_MAX.
+ *   Note that it is a compile-time constant, so all compressions will apply this limit.
+ *   Lower values will reduce compression ratio, except when input_size < LZ4_DISTANCE_MAX,
+ *   so it's a reasonable trick when inputs are known to be small.
+ * - Require the compressor to deliver a "maximum compressed size".
+ *   This is the `dstCapacity` parameter in `LZ4_compress*()`.
+ *   When this size is < LZ4_COMPRESSBOUND(inputSize), then compression can fail,
+ *   in which case, the return code will be 0 (zero).
+ *   The caller must be ready for these cases to happen,
+ *   and typically design a backup scheme to send data uncompressed.
+ * The combination of both techniques can significantly reduce
+ * the amount of margin required for in-place compression.
+ *
+ * In-place compression can work in any buffer
+ * which size is >= (maxCompressedSize)
+ * with maxCompressedSize == LZ4_COMPRESSBOUND(srcSize) for guaranteed compression success.
+ * LZ4_COMPRESS_INPLACE_BUFFER_SIZE() depends on both maxCompressedSize and LZ4_DISTANCE_MAX,
+ * so it's possible to reduce memory requirements by playing with them.
+ */
+
+#define LZ4_DECOMPRESS_INPLACE_MARGIN(compressedSize)                          \
+	(((compressedSize) >> 8) + 32)
+#define LZ4_DECOMPRESS_INPLACE_BUFFER_SIZE(decompressedSize)                   \
+	((decompressedSize) +                                                  \
+	 LZ4_DECOMPRESS_INPLACE_MARGIN(                                        \
+		 decompressedSize)) /**< note: presumes that compressedSize < decompressedSize. note2: margin is overestimated a bit, since it could use compressedSize instead */
+
+#ifndef LZ4_DISTANCE_MAX /* history window size; can be user-defined at compile time */
+#define LZ4_DISTANCE_MAX 65535 /* set to maximum value by default */
+#endif
+
+#define LZ4_COMPRESS_INPLACE_MARGIN                                            \
+	(LZ4_DISTANCE_MAX +                                                    \
+	 32) /* LZ4_DISTANCE_MAX can be safely replaced by srcSize when it's smaller */
+#define LZ4_COMPRESS_INPLACE_BUFFER_SIZE(maxCompressedSize)                    \
+	((maxCompressedSize) +                                                 \
+	 LZ4_COMPRESS_INPLACE_MARGIN) /**< maxCompressedSize is generally LZ4_COMPRESSBOUND(inputSize), but can be set to any lower value, with the risk that compression can fail (return code 0(zero)) */
+
+#endif /* LZ4_STATIC_3504398509 */
+#endif /* LZ4_STATIC_LINKING_ONLY */
+
+#ifndef LZ4_H_98237428734687
+#define LZ4_H_98237428734687
+
+/*-************************************************************
+ *  Private Definitions
+ **************************************************************
+ * Do not use these definitions directly.
+ * They are only exposed to allow static allocation of `LZ4_stream_t` and `LZ4_streamDecode_t`.
+ * Accessing members will expose user code to API and/or ABI break in future versions of the library.
+ **************************************************************/
+#define LZ4_HASHLOG (LZ4_MEMORY_USAGE - 2)
+#define LZ4_HASHTABLESIZE (1 << LZ4_MEMORY_USAGE)
+#define LZ4_HASH_SIZE_U32                                                      \
+	(1 << LZ4_HASHLOG) /* required as macro for static allocation */
+
+#include <linux/types.h>
+#include <linux/limits.h>
+typedef int8_t LZ4_i8;
+typedef uint8_t LZ4_byte;
+typedef uint16_t LZ4_u16;
+typedef uint32_t LZ4_u32;
+
+/*! LZ4_stream_t :
+ *  Never ever use below internal definitions directly !
+ *  These definitions are not API/ABI safe, and may change in future versions.
+ *  If you need static allocation, declare or allocate an LZ4_stream_t object.
+**/
+
+typedef struct LZ4_stream_t_internal LZ4_stream_t_internal;
+struct LZ4_stream_t_internal {
+	LZ4_u32 hashTable[LZ4_HASH_SIZE_U32];
+	const LZ4_byte *dictionary;
+	const LZ4_stream_t_internal *dictCtx;
+	LZ4_u32 currentOffset;
+	LZ4_u32 tableType;
+	LZ4_u32 dictSize;
+	/* Implicit padding to ensure structure is aligned */
+};
+
+#define LZ4_STREAM_MINSIZE                                                     \
+	((1UL << (LZ4_MEMORY_USAGE)) +                                         \
+	 32) /* static size, for inter-version compatibility */
+union LZ4_stream_u {
+	char minStateSize[LZ4_STREAM_MINSIZE];
+	LZ4_stream_t_internal internal_donotuse;
+}; /* previously typedef'd to LZ4_stream_t */
+
+/*! LZ4_initStream() : v1.9.0+
+ *  An LZ4_stream_t structure must be initialized at least once.
+ *  This is automatically done when invoking LZ4_createStream(),
+ *  but it's not when the structure is simply declared on stack (for example).
+ *
+ *  Use LZ4_initStream() to properly initialize a newly declared LZ4_stream_t.
+ *  It can also initialize any arbitrary buffer of sufficient size,
+ *  and will @return a pointer of proper type upon initialization.
+ *
+ *  Note : initialization fails if size and alignment conditions are not respected.
+ *         In which case, the function will @return NULL.
+ *  Note2: An LZ4_stream_t structure guarantees correct alignment and size.
+ *  Note3: Before v1.9.0, use LZ4_resetStream() instead
+**/
+LZ4LIB_API LZ4_stream_t *LZ4_initStream(void *stateBuffer, size_t size);
+
+/*! LZ4_streamDecode_t :
+ *  Never ever use below internal definitions directly !
+ *  These definitions are not API/ABI safe, and may change in future versions.
+ *  If you need static allocation, declare or allocate an LZ4_streamDecode_t object.
+**/
+typedef struct {
+	const LZ4_byte *externalDict;
+	const LZ4_byte *prefixEnd;
+	size_t extDictSize;
+	size_t prefixSize;
+} LZ4_streamDecode_t_internal;
+
+#define LZ4_STREAMDECODE_MINSIZE 32
+union LZ4_streamDecode_u {
+	char minStateSize[LZ4_STREAMDECODE_MINSIZE];
+	LZ4_streamDecode_t_internal internal_donotuse;
+}; /* previously typedef'd to LZ4_streamDecode_t */
+
+/*-************************************
+*  Obsolete Functions
+**************************************/
+
+/*! Deprecation warnings
+ *
+ *  Deprecated functions make the compiler generate a warning when invoked.
+ *  This is meant to invite users to update their source code.
+ *  Should deprecation warnings be a problem, it is generally possible to disable them,
+ *  typically with -Wno-deprecated-declarations for gcc
+ *  or _CRT_SECURE_NO_WARNINGS in Visual.
+ *
+ *  Another method is to define LZ4_DISABLE_DEPRECATE_WARNINGS
+ *  before including the header file.
+ */
+#ifdef LZ4_DISABLE_DEPRECATE_WARNINGS
+#define LZ4_DEPRECATED(message) /* disable deprecation warnings */
+#else
+#if defined(__cplusplus) && (__cplusplus >= 201402) /* C++14 or greater */
+#define LZ4_DEPRECATED(message) [[deprecated(message)]]
+#elif defined(_MSC_VER)
+#define LZ4_DEPRECATED(message) __declspec(deprecated(message))
+#elif defined(__clang__) ||                                                    \
+	(defined(__GNUC__) && (__GNUC__ * 10 + __GNUC_MINOR__ >= 45))
+#define LZ4_DEPRECATED(message) __attribute__((deprecated(message)))
+#elif defined(__GNUC__) && (__GNUC__ * 10 + __GNUC_MINOR__ >= 31)
+#define LZ4_DEPRECATED(message) __attribute__((deprecated))
+#else
+#pragma message(                                                               \
+	"WARNING: LZ4_DEPRECATED needs custom implementation for this compiler")
+#define LZ4_DEPRECATED(message) /* disabled */
+#endif
+#endif /* LZ4_DISABLE_DEPRECATE_WARNINGS */
+
+/*! Obsolete compression functions (since v1.7.3) */
+LZ4_DEPRECATED("use LZ4_compress_default() instead")
+LZ4LIB_API int LZ4_compress(const char *src, char *dest, int srcSize);
+LZ4_DEPRECATED("use LZ4_compress_default() instead")
+LZ4LIB_API int LZ4_compress_limitedOutput(const char *src, char *dest,
+					  int srcSize, int maxOutputSize);
+LZ4_DEPRECATED("use LZ4_compress_fast_extState() instead")
+LZ4LIB_API int LZ4_compress_withState(void *state, const char *source,
+				      char *dest, int inputSize);
+LZ4_DEPRECATED("use LZ4_compress_fast_extState() instead")
+LZ4LIB_API int LZ4_compress_limitedOutput_withState(void *state,
+						    const char *source,
+						    char *dest, int inputSize,
+						    int maxOutputSize);
+LZ4_DEPRECATED("use LZ4_compress_fast_continue() instead")
+LZ4LIB_API int LZ4_compress_continue(LZ4_stream_t *LZ4_streamPtr,
+				     const char *source, char *dest,
+				     int inputSize);
+LZ4_DEPRECATED("use LZ4_compress_fast_continue() instead")
+LZ4LIB_API int LZ4_compress_limitedOutput_continue(LZ4_stream_t *LZ4_streamPtr,
+						   const char *source,
+						   char *dest, int inputSize,
+						   int maxOutputSize);
+
+/*! Obsolete decompression functions (since v1.8.0) */
+LZ4_DEPRECATED("use LZ4_decompress_fast() instead")
+LZ4LIB_API int LZ4_uncompress(const char *source, char *dest, int outputSize);
+LZ4_DEPRECATED("use LZ4_decompress_safe() instead")
+LZ4LIB_API int LZ4_uncompress_unknownOutputSize(const char *source, char *dest,
+						int isize, int maxOutputSize);
+
+/* Obsolete streaming functions (since v1.7.0)
+ * degraded functionality; do not use!
+ *
+ * In order to perform streaming compression, these functions depended on data
+ * that is no longer tracked in the state. They have been preserved as well as
+ * possible: using them will still produce a correct output. However, they don't
+ * actually retain any history between compression calls. The compression ratio
+ * achieved will therefore be no better than compressing each chunk
+ * independently.
+ */
+LZ4_DEPRECATED("Use LZ4_createStream() instead")
+LZ4LIB_API void *LZ4_create(char *inputBuffer);
+LZ4_DEPRECATED("Use LZ4_createStream() instead")
+LZ4LIB_API int LZ4_sizeofStreamState(void);
+LZ4_DEPRECATED("Use LZ4_resetStream() instead")
+LZ4LIB_API int LZ4_resetStreamState(void *state, char *inputBuffer);
+LZ4_DEPRECATED("Use LZ4_saveDict() instead")
+LZ4LIB_API char *LZ4_slideInputBuffer(void *state);
+
+/*! Obsolete streaming decoding functions (since v1.7.0) */
+LZ4_DEPRECATED("use LZ4_decompress_safe_usingDict() instead")
+LZ4LIB_API int LZ4_decompress_safe_withPrefix64k(const char *src, char *dst,
+						 int compressedSize,
+						 int maxDstSize);
+LZ4_DEPRECATED("use LZ4_decompress_fast_usingDict() instead")
+LZ4LIB_API int LZ4_decompress_fast_withPrefix64k(const char *src, char *dst,
+						 int originalSize);
+
+/*! Obsolete LZ4_decompress_fast variants (since v1.9.0) :
+ *  These functions used to be faster than LZ4_decompress_safe(),
+ *  but this is no longer the case. They are now slower.
+ *  This is because LZ4_decompress_fast() doesn't know the input size,
+ *  and therefore must progress more cautiously into the input buffer to not read beyond the end of block.
+ *  On top of that `LZ4_decompress_fast()` is not protected vs malformed or malicious inputs, making it a security liability.
+ *  As a consequence, LZ4_decompress_fast() is strongly discouraged, and deprecated.
+ *
+ *  The last remaining LZ4_decompress_fast() specificity is that
+ *  it can decompress a block without knowing its compressed size.
+ *  Such functionality can be achieved in a more secure manner
+ *  by employing LZ4_decompress_safe_partial().
+ *
+ *  Parameters:
+ *  originalSize : is the uncompressed size to regenerate.
+ *                 `dst` must be already allocated, its size must be >= 'originalSize' bytes.
+ * @return : number of bytes read from source buffer (== compressed size).
+ *           The function expects to finish at block's end exactly.
+ *           If the source stream is detected malformed, the function stops decoding and returns a negative result.
+ *  note : LZ4_decompress_fast*() requires originalSize. Thanks to this information, it never writes past the output buffer.
+ *         However, since it doesn't know its 'src' size, it may read an unknown amount of input, past input buffer bounds.
+ *         Also, since match offsets are not validated, match reads from 'src' may underflow too.
+ *         These issues never happen if input (compressed) data is correct.
+ *         But they may happen if input data is invalid (error or intentional tampering).
+ *         As a consequence, use these functions in trusted environments with trusted data **only**.
+ */
+LZ4_DEPRECATED(
+	"This function is deprecated and unsafe. Consider using LZ4_decompress_safe_partial() instead")
+LZ4LIB_API int LZ4_decompress_fast(const char *src, char *dst,
+				   int originalSize);
+LZ4_DEPRECATED(
+	"This function is deprecated and unsafe. Consider migrating towards LZ4_decompress_safe_continue() instead. "
+	"Note that the contract will change (requires block's compressed size, instead of decompressed size)")
+LZ4LIB_API int
+LZ4_decompress_fast_continue(LZ4_streamDecode_t *LZ4_streamDecode,
+			     const char *src, char *dst, int originalSize);
+LZ4_DEPRECATED(
+	"This function is deprecated and unsafe. Consider using LZ4_decompress_safe_partial_usingDict() instead")
+LZ4LIB_API int LZ4_decompress_fast_usingDict(const char *src, char *dst,
+					     int originalSize,
+					     const char *dictStart,
+					     int dictSize);
+
+/*! LZ4_resetStream() :
+ *  An LZ4_stream_t structure must be initialized at least once.
+ *  This is done with LZ4_initStream(), or LZ4_resetStream().
+ *  Consider switching to LZ4_initStream(),
+ *  invoking LZ4_resetStream() will trigger deprecation warnings in the future.
+ */
+LZ4LIB_API void LZ4_resetStream(LZ4_stream_t *streamPtr);
+
+#endif /* LZ4_H_98237428734687 */
+
+#if defined(__cplusplus)
+}
+#endif
Index: fs/f2fs/lz4armv8/lz4accel.c
===================================================================
diff --git a/fs/f2fs/lz4armv8/lz4accel.c b/lib/lz4/lz4armv8/lz4accel.c
rename from fs/f2fs/lz4armv8/lz4accel.c
rename to lib/lz4/lz4armv8/lz4accel.c
--- a/fs/f2fs/lz4armv8/lz4accel.c	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ b/lib/lz4/lz4armv8/lz4accel.c	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -46,4 +46,3 @@
 __read_mostly = {
 	[0 ... NR_CPUS-1]  = lz4_decompress_asm_select,
 };
-
Index: lib/lz4/lz4hc.c
===================================================================
diff --git a/lib/lz4/lz4hc.c b/lib/lz4/lz4hc.c
new file mode 100644
--- /dev/null	(revision b2497e4243461a835c25469028cd355bfc2e993f)
+++ b/lib/lz4/lz4hc.c	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -0,0 +1,2805 @@
+/*
+    LZ4 HC - High Compression Mode of LZ4
+    Copyright (C) 2011-2020, Yann Collet.
+
+    BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)
+
+    Redistribution and use in source and binary forms, with or without
+    modification, are permitted provided that the following conditions are
+    met:
+
+    * Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above
+    copyright notice, this list of conditions and the following disclaimer
+    in the documentation and/or other materials provided with the
+    distribution.
+
+    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+    "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+    You can contact the author at :
+       - LZ4 source repository : https://github.com/lz4/lz4
+       - LZ4 public forum : https://groups.google.com/forum/#!forum/lz4c
+*/
+/* note : lz4hc is not an independent module, it requires lz4.h/lz4.c for proper compilation */
+
+/* *************************************
+*  Tuning Parameter
+***************************************/
+
+#include <linux/slab.h>
+#define ALLOC(size) kzalloc(size, GFP_KERNEL)
+#define FREEMEM(ptr) kfree(ptr)
+
+/*===    Dependency    ===*/
+#define LZ4_HC_STATIC_LINKING_ONLY
+#include "lz4hc.h"
+
+/*! HEAPMODE :
+ *  Select how stateless HC compression functions like `LZ4_compress_HC()`
+ *  allocate memory for their workspace:
+ *  in stack (0:fastest), or in heap (1:default, requires malloc()).
+ *  Since workspace is rather large, heap mode is recommended.
+**/
+#ifndef LZ4HC_HEAPMODE
+#define LZ4HC_HEAPMODE 1
+#endif
+
+/*===   Shared lz4.c code   ===*/
+#ifndef LZ4_SRC_INCLUDED
+#if defined(__GNUC__)
+#pragma GCC diagnostic ignored "-Wunused-function"
+#endif
+#if defined(__clang__)
+#pragma clang diagnostic ignored "-Wunused-function"
+#endif
+#define LZ4_COMMONDEFS_ONLY
+#include "lz4.c" /* LZ4_count, constants, mem */
+#endif
+
+/*===   Enums   ===*/
+typedef enum { noDictCtx, usingDictCtxHc } dictCtx_directive;
+
+/*===   Constants   ===*/
+#define OPTIMAL_ML (int)((ML_MASK - 1) + MINMATCH)
+#define LZ4_OPT_NUM (1 << 12)
+
+/*===   Macros   ===*/
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+
+/*===   Levels definition   ===*/
+typedef enum { lz4mid, lz4hc, lz4opt } lz4hc_strat_e;
+typedef struct {
+	lz4hc_strat_e strat;
+	int nbSearches;
+	U32 targetLength;
+} cParams_t;
+static const cParams_t k_clTable[LZ4HC_CLEVEL_MAX + 1] = {
+	{ lz4mid, 2, 16 }, /* 0, unused */
+	{ lz4mid, 2, 16 }, /* 1, unused */
+	{ lz4mid, 2, 16 }, /* 2 */
+	{ lz4hc, 4, 16 }, /* 3 */
+	{ lz4hc, 8, 16 }, /* 4 */
+	{ lz4hc, 16, 16 }, /* 5 */
+	{ lz4hc, 32, 16 }, /* 6 */
+	{ lz4hc, 64, 16 }, /* 7 */
+	{ lz4hc, 128, 16 }, /* 8 */
+	{ lz4hc, 256, 16 }, /* 9 */
+	{ lz4opt, 96, 64 }, /*10==LZ4HC_CLEVEL_OPT_MIN*/
+	{ lz4opt, 512, 128 }, /*11 */
+	{ lz4opt, 16384, LZ4_OPT_NUM }, /* 12==LZ4HC_CLEVEL_MAX */
+};
+
+static cParams_t LZ4HC_getCLevelParams(int cLevel)
+{
+	/* note : clevel convention is a bit different from lz4frame,
+     * possibly something worth revisiting for consistency */
+	if (cLevel < 1)
+		cLevel = LZ4HC_CLEVEL_DEFAULT;
+	cLevel = MIN(LZ4HC_CLEVEL_MAX, cLevel);
+	return k_clTable[cLevel];
+}
+
+/*===   Hashing   ===*/
+#define LZ4HC_HASHSIZE 4
+#define HASH_FUNCTION(i)                                                       \
+	(((i) * 2654435761U) >> ((MINMATCH * 8) - LZ4HC_HASH_LOG))
+static U32 LZ4HC_hashPtr(const void *ptr)
+{
+	return HASH_FUNCTION(LZ4_read32(ptr));
+}
+
+#if defined(LZ4_FORCE_MEMORY_ACCESS) && (LZ4_FORCE_MEMORY_ACCESS == 2)
+/* lie to the compiler about data alignment; use with caution */
+static U64 LZ4_read64(const void *memPtr)
+{
+	return *(const U64 *)memPtr;
+}
+
+#elif defined(LZ4_FORCE_MEMORY_ACCESS) && (LZ4_FORCE_MEMORY_ACCESS == 1)
+/* __pack instructions are safer, but compiler specific */
+LZ4_PACK(typedef struct { U64 u64; }) LZ4_unalign64;
+static U64 LZ4_read64(const void *ptr)
+{
+	return ((const LZ4_unalign64 *)ptr)->u64;
+}
+
+#else /* safe and portable access using memcpy() */
+static U64 LZ4_read64(const void *memPtr)
+{
+	U64 val;
+	LZ4_memcpy(&val, memPtr, sizeof(val));
+	return val;
+}
+
+#endif /* LZ4_FORCE_MEMORY_ACCESS */
+
+#define LZ4MID_HASHSIZE 8
+#define LZ4MID_HASHLOG (LZ4HC_HASH_LOG - 1)
+#define LZ4MID_HASHTABLESIZE (1 << LZ4MID_HASHLOG)
+
+static U32 LZ4MID_hash4(U32 v)
+{
+	return (v * 2654435761U) >> (32 - LZ4MID_HASHLOG);
+}
+static U32 LZ4MID_hash4Ptr(const void *ptr)
+{
+	return LZ4MID_hash4(LZ4_read32(ptr));
+}
+/* note: hash7 hashes the lower 56-bits.
+ * It presumes input was read using little endian.*/
+static U32 LZ4MID_hash7(U64 v)
+{
+	return (U32)(((v << (64 - 56)) * 58295818150454627ULL) >>
+		     (64 - LZ4MID_HASHLOG));
+}
+static U64 LZ4_readLE64(const void *memPtr);
+static U32 LZ4MID_hash8Ptr(const void *ptr)
+{
+	return LZ4MID_hash7(LZ4_readLE64(ptr));
+}
+
+static U64 LZ4_readLE64(const void *memPtr)
+{
+	if (LZ4_isLittleEndian()) {
+		return LZ4_read64(memPtr);
+	} else {
+		const BYTE *p = (const BYTE *)memPtr;
+		/* note: relies on the compiler to simplify this expression */
+		return (U64)p[0] | ((U64)p[1] << 8) | ((U64)p[2] << 16) |
+		       ((U64)p[3] << 24) | ((U64)p[4] << 32) |
+		       ((U64)p[5] << 40) | ((U64)p[6] << 48) |
+		       ((U64)p[7] << 56);
+	}
+}
+
+/*===   Count match length   ===*/
+LZ4_FORCE_INLINE
+unsigned LZ4HC_NbCommonBytes32(U32 val)
+{
+	assert(val != 0);
+	if (LZ4_isLittleEndian()) {
+#if defined(_MSC_VER) && (_MSC_VER >= 1400) && !defined(LZ4_FORCE_SW_BITCOUNT)
+		unsigned long r;
+		_BitScanReverse(&r, val);
+		return (unsigned)((31 - r) >> 3);
+#elif (defined(__clang__) ||                                                   \
+       (defined(__GNUC__) &&                                                   \
+	((__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 4))))) &&    \
+	!defined(LZ4_FORCE_SW_BITCOUNT)
+		return (unsigned)__builtin_clz(val) >> 3;
+#else
+		val >>= 8;
+		val = ((((val + 0x00FFFF00) | 0x00FFFFFF) + val) |
+		       (val + 0x00FF0000)) >>
+		      24;
+		return (unsigned)val ^ 3;
+#endif
+	} else {
+#if defined(_MSC_VER) && (_MSC_VER >= 1400) && !defined(LZ4_FORCE_SW_BITCOUNT)
+		unsigned long r;
+		_BitScanForward(&r, val);
+		return (unsigned)(r >> 3);
+#elif (defined(__clang__) ||                                                   \
+       (defined(__GNUC__) &&                                                   \
+	((__GNUC__ > 3) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 4))))) &&    \
+	!defined(LZ4_FORCE_SW_BITCOUNT)
+		return (unsigned)__builtin_ctz(val) >> 3;
+#else
+		const U32 m = 0x01010101;
+		return (unsigned)((((val - 1) ^ val) & (m - 1)) * m) >> 24;
+#endif
+	}
+}
+
+/** LZ4HC_countBack() :
+ * @return : negative value, nb of common bytes before ip/match */
+LZ4_FORCE_INLINE
+int LZ4HC_countBack(const BYTE *const ip, const BYTE *const match,
+		    const BYTE *const iMin, const BYTE *const mMin)
+{
+	int back = 0;
+	int const min = (int)MAX(iMin - ip, mMin - match);
+	assert(min <= 0);
+	assert(ip >= iMin);
+	assert((size_t)(ip - iMin) < (1U << 31));
+	assert(match >= mMin);
+	assert((size_t)(match - mMin) < (1U << 31));
+
+	while ((back - min) > 3) {
+		U32 const v = LZ4_read32(ip + back - 4) ^
+			      LZ4_read32(match + back - 4);
+		if (v) {
+			return (back - (int)LZ4HC_NbCommonBytes32(v));
+		} else
+			back -= 4; /* 4-byte step */
+	}
+	/* check remainder if any */
+	while ((back > min) && (ip[back - 1] == match[back - 1]))
+		back--;
+	return back;
+}
+
+/*===   Chain table updates   ===*/
+#define DELTANEXTU16(table, pos) table[(U16)(pos)] /* faster */
+/* Make fields passed to, and updated by LZ4HC_encodeSequence explicit */
+#define UPDATABLE(ip, op, anchor) &ip, &op, &anchor
+
+/**************************************
+*  Init
+**************************************/
+static void LZ4HC_clearTables(LZ4HC_CCtx_internal *hc4)
+{
+	MEM_INIT(hc4->hashTable, 0, sizeof(hc4->hashTable));
+	MEM_INIT(hc4->chainTable, 0xFF, sizeof(hc4->chainTable));
+}
+
+static void LZ4HC_init_internal(LZ4HC_CCtx_internal *hc4, const BYTE *start)
+{
+	size_t const bufferSize = (size_t)(hc4->end - hc4->prefixStart);
+	size_t newStartingOffset = bufferSize + hc4->dictLimit;
+	DEBUGLOG(5, "LZ4HC_init_internal");
+	assert(newStartingOffset >= bufferSize); /* check overflow */
+	if (newStartingOffset > 1 GB) {
+		LZ4HC_clearTables(hc4);
+		newStartingOffset = 0;
+	}
+	newStartingOffset += 64 KB;
+	hc4->nextToUpdate = (U32)newStartingOffset;
+	hc4->prefixStart = start;
+	hc4->end = start;
+	hc4->dictStart = start;
+	hc4->dictLimit = (U32)newStartingOffset;
+	hc4->lowLimit = (U32)newStartingOffset;
+}
+
+/**************************************
+*  Encode
+**************************************/
+/* LZ4HC_encodeSequence() :
+ * @return : 0 if ok,
+ *           1 if buffer issue detected */
+LZ4_FORCE_INLINE int LZ4HC_encodeSequence(const BYTE **_ip, BYTE **_op,
+					  const BYTE **_anchor, int matchLength,
+					  int offset,
+					  limitedOutput_directive limit,
+					  BYTE *oend)
+{
+#define ip (*_ip)
+#define op (*_op)
+#define anchor (*_anchor)
+
+	size_t length;
+	BYTE *const token = op++;
+
+#if defined(LZ4_DEBUG) && (LZ4_DEBUG >= 6)
+	static const BYTE *start = NULL;
+	static U32 totalCost = 0;
+	U32 const pos = (start == NULL) ? 0 : (U32)(anchor - start);
+	U32 const ll = (U32)(ip - anchor);
+	U32 const llAdd = (ll >= 15) ? ((ll - 15) / 255) + 1 : 0;
+	U32 const mlAdd =
+		(matchLength >= 19) ? ((matchLength - 19) / 255) + 1 : 0;
+	U32 const cost = 1 + llAdd + ll + 2 + mlAdd;
+	if (start == NULL)
+		start = anchor; /* only works for single segment */
+	/* g_debuglog_enable = (pos >= 2228) & (pos <= 2262); */
+	DEBUGLOG(
+		6,
+		"pos:%7u -- literals:%4u, match:%4i, offset:%5i, cost:%4u + %5u",
+		pos, (U32)(ip - anchor), matchLength, offset, cost, totalCost);
+	totalCost += cost;
+#endif
+
+	/* Encode Literal length */
+	length = (size_t)(ip - anchor);
+	LZ4_STATIC_ASSERT(notLimited == 0);
+	/* Check output limit */
+	if (limit &&
+	    ((op + (length / 255) + length + (2 + 1 + LASTLITERALS)) > oend)) {
+		DEBUGLOG(
+			6,
+			"Not enough room to write %i literals (%i bytes remaining)",
+			(int)length, (int)(oend - op));
+		return 1;
+	}
+	if (length >= RUN_MASK) {
+		size_t len = length - RUN_MASK;
+		*token = (RUN_MASK << ML_BITS);
+		for (; len >= 255; len -= 255)
+			*op++ = 255;
+		*op++ = (BYTE)len;
+	} else {
+		*token = (BYTE)(length << ML_BITS);
+	}
+
+	/* Copy Literals */
+	LZ4_wildCopy8(op, anchor, op + length);
+	op += length;
+
+	/* Encode Offset */
+	assert(offset <= LZ4_DISTANCE_MAX);
+	assert(offset > 0);
+	LZ4_writeLE16(op, (U16)(offset));
+	op += 2;
+
+	/* Encode MatchLength */
+	assert(matchLength >= MINMATCH);
+	length = (size_t)matchLength - MINMATCH;
+	if (limit && (op + (length / 255) + (1 + LASTLITERALS) > oend)) {
+		DEBUGLOG(6, "Not enough room to write match length");
+		return 1; /* Check output limit */
+	}
+	if (length >= ML_MASK) {
+		*token += ML_MASK;
+		length -= ML_MASK;
+		for (; length >= 510; length -= 510) {
+			*op++ = 255;
+			*op++ = 255;
+		}
+		if (length >= 255) {
+			length -= 255;
+			*op++ = 255;
+		}
+		*op++ = (BYTE)length;
+	} else {
+		*token += (BYTE)(length);
+	}
+
+	/* Prepare next loop */
+	ip += matchLength;
+	anchor = ip;
+
+	return 0;
+
+#undef ip
+#undef op
+#undef anchor
+}
+
+typedef struct {
+	int off;
+	int len;
+	int back; /* negative value */
+} LZ4HC_match_t;
+
+LZ4HC_match_t LZ4HC_searchExtDict(const BYTE *ip, U32 ipIndex,
+				  const BYTE *const iLowLimit,
+				  const BYTE *const iHighLimit,
+				  const LZ4HC_CCtx_internal *dictCtx,
+				  U32 gDictEndIndex, int currentBestML,
+				  int nbAttempts)
+{
+	size_t const lDictEndIndex =
+		(size_t)(dictCtx->end - dictCtx->prefixStart) +
+		dictCtx->dictLimit;
+	U32 lDictMatchIndex = dictCtx->hashTable[LZ4HC_hashPtr(ip)];
+	U32 matchIndex = lDictMatchIndex + gDictEndIndex - (U32)lDictEndIndex;
+	int offset = 0, sBack = 0;
+	assert(lDictEndIndex <= 1 GB);
+	if (lDictMatchIndex > 0)
+		DEBUGLOG(7, "lDictEndIndex = %zu, lDictMatchIndex = %u",
+			 lDictEndIndex, lDictMatchIndex);
+	while (ipIndex - matchIndex <= LZ4_DISTANCE_MAX && nbAttempts--) {
+		const BYTE *const matchPtr = dictCtx->prefixStart -
+					     dictCtx->dictLimit +
+					     lDictMatchIndex;
+
+		if (LZ4_read32(matchPtr) == LZ4_read32(ip)) {
+			int mlt;
+			int back = 0;
+			const BYTE *vLimit =
+				ip + (lDictEndIndex - lDictMatchIndex);
+			if (vLimit > iHighLimit)
+				vLimit = iHighLimit;
+			mlt = (int)LZ4_count(ip + MINMATCH, matchPtr + MINMATCH,
+					     vLimit) +
+			      MINMATCH;
+			back = (ip > iLowLimit) ?
+				       LZ4HC_countBack(ip, matchPtr, iLowLimit,
+						       dictCtx->prefixStart) :
+				       0;
+			mlt -= back;
+			if (mlt > currentBestML) {
+				currentBestML = mlt;
+				offset = (int)(ipIndex - matchIndex);
+				sBack = back;
+				DEBUGLOG(
+					7,
+					"found match of length %i within extDictCtx",
+					currentBestML);
+			}
+		}
+
+		{
+			U32 const nextOffset = DELTANEXTU16(dictCtx->chainTable,
+							    lDictMatchIndex);
+			lDictMatchIndex -= nextOffset;
+			matchIndex -= nextOffset;
+		}
+	}
+
+	{
+		LZ4HC_match_t md;
+		md.len = currentBestML;
+		md.off = offset;
+		md.back = sBack;
+		return md;
+	}
+}
+
+typedef LZ4HC_match_t (*LZ4MID_searchIntoDict_f)(
+	const BYTE *ip, U32 ipIndex, const BYTE *const iHighLimit,
+	const LZ4HC_CCtx_internal *dictCtx, U32 gDictEndIndex);
+
+static LZ4HC_match_t LZ4MID_searchHCDict(const BYTE *ip, U32 ipIndex,
+					 const BYTE *const iHighLimit,
+					 const LZ4HC_CCtx_internal *dictCtx,
+					 U32 gDictEndIndex)
+{
+	return LZ4HC_searchExtDict(ip, ipIndex, ip, iHighLimit, dictCtx,
+				   gDictEndIndex, MINMATCH - 1, 2);
+}
+
+static LZ4HC_match_t LZ4MID_searchExtDict(const BYTE *ip, U32 ipIndex,
+					  const BYTE *const iHighLimit,
+					  const LZ4HC_CCtx_internal *dictCtx,
+					  U32 gDictEndIndex)
+{
+	size_t const lDictEndIndex =
+		(size_t)(dictCtx->end - dictCtx->prefixStart) +
+		dictCtx->dictLimit;
+	const U32 *const hash4Table = dictCtx->hashTable;
+	const U32 *const hash8Table = hash4Table + LZ4MID_HASHTABLESIZE;
+	DEBUGLOG(7, "LZ4MID_searchExtDict (ipIdx=%u)", ipIndex);
+
+	/* search long match first */
+	{
+		U32 l8DictMatchIndex = hash8Table[LZ4MID_hash8Ptr(ip)];
+		U32 m8Index =
+			l8DictMatchIndex + gDictEndIndex - (U32)lDictEndIndex;
+		assert(lDictEndIndex <= 1 GB);
+		if (ipIndex - m8Index <= LZ4_DISTANCE_MAX) {
+			const BYTE *const matchPtr = dictCtx->prefixStart -
+						     dictCtx->dictLimit +
+						     l8DictMatchIndex;
+			const size_t safeLen =
+				MIN(lDictEndIndex - l8DictMatchIndex,
+				    (size_t)(iHighLimit - ip));
+			int mlt = (int)LZ4_count(ip, matchPtr, ip + safeLen);
+			if (mlt >= MINMATCH) {
+				LZ4HC_match_t md;
+				DEBUGLOG(7,
+					 "Found long ExtDict match of len=%u",
+					 mlt);
+				md.len = mlt;
+				md.off = (int)(ipIndex - m8Index);
+				md.back = 0;
+				return md;
+			}
+		}
+	}
+
+	/* search for short match second */
+	{
+		U32 l4DictMatchIndex = hash4Table[LZ4MID_hash4Ptr(ip)];
+		U32 m4Index =
+			l4DictMatchIndex + gDictEndIndex - (U32)lDictEndIndex;
+		if (ipIndex - m4Index <= LZ4_DISTANCE_MAX) {
+			const BYTE *const matchPtr = dictCtx->prefixStart -
+						     dictCtx->dictLimit +
+						     l4DictMatchIndex;
+			const size_t safeLen =
+				MIN(lDictEndIndex - l4DictMatchIndex,
+				    (size_t)(iHighLimit - ip));
+			int mlt = (int)LZ4_count(ip, matchPtr, ip + safeLen);
+			if (mlt >= MINMATCH) {
+				LZ4HC_match_t md;
+				DEBUGLOG(7,
+					 "Found short ExtDict match of len=%u",
+					 mlt);
+				md.len = mlt;
+				md.off = (int)(ipIndex - m4Index);
+				md.back = 0;
+				return md;
+			}
+		}
+	}
+
+	/* nothing found */
+	{
+		LZ4HC_match_t const md = { 0, 0, 0 };
+		return md;
+	}
+}
+
+/**************************************
+*  Mid Compression (level 2)
+**************************************/
+
+LZ4_FORCE_INLINE void LZ4MID_addPosition(U32 *hTable, U32 hValue, U32 index)
+{
+	hTable[hValue] = index;
+}
+
+#define ADDPOS8(_p, _idx)                                                      \
+	LZ4MID_addPosition(hash8Table, LZ4MID_hash8Ptr(_p), _idx)
+#define ADDPOS4(_p, _idx)                                                      \
+	LZ4MID_addPosition(hash4Table, LZ4MID_hash4Ptr(_p), _idx)
+
+/* Fill hash tables with references into dictionary.
+ * The resulting table is only exploitable by LZ4MID (level 2) */
+static void LZ4MID_fillHTable(LZ4HC_CCtx_internal *cctx, const void *dict,
+			      size_t size)
+{
+	U32 *const hash4Table = cctx->hashTable;
+	U32 *const hash8Table = hash4Table + LZ4MID_HASHTABLESIZE;
+	const BYTE *const prefixPtr = (const BYTE *)dict;
+	U32 const prefixIdx = cctx->dictLimit;
+	U32 const target = prefixIdx + (U32)size - LZ4MID_HASHSIZE;
+	U32 idx = cctx->nextToUpdate;
+	assert(dict == cctx->prefixStart);
+	DEBUGLOG(4, "LZ4MID_fillHTable (size:%zu)", size);
+	if (size <= LZ4MID_HASHSIZE)
+		return;
+
+	for (; idx < target; idx += 3) {
+		ADDPOS4(prefixPtr + idx - prefixIdx, idx);
+		ADDPOS8(prefixPtr + idx + 1 - prefixIdx, idx + 1);
+	}
+
+	idx = (size > 32 KB + LZ4MID_HASHSIZE) ? target - 32 KB :
+						 cctx->nextToUpdate;
+	for (; idx < target; idx += 1) {
+		ADDPOS8(prefixPtr + idx - prefixIdx, idx);
+	}
+
+	cctx->nextToUpdate = target;
+}
+
+static LZ4MID_searchIntoDict_f
+select_searchDict_function(const LZ4HC_CCtx_internal *dictCtx)
+{
+	if (dictCtx == NULL)
+		return NULL;
+	if (LZ4HC_getCLevelParams(dictCtx->compressionLevel).strat == lz4mid)
+		return LZ4MID_searchExtDict;
+	return LZ4MID_searchHCDict;
+}
+
+static int LZ4MID_compress(LZ4HC_CCtx_internal *const ctx,
+			   const char *const src, char *const dst,
+			   int *srcSizePtr, int const maxOutputSize,
+			   const limitedOutput_directive limit,
+			   const dictCtx_directive dict)
+{
+	U32 *const hash4Table = ctx->hashTable;
+	U32 *const hash8Table = hash4Table + LZ4MID_HASHTABLESIZE;
+	const BYTE *ip = (const BYTE *)src;
+	const BYTE *anchor = ip;
+	const BYTE *const iend = ip + *srcSizePtr;
+	const BYTE *const mflimit = iend - MFLIMIT;
+	const BYTE *const matchlimit = (iend - LASTLITERALS);
+	const BYTE *const ilimit = (iend - LZ4MID_HASHSIZE);
+	BYTE *op = (BYTE *)dst;
+	BYTE *oend = op + maxOutputSize;
+
+	const BYTE *const prefixPtr = ctx->prefixStart;
+	const U32 prefixIdx = ctx->dictLimit;
+	const U32 ilimitIdx = (U32)(ilimit - prefixPtr) + prefixIdx;
+	const BYTE *const dictStart = ctx->dictStart;
+	const U32 dictIdx = ctx->lowLimit;
+	const U32 gDictEndIndex = ctx->lowLimit;
+	const LZ4MID_searchIntoDict_f searchIntoDict =
+		(dict == usingDictCtxHc) ?
+			select_searchDict_function(ctx->dictCtx) :
+			NULL;
+	unsigned matchLength;
+	unsigned matchDistance;
+
+	/* input sanitization */
+	DEBUGLOG(5, "LZ4MID_compress (%i bytes)", *srcSizePtr);
+	if (dict == usingDictCtxHc)
+		DEBUGLOG(5, "usingDictCtxHc");
+	assert(*srcSizePtr >= 0);
+	if (*srcSizePtr)
+		assert(src != NULL);
+	if (maxOutputSize)
+		assert(dst != NULL);
+	if (*srcSizePtr < 0)
+		return 0; /* invalid */
+	if (maxOutputSize < 0)
+		return 0; /* invalid */
+	if (*srcSizePtr > LZ4_MAX_INPUT_SIZE) {
+		/* forbidden: no input is allowed to be that large */
+		return 0;
+	}
+	if (limit == fillOutput)
+		oend -= LASTLITERALS; /* Hack for support LZ4 format restriction */
+	if (*srcSizePtr < LZ4_minLength)
+		goto _lz4mid_last_literals; /* Input too small, no compression (all literals) */
+
+	/* main loop */
+	while (ip <= mflimit) {
+		const U32 ipIndex = (U32)(ip - prefixPtr) + prefixIdx;
+		/* search long match */
+		{
+			U32 const h8 = LZ4MID_hash8Ptr(ip);
+			U32 const pos8 = hash8Table[h8];
+			assert(h8 < LZ4MID_HASHTABLESIZE);
+			assert(pos8 < ipIndex);
+			LZ4MID_addPosition(hash8Table, h8, ipIndex);
+			if (ipIndex - pos8 <= LZ4_DISTANCE_MAX) {
+				/* match candidate found */
+				if (pos8 >= prefixIdx) {
+					const BYTE *const matchPtr =
+						prefixPtr + pos8 - prefixIdx;
+					assert(matchPtr < ip);
+					matchLength = LZ4_count(ip, matchPtr,
+								matchlimit);
+					if (matchLength >= MINMATCH) {
+						DEBUGLOG(
+							7,
+							"found long match at pos %u (len=%u)",
+							pos8, matchLength);
+						matchDistance = ipIndex - pos8;
+						goto _lz4mid_encode_sequence;
+					}
+				} else {
+					if (pos8 >= dictIdx) {
+						/* extDict match candidate */
+						const BYTE *const matchPtr =
+							dictStart +
+							(pos8 - dictIdx);
+						const size_t safeLen = MIN(
+							prefixIdx - pos8,
+							(size_t)(matchlimit -
+								 ip));
+						matchLength =
+							LZ4_count(ip, matchPtr,
+								  ip + safeLen);
+						if (matchLength >= MINMATCH) {
+							DEBUGLOG(
+								7,
+								"found long match at ExtDict pos %u (len=%u)",
+								pos8,
+								matchLength);
+							matchDistance =
+								ipIndex - pos8;
+							goto _lz4mid_encode_sequence;
+						}
+					}
+				}
+			}
+		}
+		/* search short match */
+		{
+			U32 const h4 = LZ4MID_hash4Ptr(ip);
+			U32 const pos4 = hash4Table[h4];
+			assert(h4 < LZ4MID_HASHTABLESIZE);
+			assert(pos4 < ipIndex);
+			LZ4MID_addPosition(hash4Table, h4, ipIndex);
+			if (ipIndex - pos4 <= LZ4_DISTANCE_MAX) {
+				/* match candidate found */
+				if (pos4 >= prefixIdx) {
+					/* only search within prefix */
+					const BYTE *const matchPtr =
+						prefixPtr + (pos4 - prefixIdx);
+					assert(matchPtr < ip);
+					assert(matchPtr >= prefixPtr);
+					matchLength = LZ4_count(ip, matchPtr,
+								matchlimit);
+					if (matchLength >= MINMATCH) {
+						/* short match found, let's just check ip+1 for longer */
+						U32 const h8 =
+							LZ4MID_hash8Ptr(ip + 1);
+						U32 const pos8 = hash8Table[h8];
+						U32 const m2Distance =
+							ipIndex + 1 - pos8;
+						matchDistance = ipIndex - pos4;
+						if (m2Distance <=
+							    LZ4_DISTANCE_MAX &&
+						    pos8 >= prefixIdx /* only search within prefix */
+						    && likely(ip < mflimit)) {
+							const BYTE *const m2Ptr =
+								prefixPtr +
+								(pos8 -
+								 prefixIdx);
+							unsigned ml2 = LZ4_count(
+								ip + 1, m2Ptr,
+								matchlimit);
+							if (ml2 > matchLength) {
+								LZ4MID_addPosition(
+									hash8Table,
+									h8,
+									ipIndex +
+										1);
+								ip++;
+								matchLength =
+									ml2;
+								matchDistance =
+									m2Distance;
+							}
+						}
+						goto _lz4mid_encode_sequence;
+					}
+				} else {
+					if (pos4 >= dictIdx) {
+						/* extDict match candidate */
+						const BYTE *const matchPtr =
+							dictStart +
+							(pos4 - dictIdx);
+						const size_t safeLen = MIN(
+							prefixIdx - pos4,
+							(size_t)(matchlimit -
+								 ip));
+						matchLength =
+							LZ4_count(ip, matchPtr,
+								  ip + safeLen);
+						if (matchLength >= MINMATCH) {
+							DEBUGLOG(
+								7,
+								"found match at ExtDict pos %u (len=%u)",
+								pos4,
+								matchLength);
+							matchDistance =
+								ipIndex - pos4;
+							goto _lz4mid_encode_sequence;
+						}
+					}
+				}
+			}
+		}
+		/* no match found in prefix */
+		if ((dict == usingDictCtxHc) &&
+		    (ipIndex - gDictEndIndex < LZ4_DISTANCE_MAX - 8)) {
+			/* search a match into external dictionary */
+			LZ4HC_match_t dMatch =
+				searchIntoDict(ip, ipIndex, matchlimit,
+					       ctx->dictCtx, gDictEndIndex);
+			if (dMatch.len >= MINMATCH) {
+				DEBUGLOG(7,
+					 "found Dictionary match (offset=%i)",
+					 dMatch.off);
+				assert(dMatch.back == 0);
+				matchLength = (unsigned)dMatch.len;
+				matchDistance = (unsigned)dMatch.off;
+				goto _lz4mid_encode_sequence;
+			}
+		}
+		/* no match found */
+		ip += 1 + ((ip - anchor) >>
+			   9); /* skip faster over incompressible data */
+		continue;
+
+	_lz4mid_encode_sequence:
+		/* catch back */
+		while (((ip > anchor) &
+			((U32)(ip - prefixPtr) > matchDistance)) &&
+		       (unlikely(ip[-1] == ip[-(int)matchDistance - 1]))) {
+			ip--;
+			matchLength++;
+		};
+
+		/* fill table with beginning of match */
+		ADDPOS8(ip + 1, ipIndex + 1);
+		ADDPOS8(ip + 2, ipIndex + 2);
+		ADDPOS4(ip + 1, ipIndex + 1);
+
+		/* encode */
+		{
+			BYTE *const saved_op = op;
+			/* LZ4HC_encodeSequence always updates @op; on success, it updates @ip and @anchor */
+			if (LZ4HC_encodeSequence(
+				    UPDATABLE(ip, op, anchor), (int)matchLength,
+				    (int)matchDistance, limit, oend)) {
+				op = saved_op; /* restore @op value before failed LZ4HC_encodeSequence */
+				goto _lz4mid_dest_overflow;
+			}
+		}
+
+		/* fill table with end of match */
+		{
+			U32 endMatchIdx = (U32)(ip - prefixPtr) + prefixIdx;
+			U32 pos_m2 = endMatchIdx - 2;
+			if (pos_m2 < ilimitIdx) {
+				if (likely(ip - prefixPtr > 5)) {
+					ADDPOS8(ip - 5, endMatchIdx - 5);
+				}
+				ADDPOS8(ip - 3, endMatchIdx - 3);
+				ADDPOS8(ip - 2, endMatchIdx - 2);
+				ADDPOS4(ip - 2, endMatchIdx - 2);
+				ADDPOS4(ip - 1, endMatchIdx - 1);
+			}
+		}
+	}
+
+_lz4mid_last_literals:
+	/* Encode Last Literals */
+	{
+		size_t lastRunSize = (size_t)(iend - anchor); /* literals */
+		size_t llAdd = (lastRunSize + 255 - RUN_MASK) / 255;
+		size_t const totalSize = 1 + llAdd + lastRunSize;
+		if (limit == fillOutput)
+			oend += LASTLITERALS; /* restore correct value */
+		if (limit && (op + totalSize > oend)) {
+			if (limit == limitedOutput)
+				return 0; /* not enough space in @dst */
+			/* adapt lastRunSize to fill 'dest' */
+			lastRunSize = (size_t)(oend - op) - 1 /*token*/;
+			llAdd = (lastRunSize + 256 - RUN_MASK) / 256;
+			lastRunSize -= llAdd;
+		}
+		DEBUGLOG(6, "Final literal run : %i literals",
+			 (int)lastRunSize);
+		ip = anchor +
+		     lastRunSize; /* can be != iend if limit==fillOutput */
+
+		if (lastRunSize >= RUN_MASK) {
+			size_t accumulator = lastRunSize - RUN_MASK;
+			*op++ = (RUN_MASK << ML_BITS);
+			for (; accumulator >= 255; accumulator -= 255)
+				*op++ = 255;
+			*op++ = (BYTE)accumulator;
+		} else {
+			*op++ = (BYTE)(lastRunSize << ML_BITS);
+		}
+		assert(lastRunSize <= (size_t)(oend - op));
+		LZ4_memcpy(op, anchor, lastRunSize);
+		op += lastRunSize;
+	}
+
+	/* End */
+	DEBUGLOG(5, "compressed %i bytes into %i bytes", *srcSizePtr,
+		 (int)((char *)op - dst));
+	assert(ip >= (const BYTE *)src);
+	assert(ip <= iend);
+	*srcSizePtr = (int)(ip - (const BYTE *)src);
+	assert((char *)op >= dst);
+	assert(op <= oend);
+	assert((char *)op - dst < INT_MAX);
+	return (int)((char *)op - dst);
+
+_lz4mid_dest_overflow:
+	if (limit == fillOutput) {
+		/* Assumption : @ip, @anchor, @optr and @matchLength must be set correctly */
+		size_t const ll = (size_t)(ip - anchor);
+		size_t const ll_addbytes = (ll + 240) / 255;
+		size_t const ll_totalCost = 1 + ll_addbytes + ll;
+		BYTE *const maxLitPos =
+			oend - 3; /* 2 for offset, 1 for token */
+		DEBUGLOG(
+			6,
+			"Last sequence is overflowing : %u literals, %u remaining space",
+			(unsigned)ll, (unsigned)(oend - op));
+		if (op + ll_totalCost <= maxLitPos) {
+			/* ll validated; now adjust match length */
+			size_t const bytesLeftForMl =
+				(size_t)(maxLitPos - (op + ll_totalCost));
+			size_t const maxMlSize = MINMATCH + (ML_MASK - 1) +
+						 (bytesLeftForMl * 255);
+			assert(maxMlSize < INT_MAX);
+			if ((size_t)matchLength > maxMlSize)
+				matchLength = (unsigned)maxMlSize;
+			if ((oend + LASTLITERALS) - (op + ll_totalCost + 2) -
+				    1 + matchLength >=
+			    MFLIMIT) {
+				DEBUGLOG(
+					6,
+					"Let's encode a last sequence (ll=%u, ml=%u)",
+					(unsigned)ll, matchLength);
+				LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor),
+						     (int)matchLength,
+						     (int)matchDistance,
+						     notLimited, oend);
+			}
+		}
+		DEBUGLOG(6,
+			 "Let's finish with a run of literals (%u bytes left)",
+			 (unsigned)(oend - op));
+		goto _lz4mid_last_literals;
+	}
+	/* compression failed */
+	return 0;
+}
+
+/**************************************
+*  HC Compression - Search
+**************************************/
+
+/* Update chains up to ip (excluded) */
+LZ4_FORCE_INLINE void LZ4HC_Insert(LZ4HC_CCtx_internal *hc4, const BYTE *ip)
+{
+	U16 *const chainTable = hc4->chainTable;
+	U32 *const hashTable = hc4->hashTable;
+	const BYTE *const prefixPtr = hc4->prefixStart;
+	U32 const prefixIdx = hc4->dictLimit;
+	U32 const target = (U32)(ip - prefixPtr) + prefixIdx;
+	U32 idx = hc4->nextToUpdate;
+	assert(ip >= prefixPtr);
+	assert(target >= prefixIdx);
+
+	while (idx < target) {
+		U32 const h = LZ4HC_hashPtr(prefixPtr + idx - prefixIdx);
+		size_t delta = idx - hashTable[h];
+		if (delta > LZ4_DISTANCE_MAX)
+			delta = LZ4_DISTANCE_MAX;
+		DELTANEXTU16(chainTable, idx) = (U16)delta;
+		hashTable[h] = idx;
+		idx++;
+	}
+
+	hc4->nextToUpdate = target;
+}
+
+#if defined(_MSC_VER)
+#define LZ4HC_rotl32(x, r) _rotl(x, r)
+#else
+#define LZ4HC_rotl32(x, r) ((x << r) | (x >> (32 - r)))
+#endif
+
+static U32 LZ4HC_rotatePattern(size_t const rotate, U32 const pattern)
+{
+	size_t const bitsToRotate = (rotate & (sizeof(pattern) - 1)) << 3;
+	if (bitsToRotate == 0)
+		return pattern;
+	return LZ4HC_rotl32(pattern, (int)bitsToRotate);
+}
+
+/* LZ4HC_countPattern() :
+ * pattern32 must be a sample of repetitive pattern of length 1, 2 or 4 (but not 3!) */
+static unsigned LZ4HC_countPattern(const BYTE *ip, const BYTE *const iEnd,
+				   U32 const pattern32)
+{
+	const BYTE *const iStart = ip;
+	reg_t const pattern =
+		(sizeof(pattern) == 8) ?
+			(reg_t)pattern32 +
+				(((reg_t)pattern32) << (sizeof(pattern) * 4)) :
+			pattern32;
+
+	while (likely(ip < iEnd - (sizeof(pattern) - 1))) {
+		reg_t const diff = LZ4_read_ARCH(ip) ^ pattern;
+		if (!diff) {
+			ip += sizeof(pattern);
+			continue;
+		}
+		ip += LZ4_NbCommonBytes(diff);
+		return (unsigned)(ip - iStart);
+	}
+
+	if (LZ4_isLittleEndian()) {
+		reg_t patternByte = pattern;
+		while ((ip < iEnd) && (*ip == (BYTE)patternByte)) {
+			ip++;
+			patternByte >>= 8;
+		}
+	} else { /* big endian */
+		U32 bitOffset = (sizeof(pattern) * 8) - 8;
+		while (ip < iEnd) {
+			BYTE const byte = (BYTE)(pattern >> bitOffset);
+			if (*ip != byte)
+				break;
+			ip++;
+			bitOffset -= 8;
+		}
+	}
+
+	return (unsigned)(ip - iStart);
+}
+
+/* LZ4HC_reverseCountPattern() :
+ * pattern must be a sample of repetitive pattern of length 1, 2 or 4 (but not 3!)
+ * read using natural platform endianness */
+static unsigned LZ4HC_reverseCountPattern(const BYTE *ip,
+					  const BYTE *const iLow, U32 pattern)
+{
+	const BYTE *const iStart = ip;
+
+	while (likely(ip >= iLow + 4)) {
+		if (LZ4_read32(ip - 4) != pattern)
+			break;
+		ip -= 4;
+	}
+	{
+		const BYTE *bytePtr = (const BYTE *)(&pattern) +
+				      3; /* works for any endianness */
+		while (likely(ip > iLow)) {
+			if (ip[-1] != *bytePtr)
+				break;
+			ip--;
+			bytePtr--;
+		}
+	}
+	return (unsigned)(iStart - ip);
+}
+
+/* LZ4HC_protectDictEnd() :
+ * Checks if the match is in the last 3 bytes of the dictionary, so reading the
+ * 4 byte MINMATCH would overflow.
+ * @returns true if the match index is okay.
+ */
+static int LZ4HC_protectDictEnd(U32 const dictLimit, U32 const matchIndex)
+{
+	return ((U32)((dictLimit - 1) - matchIndex) >= 3);
+}
+
+typedef enum { rep_untested, rep_not, rep_confirmed } repeat_state_e;
+typedef enum { favorCompressionRatio = 0, favorDecompressionSpeed } HCfavor_e;
+
+LZ4_FORCE_INLINE LZ4HC_match_t LZ4HC_InsertAndGetWiderMatch(
+	LZ4HC_CCtx_internal *const hc4, const BYTE *const ip,
+	const BYTE *const iLowLimit, const BYTE *const iHighLimit, int longest,
+	const int maxNbAttempts, const int patternAnalysis, const int chainSwap,
+	const dictCtx_directive dict, const HCfavor_e favorDecSpeed)
+{
+	U16 *const chainTable = hc4->chainTable;
+	U32 *const hashTable = hc4->hashTable;
+	const LZ4HC_CCtx_internal *const dictCtx = hc4->dictCtx;
+	const BYTE *const prefixPtr = hc4->prefixStart;
+	const U32 prefixIdx = hc4->dictLimit;
+	const U32 ipIndex = (U32)(ip - prefixPtr) + prefixIdx;
+	const int withinStartDistance =
+		(hc4->lowLimit + (LZ4_DISTANCE_MAX + 1) > ipIndex);
+	const U32 lowestMatchIndex = (withinStartDistance) ?
+					     hc4->lowLimit :
+					     ipIndex - LZ4_DISTANCE_MAX;
+	const BYTE *const dictStart = hc4->dictStart;
+	const U32 dictIdx = hc4->lowLimit;
+	const BYTE *const dictEnd = dictStart + prefixIdx - dictIdx;
+	int const lookBackLength = (int)(ip - iLowLimit);
+	int nbAttempts = maxNbAttempts;
+	U32 matchChainPos = 0;
+	U32 const pattern = LZ4_read32(ip);
+	U32 matchIndex;
+	repeat_state_e repeat = rep_untested;
+	size_t srcPatternLength = 0;
+	int offset = 0, sBack = 0;
+
+	DEBUGLOG(7, "LZ4HC_InsertAndGetWiderMatch");
+	/* First Match */
+	LZ4HC_Insert(hc4,
+		     ip); /* insert all prior positions up to ip (excluded) */
+	matchIndex = hashTable[LZ4HC_hashPtr(ip)];
+	DEBUGLOG(
+		7,
+		"First candidate match for pos %u found at index %u / %u (lowestMatchIndex)",
+		ipIndex, matchIndex, lowestMatchIndex);
+
+	while ((matchIndex >= lowestMatchIndex) && (nbAttempts > 0)) {
+		int matchLength = 0;
+		nbAttempts--;
+		assert(matchIndex < ipIndex);
+		if (favorDecSpeed && (ipIndex - matchIndex < 8)) {
+			/* do nothing:
+             * favorDecSpeed intentionally skips matches with offset < 8 */
+		} else if (matchIndex >=
+			   prefixIdx) { /* within current Prefix */
+			const BYTE *const matchPtr =
+				prefixPtr + (matchIndex - prefixIdx);
+			assert(matchPtr < ip);
+			assert(longest >= 1);
+			if (LZ4_read16(iLowLimit + longest - 1) ==
+			    LZ4_read16(matchPtr - lookBackLength + longest -
+				       1)) {
+				if (LZ4_read32(matchPtr) == pattern) {
+					int const back =
+						lookBackLength ?
+							LZ4HC_countBack(
+								ip, matchPtr,
+								iLowLimit,
+								prefixPtr) :
+							0;
+					matchLength =
+						MINMATCH +
+						(int)LZ4_count(ip + MINMATCH,
+							       matchPtr +
+								       MINMATCH,
+							       iHighLimit);
+					matchLength -= back;
+					if (matchLength > longest) {
+						longest = matchLength;
+						offset = (int)(ipIndex -
+							       matchIndex);
+						sBack = back;
+						DEBUGLOG(
+							7,
+							"Found match of len=%i within prefix, offset=%i, back=%i",
+							longest, offset, -back);
+					}
+				}
+			}
+		} else { /* lowestMatchIndex <= matchIndex < dictLimit : within Ext Dict */
+			const BYTE *const matchPtr =
+				dictStart + (matchIndex - dictIdx);
+			assert(matchIndex >= dictIdx);
+			if (likely(matchIndex <= prefixIdx - 4) &&
+			    (LZ4_read32(matchPtr) == pattern)) {
+				int back = 0;
+				const BYTE *vLimit =
+					ip + (prefixIdx - matchIndex);
+				if (vLimit > iHighLimit)
+					vLimit = iHighLimit;
+				matchLength =
+					(int)LZ4_count(ip + MINMATCH,
+						       matchPtr + MINMATCH,
+						       vLimit) +
+					MINMATCH;
+				if ((ip + matchLength == vLimit) &&
+				    (vLimit < iHighLimit))
+					matchLength +=
+						LZ4_count(ip + matchLength,
+							  prefixPtr,
+							  iHighLimit);
+				back = lookBackLength ?
+					       LZ4HC_countBack(ip, matchPtr,
+							       iLowLimit,
+							       dictStart) :
+					       0;
+				matchLength -= back;
+				if (matchLength > longest) {
+					longest = matchLength;
+					offset = (int)(ipIndex - matchIndex);
+					sBack = back;
+					DEBUGLOG(
+						7,
+						"Found match of len=%i within dict, offset=%i, back=%i",
+						longest, offset, -back);
+				}
+			}
+		}
+
+		if (chainSwap &&
+		    matchLength ==
+			    longest) { /* better match => select a better chain */
+			assert(lookBackLength == 0); /* search forward only */
+			if (matchIndex + (U32)longest <= ipIndex) {
+				int const kTrigger = 4;
+				U32 distanceToNextMatch = 1;
+				int const end = longest - MINMATCH + 1;
+				int step = 1;
+				int accel = 1 << kTrigger;
+				int pos;
+				for (pos = 0; pos < end; pos += step) {
+					U32 const candidateDist = DELTANEXTU16(
+						chainTable,
+						matchIndex + (U32)pos);
+					step = (accel++ >> kTrigger);
+					if (candidateDist >
+					    distanceToNextMatch) {
+						distanceToNextMatch =
+							candidateDist;
+						matchChainPos = (U32)pos;
+						accel = 1 << kTrigger;
+					}
+				}
+				if (distanceToNextMatch > 1) {
+					if (distanceToNextMatch > matchIndex)
+						break; /* avoid overflow */
+					matchIndex -= distanceToNextMatch;
+					continue;
+				}
+			}
+		}
+
+		{
+			U32 const distNextMatch =
+				DELTANEXTU16(chainTable, matchIndex);
+			if (patternAnalysis && distNextMatch == 1 &&
+			    matchChainPos == 0) {
+				U32 const matchCandidateIdx = matchIndex - 1;
+				/* may be a repeated pattern */
+				if (repeat == rep_untested) {
+					if (((pattern & 0xFFFF) ==
+					     (pattern >> 16)) &
+					    ((pattern & 0xFF) ==
+					     (pattern >> 24))) {
+						DEBUGLOG(
+							7,
+							"Repeat pattern detected, char %02X",
+							pattern >> 24);
+						repeat = rep_confirmed;
+						srcPatternLength =
+							LZ4HC_countPattern(
+								ip + sizeof(pattern),
+								iHighLimit,
+								pattern) +
+							sizeof(pattern);
+					} else {
+						repeat = rep_not;
+					}
+				}
+				if ((repeat == rep_confirmed) &&
+				    (matchCandidateIdx >= lowestMatchIndex) &&
+				    LZ4HC_protectDictEnd(prefixIdx,
+							 matchCandidateIdx)) {
+					const int extDict =
+						matchCandidateIdx < prefixIdx;
+					const BYTE *const matchPtr =
+						extDict ?
+							dictStart +
+								(matchCandidateIdx -
+								 dictIdx) :
+							prefixPtr +
+								(matchCandidateIdx -
+								 prefixIdx);
+					if (LZ4_read32(matchPtr) ==
+					    pattern) { /* good candidate */
+						const BYTE *const iLimit =
+							extDict ? dictEnd :
+								  iHighLimit;
+						size_t forwardPatternLength =
+							LZ4HC_countPattern(
+								matchPtr +
+									sizeof(pattern),
+								iLimit,
+								pattern) +
+							sizeof(pattern);
+						if (extDict &&
+						    matchPtr + forwardPatternLength ==
+							    iLimit) {
+							U32 const rotatedPattern =
+								LZ4HC_rotatePattern(
+									forwardPatternLength,
+									pattern);
+							forwardPatternLength +=
+								LZ4HC_countPattern(
+									prefixPtr,
+									iHighLimit,
+									rotatedPattern);
+						}
+						{
+							const BYTE *const lowestMatchPtr =
+								extDict ?
+									dictStart :
+									prefixPtr;
+							size_t backLength =
+								LZ4HC_reverseCountPattern(
+									matchPtr,
+									lowestMatchPtr,
+									pattern);
+							size_t currentSegmentLength;
+							if (!extDict &&
+							    matchPtr - backLength ==
+								    prefixPtr &&
+							    dictIdx <
+								    prefixIdx) {
+								U32 const rotatedPattern = LZ4HC_rotatePattern(
+									(U32)(-(int)backLength),
+									pattern);
+								backLength += LZ4HC_reverseCountPattern(
+									dictEnd,
+									dictStart,
+									rotatedPattern);
+							}
+							/* Limit backLength not go further than lowestMatchIndex */
+							backLength =
+								matchCandidateIdx -
+								MAX(matchCandidateIdx -
+									    (U32)backLength,
+								    lowestMatchIndex);
+							assert(matchCandidateIdx -
+								       backLength >=
+							       lowestMatchIndex);
+							currentSegmentLength =
+								backLength +
+								forwardPatternLength;
+							/* Adjust to end of pattern if the source pattern fits, otherwise the beginning of the pattern */
+							if ((currentSegmentLength >=
+							     srcPatternLength) /* current pattern segment large enough to contain full srcPatternLength */
+							    &&
+							    (forwardPatternLength <=
+							     srcPatternLength)) { /* haven't reached this position yet */
+								U32 const newMatchIndex =
+									matchCandidateIdx +
+									(U32)forwardPatternLength -
+									(U32)srcPatternLength; /* best position, full pattern, might be followed by more match */
+								if (LZ4HC_protectDictEnd(
+									    prefixIdx,
+									    newMatchIndex))
+									matchIndex =
+										newMatchIndex;
+								else {
+									/* Can only happen if started in the prefix */
+									assert(newMatchIndex >=
+										       prefixIdx -
+											       3 &&
+									       newMatchIndex <
+										       prefixIdx &&
+									       !extDict);
+									matchIndex =
+										prefixIdx;
+								}
+							} else {
+								U32 const newMatchIndex =
+									matchCandidateIdx -
+									(U32)backLength; /* farthest position in current segment, will find a match of length currentSegmentLength + maybe some back */
+								if (!LZ4HC_protectDictEnd(
+									    prefixIdx,
+									    newMatchIndex)) {
+									assert(newMatchIndex >=
+										       prefixIdx -
+											       3 &&
+									       newMatchIndex <
+										       prefixIdx &&
+									       !extDict);
+									matchIndex =
+										prefixIdx;
+								} else {
+									matchIndex =
+										newMatchIndex;
+									if (lookBackLength ==
+									    0) { /* no back possible */
+										size_t const maxML =
+											MIN(currentSegmentLength,
+											    srcPatternLength);
+										if ((size_t)longest <
+										    maxML) {
+											assert(prefixPtr -
+												       prefixIdx +
+												       matchIndex !=
+											       ip);
+											if ((size_t)(ip -
+												     prefixPtr) +
+												    prefixIdx -
+												    matchIndex >
+											    LZ4_DISTANCE_MAX)
+												break;
+											assert(maxML <
+											       2 GB);
+											longest = (int)
+												maxML;
+											offset =
+												(int)(ipIndex -
+												      matchIndex);
+											assert(sBack ==
+											       0);
+											DEBUGLOG(
+												7,
+												"Found repeat pattern match of len=%i, offset=%i",
+												longest,
+												offset);
+										}
+										{
+											U32 const distToNextPattern =
+												DELTANEXTU16(
+													chainTable,
+													matchIndex);
+											if (distToNextPattern >
+											    matchIndex)
+												break; /* avoid overflow */
+											matchIndex -=
+												distToNextPattern;
+										}
+									}
+								}
+							}
+						}
+						continue;
+					}
+				}
+			}
+		} /* PA optimization */
+
+		/* follow current chain */
+		matchIndex -=
+			DELTANEXTU16(chainTable, matchIndex + matchChainPos);
+
+	} /* while ((matchIndex>=lowestMatchIndex) && (nbAttempts)) */
+
+	if (dict == usingDictCtxHc && nbAttempts > 0 && withinStartDistance) {
+		size_t const dictEndOffset =
+			(size_t)(dictCtx->end - dictCtx->prefixStart) +
+			dictCtx->dictLimit;
+		U32 dictMatchIndex = dictCtx->hashTable[LZ4HC_hashPtr(ip)];
+		assert(dictEndOffset <= 1 GB);
+		matchIndex =
+			dictMatchIndex + lowestMatchIndex - (U32)dictEndOffset;
+		if (dictMatchIndex > 0)
+			DEBUGLOG(
+				7,
+				"dictEndOffset = %zu, dictMatchIndex = %u => relative matchIndex = %i",
+				dictEndOffset, dictMatchIndex,
+				(int)dictMatchIndex - (int)dictEndOffset);
+		while (ipIndex - matchIndex <= LZ4_DISTANCE_MAX &&
+		       nbAttempts--) {
+			const BYTE *const matchPtr = dictCtx->prefixStart -
+						     dictCtx->dictLimit +
+						     dictMatchIndex;
+
+			if (LZ4_read32(matchPtr) == pattern) {
+				int mlt;
+				int back = 0;
+				const BYTE *vLimit =
+					ip + (dictEndOffset - dictMatchIndex);
+				if (vLimit > iHighLimit)
+					vLimit = iHighLimit;
+				mlt = (int)LZ4_count(ip + MINMATCH,
+						     matchPtr + MINMATCH,
+						     vLimit) +
+				      MINMATCH;
+				back = lookBackLength ?
+					       LZ4HC_countBack(
+						       ip, matchPtr, iLowLimit,
+						       dictCtx->prefixStart) :
+					       0;
+				mlt -= back;
+				if (mlt > longest) {
+					longest = mlt;
+					offset = (int)(ipIndex - matchIndex);
+					sBack = back;
+					DEBUGLOG(
+						7,
+						"found match of length %i within extDictCtx",
+						longest);
+				}
+			}
+
+			{
+				U32 const nextOffset = DELTANEXTU16(
+					dictCtx->chainTable, dictMatchIndex);
+				dictMatchIndex -= nextOffset;
+				matchIndex -= nextOffset;
+			}
+		}
+	}
+
+	{
+		LZ4HC_match_t md;
+		assert(longest >= 0);
+		md.len = longest;
+		md.off = offset;
+		md.back = sBack;
+		return md;
+	}
+}
+
+LZ4_FORCE_INLINE LZ4HC_match_t LZ4HC_InsertAndFindBestMatch(
+	LZ4HC_CCtx_internal *const hc4, /* Index table will be updated */
+	const BYTE *const ip, const BYTE *const iLimit, const int maxNbAttempts,
+	const int patternAnalysis, const dictCtx_directive dict)
+{
+	DEBUGLOG(7, "LZ4HC_InsertAndFindBestMatch");
+	/* note : LZ4HC_InsertAndGetWiderMatch() is able to modify the starting position of a match (*startpos),
+     * but this won't be the case here, as we define iLowLimit==ip,
+     * so LZ4HC_InsertAndGetWiderMatch() won't be allowed to search past ip */
+	return LZ4HC_InsertAndGetWiderMatch(hc4, ip, ip, iLimit, MINMATCH - 1,
+					    maxNbAttempts, patternAnalysis,
+					    0 /*chainSwap*/, dict,
+					    favorCompressionRatio);
+}
+
+LZ4_FORCE_INLINE int
+LZ4HC_compress_hashChain(LZ4HC_CCtx_internal *const ctx,
+			 const char *const source, char *const dest,
+			 int *srcSizePtr, int const maxOutputSize,
+			 int maxNbAttempts, const limitedOutput_directive limit,
+			 const dictCtx_directive dict)
+{
+	const int inputSize = *srcSizePtr;
+	const int patternAnalysis = (maxNbAttempts > 128); /* levels 9+ */
+
+	const BYTE *ip = (const BYTE *)source;
+	const BYTE *anchor = ip;
+	const BYTE *const iend = ip + inputSize;
+	const BYTE *const mflimit = iend - MFLIMIT;
+	const BYTE *const matchlimit = (iend - LASTLITERALS);
+
+	BYTE *optr = (BYTE *)dest;
+	BYTE *op = (BYTE *)dest;
+	BYTE *oend = op + maxOutputSize;
+
+	const BYTE *start0;
+	const BYTE *start2 = NULL;
+	const BYTE *start3 = NULL;
+	LZ4HC_match_t m0, m1, m2, m3;
+	const LZ4HC_match_t nomatch = { 0, 0, 0 };
+
+	/* init */
+	DEBUGLOG(5, "LZ4HC_compress_hashChain (dict?=>%i)", dict);
+	*srcSizePtr = 0;
+	if (limit == fillOutput)
+		oend -= LASTLITERALS; /* Hack for support LZ4 format restriction */
+	if (inputSize < LZ4_minLength)
+		goto _last_literals; /* Input too small, no compression (all literals) */
+
+	/* Main Loop */
+	while (ip <= mflimit) {
+		m1 = LZ4HC_InsertAndFindBestMatch(ctx, ip, matchlimit,
+						  maxNbAttempts,
+						  patternAnalysis, dict);
+		if (m1.len < MINMATCH) {
+			ip++;
+			continue;
+		}
+
+		/* saved, in case we would skip too much */
+		start0 = ip;
+		m0 = m1;
+
+	_Search2:
+		DEBUGLOG(7, "_Search2 (currently found match of size %i)",
+			 m1.len);
+		if (ip + m1.len <= mflimit) {
+			start2 = ip + m1.len - 2;
+			m2 = LZ4HC_InsertAndGetWiderMatch(
+				ctx, start2, ip + 0, matchlimit, m1.len,
+				maxNbAttempts, patternAnalysis, 0, dict,
+				favorCompressionRatio);
+			start2 += m2.back;
+		} else {
+			m2 = nomatch; /* do not search further */
+		}
+
+		if (m2.len <=
+		    m1.len) { /* No better match => encode ML1 immediately */
+			optr = op;
+			if (LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor),
+						 m1.len, m1.off, limit, oend))
+				goto _dest_overflow;
+			continue;
+		}
+
+		if (start0 < ip) { /* first match was skipped at least once */
+			if (start2 <
+			    ip + m0.len) { /* squeezing ML1 between ML0(original ML1) and ML2 */
+				ip = start0;
+				m1 = m0; /* restore initial Match1 */
+			}
+		}
+
+		/* Here, start0==ip */
+		if ((start2 - ip) < 3) { /* First Match too small : removed */
+			ip = start2;
+			m1 = m2;
+			goto _Search2;
+		}
+
+	_Search3:
+		if ((start2 - ip) < OPTIMAL_ML) {
+			int correction;
+			int new_ml = m1.len;
+			if (new_ml > OPTIMAL_ML)
+				new_ml = OPTIMAL_ML;
+			if (ip + new_ml > start2 + m2.len - MINMATCH)
+				new_ml = (int)(start2 - ip) + m2.len - MINMATCH;
+			correction = new_ml - (int)(start2 - ip);
+			if (correction > 0) {
+				start2 += correction;
+				m2.len -= correction;
+			}
+		}
+
+		if (start2 + m2.len <= mflimit) {
+			start3 = start2 + m2.len - 3;
+			m3 = LZ4HC_InsertAndGetWiderMatch(
+				ctx, start3, start2, matchlimit, m2.len,
+				maxNbAttempts, patternAnalysis, 0, dict,
+				favorCompressionRatio);
+			start3 += m3.back;
+		} else {
+			m3 = nomatch; /* do not search further */
+		}
+
+		if (m3.len <=
+		    m2.len) { /* No better match => encode ML1 and ML2 */
+			/* ip & ref are known; Now for ml */
+			if (start2 < ip + m1.len)
+				m1.len = (int)(start2 - ip);
+			/* Now, encode 2 sequences */
+			optr = op;
+			if (LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor),
+						 m1.len, m1.off, limit, oend))
+				goto _dest_overflow;
+			ip = start2;
+			optr = op;
+			if (LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor),
+						 m2.len, m2.off, limit, oend)) {
+				m1 = m2;
+				goto _dest_overflow;
+			}
+			continue;
+		}
+
+		if (start3 <
+		    ip + m1.len +
+			    3) { /* Not enough space for match 2 : remove it */
+			if (start3 >=
+			    (ip +
+			     m1.len)) { /* can write Seq1 immediately ==> Seq2 is removed, so Seq3 becomes Seq1 */
+				if (start2 < ip + m1.len) {
+					int correction =
+						(int)(ip + m1.len - start2);
+					start2 += correction;
+					m2.len -= correction;
+					if (m2.len < MINMATCH) {
+						start2 = start3;
+						m2 = m3;
+					}
+				}
+
+				optr = op;
+				if (LZ4HC_encodeSequence(
+					    UPDATABLE(ip, op, anchor), m1.len,
+					    m1.off, limit, oend))
+					goto _dest_overflow;
+				ip = start3;
+				m1 = m3;
+
+				start0 = start2;
+				m0 = m2;
+				goto _Search2;
+			}
+
+			start2 = start3;
+			m2 = m3;
+			goto _Search3;
+		}
+
+		/*
+        * OK, now we have 3 ascending matches;
+        * let's write the first one ML1.
+        * ip & ref are known; Now decide ml.
+        */
+		if (start2 < ip + m1.len) {
+			if ((start2 - ip) < OPTIMAL_ML) {
+				int correction;
+				if (m1.len > OPTIMAL_ML)
+					m1.len = OPTIMAL_ML;
+				if (ip + m1.len > start2 + m2.len - MINMATCH)
+					m1.len = (int)(start2 - ip) + m2.len -
+						 MINMATCH;
+				correction = m1.len - (int)(start2 - ip);
+				if (correction > 0) {
+					start2 += correction;
+					m2.len -= correction;
+				}
+			} else {
+				m1.len = (int)(start2 - ip);
+			}
+		}
+		optr = op;
+		if (LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor), m1.len,
+					 m1.off, limit, oend))
+			goto _dest_overflow;
+
+		/* ML2 becomes ML1 */
+		ip = start2;
+		m1 = m2;
+
+		/* ML3 becomes ML2 */
+		start2 = start3;
+		m2 = m3;
+
+		/* let's find a new ML3 */
+		goto _Search3;
+	}
+
+_last_literals:
+	/* Encode Last Literals */
+	{
+		size_t lastRunSize = (size_t)(iend - anchor); /* literals */
+		size_t llAdd = (lastRunSize + 255 - RUN_MASK) / 255;
+		size_t const totalSize = 1 + llAdd + lastRunSize;
+		if (limit == fillOutput)
+			oend += LASTLITERALS; /* restore correct value */
+		if (limit && (op + totalSize > oend)) {
+			if (limit == limitedOutput)
+				return 0;
+			/* adapt lastRunSize to fill 'dest' */
+			lastRunSize = (size_t)(oend - op) - 1 /*token*/;
+			llAdd = (lastRunSize + 256 - RUN_MASK) / 256;
+			lastRunSize -= llAdd;
+		}
+		DEBUGLOG(6, "Final literal run : %i literals",
+			 (int)lastRunSize);
+		ip = anchor +
+		     lastRunSize; /* can be != iend if limit==fillOutput */
+
+		if (lastRunSize >= RUN_MASK) {
+			size_t accumulator = lastRunSize - RUN_MASK;
+			*op++ = (RUN_MASK << ML_BITS);
+			for (; accumulator >= 255; accumulator -= 255)
+				*op++ = 255;
+			*op++ = (BYTE)accumulator;
+		} else {
+			*op++ = (BYTE)(lastRunSize << ML_BITS);
+		}
+		LZ4_memcpy(op, anchor, lastRunSize);
+		op += lastRunSize;
+	}
+
+	/* End */
+	*srcSizePtr = (int)(((const char *)ip) - source);
+	return (int)(((char *)op) - dest);
+
+_dest_overflow:
+	if (limit == fillOutput) {
+		/* Assumption : @ip, @anchor, @optr and @m1 must be set correctly */
+		size_t const ll = (size_t)(ip - anchor);
+		size_t const ll_addbytes = (ll + 240) / 255;
+		size_t const ll_totalCost = 1 + ll_addbytes + ll;
+		BYTE *const maxLitPos =
+			oend - 3; /* 2 for offset, 1 for token */
+		DEBUGLOG(6, "Last sequence overflowing");
+		op = optr; /* restore correct out pointer */
+		if (op + ll_totalCost <= maxLitPos) {
+			/* ll validated; now adjust match length */
+			size_t const bytesLeftForMl =
+				(size_t)(maxLitPos - (op + ll_totalCost));
+			size_t const maxMlSize = MINMATCH + (ML_MASK - 1) +
+						 (bytesLeftForMl * 255);
+			assert(maxMlSize < INT_MAX);
+			assert(m1.len >= 0);
+			if ((size_t)m1.len > maxMlSize)
+				m1.len = (int)maxMlSize;
+			if ((oend + LASTLITERALS) - (op + ll_totalCost + 2) -
+				    1 + m1.len >=
+			    MFLIMIT) {
+				LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor),
+						     m1.len, m1.off, notLimited,
+						     oend);
+			}
+		}
+		goto _last_literals;
+	}
+	/* compression failed */
+	return 0;
+}
+
+static int LZ4HC_compress_optimal(LZ4HC_CCtx_internal *ctx,
+				  const char *const source, char *dst,
+				  int *srcSizePtr, int dstCapacity,
+				  int const nbSearches, size_t sufficient_len,
+				  const limitedOutput_directive limit,
+				  int const fullUpdate,
+				  const dictCtx_directive dict,
+				  const HCfavor_e favorDecSpeed);
+
+LZ4_FORCE_INLINE int LZ4HC_compress_generic_internal(
+	LZ4HC_CCtx_internal *const ctx, const char *const src, char *const dst,
+	int *const srcSizePtr, int const dstCapacity, int cLevel,
+	const limitedOutput_directive limit, const dictCtx_directive dict)
+{
+	DEBUGLOG(5, "LZ4HC_compress_generic_internal(src=%p, srcSize=%d)", src,
+		 *srcSizePtr);
+
+	if (limit == fillOutput && dstCapacity < 1)
+		return 0; /* Impossible to store anything */
+	if ((U32)*srcSizePtr > (U32)LZ4_MAX_INPUT_SIZE)
+		return 0; /* Unsupported input size (too large or negative) */
+
+	ctx->end += *srcSizePtr;
+	{
+		cParams_t const cParam = LZ4HC_getCLevelParams(cLevel);
+		HCfavor_e const favor = ctx->favorDecSpeed ?
+						favorDecompressionSpeed :
+						favorCompressionRatio;
+		int result;
+
+		if (cParam.strat == lz4mid) {
+			result = LZ4MID_compress(ctx, src, dst, srcSizePtr,
+						 dstCapacity, limit, dict);
+		} else if (cParam.strat == lz4hc) {
+			result = LZ4HC_compress_hashChain(
+				ctx, src, dst, srcSizePtr, dstCapacity,
+				cParam.nbSearches, limit, dict);
+		} else {
+			assert(cParam.strat == lz4opt);
+			result = LZ4HC_compress_optimal(
+				ctx, src, dst, srcSizePtr, dstCapacity,
+				cParam.nbSearches, cParam.targetLength, limit,
+				cLevel >= LZ4HC_CLEVEL_MAX, /* ultra mode */
+				dict, favor);
+		}
+		if (result <= 0)
+			ctx->dirty = 1;
+		return result;
+	}
+}
+
+static void LZ4HC_setExternalDict(LZ4HC_CCtx_internal *ctxPtr,
+				  const BYTE *newBlock);
+
+static int LZ4HC_compress_generic_noDictCtx(LZ4HC_CCtx_internal *const ctx,
+					    const char *const src,
+					    char *const dst,
+					    int *const srcSizePtr,
+					    int const dstCapacity, int cLevel,
+					    limitedOutput_directive limit)
+{
+	assert(ctx->dictCtx == NULL);
+	return LZ4HC_compress_generic_internal(ctx, src, dst, srcSizePtr,
+					       dstCapacity, cLevel, limit,
+					       noDictCtx);
+}
+
+static int isStateCompatible(const LZ4HC_CCtx_internal *ctx1,
+			     const LZ4HC_CCtx_internal *ctx2)
+{
+	int const isMid1 =
+		LZ4HC_getCLevelParams(ctx1->compressionLevel).strat == lz4mid;
+	int const isMid2 =
+		LZ4HC_getCLevelParams(ctx2->compressionLevel).strat == lz4mid;
+	return !(isMid1 ^ isMid2);
+}
+
+static int LZ4HC_compress_generic_dictCtx(LZ4HC_CCtx_internal *const ctx,
+					  const char *const src,
+					  char *const dst,
+					  int *const srcSizePtr,
+					  int const dstCapacity, int cLevel,
+					  limitedOutput_directive limit)
+{
+	const size_t position = (size_t)(ctx->end - ctx->prefixStart) +
+				(ctx->dictLimit - ctx->lowLimit);
+	assert(ctx->dictCtx != NULL);
+	if (position >= 64 KB) {
+		ctx->dictCtx = NULL;
+		return LZ4HC_compress_generic_noDictCtx(
+			ctx, src, dst, srcSizePtr, dstCapacity, cLevel, limit);
+	} else if (position == 0 && *srcSizePtr > 4 KB &&
+		   isStateCompatible(ctx, ctx->dictCtx)) {
+		LZ4_memcpy(ctx, ctx->dictCtx, sizeof(LZ4HC_CCtx_internal));
+		LZ4HC_setExternalDict(ctx, (const BYTE *)src);
+		ctx->compressionLevel = (short)cLevel;
+		return LZ4HC_compress_generic_noDictCtx(
+			ctx, src, dst, srcSizePtr, dstCapacity, cLevel, limit);
+	} else {
+		return LZ4HC_compress_generic_internal(ctx, src, dst,
+						       srcSizePtr, dstCapacity,
+						       cLevel, limit,
+						       usingDictCtxHc);
+	}
+}
+
+static int LZ4HC_compress_generic(LZ4HC_CCtx_internal *const ctx,
+				  const char *const src, char *const dst,
+				  int *const srcSizePtr, int const dstCapacity,
+				  int cLevel, limitedOutput_directive limit)
+{
+	if (ctx->dictCtx == NULL) {
+		return LZ4HC_compress_generic_noDictCtx(
+			ctx, src, dst, srcSizePtr, dstCapacity, cLevel, limit);
+	} else {
+		return LZ4HC_compress_generic_dictCtx(
+			ctx, src, dst, srcSizePtr, dstCapacity, cLevel, limit);
+	}
+}
+
+int LZ4_sizeofStateHC(void)
+{
+	return (int)sizeof(LZ4_streamHC_t);
+}
+
+static size_t LZ4_streamHC_t_alignment(void)
+{
+#if LZ4_ALIGN_TEST
+	typedef struct {
+		char c;
+		LZ4_streamHC_t t;
+	} t_a;
+	return sizeof(t_a) - sizeof(LZ4_streamHC_t);
+#else
+	return 1; /* effectively disabled */
+#endif
+}
+
+/* state is presumed correctly initialized,
+ * in which case its size and alignment have already been validate */
+int LZ4_compress_HC_extStateHC_fastReset(void *state, const char *src,
+					 char *dst, int srcSize,
+					 int dstCapacity, int compressionLevel)
+{
+	LZ4HC_CCtx_internal *const ctx =
+		&((LZ4_streamHC_t *)state)->internal_donotuse;
+	if (!LZ4_isAligned(state, LZ4_streamHC_t_alignment()))
+		return 0;
+	LZ4_resetStreamHC_fast((LZ4_streamHC_t *)state, compressionLevel);
+	LZ4HC_init_internal(ctx, (const BYTE *)src);
+	if (dstCapacity < LZ4_compressBound(srcSize))
+		return LZ4HC_compress_generic(ctx, src, dst, &srcSize,
+					      dstCapacity, compressionLevel,
+					      limitedOutput);
+	else
+		return LZ4HC_compress_generic(ctx, src, dst, &srcSize,
+					      dstCapacity, compressionLevel,
+					      notLimited);
+}
+
+int LZ4_compress_HC_extStateHC(void *state, const char *src, char *dst,
+			       int srcSize, int dstCapacity,
+			       int compressionLevel)
+{
+	LZ4_streamHC_t *const ctx = LZ4_initStreamHC(state, sizeof(*ctx));
+	if (ctx == NULL)
+		return 0; /* init failure */
+	return LZ4_compress_HC_extStateHC_fastReset(
+		state, src, dst, srcSize, dstCapacity, compressionLevel);
+}
+
+int LZ4_compress_HC(const char *src, char *dst, int srcSize, int dstCapacity,
+		    int compressionLevel, void *wrkmem)
+{
+	DEBUGLOG(5, "LZ4_compress_HC")
+	return LZ4_compress_HC_extStateHC(wrkmem, src, dst, srcSize,
+					  dstCapacity, compressionLevel);
+}
+EXPORT_SYMBOL(LZ4_compress_HC);
+
+/* state is presumed sized correctly (>= sizeof(LZ4_streamHC_t)) */
+int LZ4_compress_HC_destSize(void *state, const char *source, char *dest,
+			     int *sourceSizePtr, int targetDestSize, int cLevel)
+{
+	LZ4_streamHC_t *const ctx = LZ4_initStreamHC(state, sizeof(*ctx));
+	if (ctx == NULL)
+		return 0; /* init failure */
+	LZ4HC_init_internal(&ctx->internal_donotuse, (const BYTE *)source);
+	LZ4_setCompressionLevel(ctx, cLevel);
+	return LZ4HC_compress_generic(&ctx->internal_donotuse, source, dest,
+				      sourceSizePtr, targetDestSize, cLevel,
+				      fillOutput);
+}
+
+/**************************************
+*  Streaming Functions
+**************************************/
+/* allocation */
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+LZ4_streamHC_t *LZ4_createStreamHC(void)
+{
+	LZ4_streamHC_t *const state =
+		(LZ4_streamHC_t *)ALLOC_AND_ZERO(sizeof(LZ4_streamHC_t));
+	if (state == NULL)
+		return NULL;
+	LZ4_setCompressionLevel(state, LZ4HC_CLEVEL_DEFAULT);
+	return state;
+}
+
+int LZ4_freeStreamHC(LZ4_streamHC_t *LZ4_streamHCPtr)
+{
+	DEBUGLOG(4, "LZ4_freeStreamHC(%p)", LZ4_streamHCPtr);
+	if (!LZ4_streamHCPtr)
+		return 0; /* support free on NULL */
+	FREEMEM(LZ4_streamHCPtr);
+	return 0;
+}
+#endif
+
+LZ4_streamHC_t *LZ4_initStreamHC(void *buffer, size_t size)
+{
+	LZ4_streamHC_t *const LZ4_streamHCPtr = (LZ4_streamHC_t *)buffer;
+	DEBUGLOG(4, "LZ4_initStreamHC(%p, %u)", buffer, (unsigned)size);
+	/* check conditions */
+	if (buffer == NULL)
+		return NULL;
+	if (size < sizeof(LZ4_streamHC_t))
+		return NULL;
+	if (!LZ4_isAligned(buffer, LZ4_streamHC_t_alignment()))
+		return NULL;
+	/* init */
+	{
+		LZ4HC_CCtx_internal *const hcstate =
+			&(LZ4_streamHCPtr->internal_donotuse);
+		MEM_INIT(hcstate, 0, sizeof(*hcstate));
+	}
+	LZ4_setCompressionLevel(LZ4_streamHCPtr, LZ4HC_CLEVEL_DEFAULT);
+	return LZ4_streamHCPtr;
+}
+
+/* just a stub */
+void LZ4_resetStreamHC(LZ4_streamHC_t *LZ4_streamHCPtr, int compressionLevel)
+{
+	LZ4_initStreamHC(LZ4_streamHCPtr, sizeof(*LZ4_streamHCPtr));
+	LZ4_setCompressionLevel(LZ4_streamHCPtr, compressionLevel);
+}
+
+void LZ4_resetStreamHC_fast(LZ4_streamHC_t *LZ4_streamHCPtr,
+			    int compressionLevel)
+{
+	LZ4HC_CCtx_internal *const s = &LZ4_streamHCPtr->internal_donotuse;
+	DEBUGLOG(5, "LZ4_resetStreamHC_fast(%p, %d)", LZ4_streamHCPtr,
+		 compressionLevel);
+	if (s->dirty) {
+		LZ4_initStreamHC(LZ4_streamHCPtr, sizeof(*LZ4_streamHCPtr));
+	} else {
+		assert(s->end >= s->prefixStart);
+		s->dictLimit += (U32)(s->end - s->prefixStart);
+		s->prefixStart = NULL;
+		s->end = NULL;
+		s->dictCtx = NULL;
+	}
+	LZ4_setCompressionLevel(LZ4_streamHCPtr, compressionLevel);
+}
+
+void LZ4_setCompressionLevel(LZ4_streamHC_t *LZ4_streamHCPtr,
+			     int compressionLevel)
+{
+	DEBUGLOG(5, "LZ4_setCompressionLevel(%p, %d)", LZ4_streamHCPtr,
+		 compressionLevel);
+	if (compressionLevel < 1)
+		compressionLevel = LZ4HC_CLEVEL_DEFAULT;
+	if (compressionLevel > LZ4HC_CLEVEL_MAX)
+		compressionLevel = LZ4HC_CLEVEL_MAX;
+	LZ4_streamHCPtr->internal_donotuse.compressionLevel =
+		(short)compressionLevel;
+}
+
+void LZ4_favorDecompressionSpeed(LZ4_streamHC_t *LZ4_streamHCPtr, int favor)
+{
+	LZ4_streamHCPtr->internal_donotuse.favorDecSpeed = (favor != 0);
+}
+
+/* LZ4_loadDictHC() :
+ * LZ4_streamHCPtr is presumed properly initialized */
+int LZ4_loadDictHC(LZ4_streamHC_t *LZ4_streamHCPtr, const char *dictionary,
+		   int dictSize)
+{
+	LZ4HC_CCtx_internal *const ctxPtr = &LZ4_streamHCPtr->internal_donotuse;
+	cParams_t cp;
+	DEBUGLOG(4, "LZ4_loadDictHC(ctx:%p, dict:%p, dictSize:%d, clevel=%d)",
+		 LZ4_streamHCPtr, dictionary, dictSize,
+		 ctxPtr->compressionLevel);
+	assert(dictSize >= 0);
+	assert(LZ4_streamHCPtr != NULL);
+	if (dictSize > 64 KB) {
+		dictionary += (size_t)dictSize - 64 KB;
+		dictSize = 64 KB;
+	}
+	/* need a full initialization, there are bad side-effects when using resetFast() */
+	{
+		int const cLevel = ctxPtr->compressionLevel;
+		LZ4_initStreamHC(LZ4_streamHCPtr, sizeof(*LZ4_streamHCPtr));
+		LZ4_setCompressionLevel(LZ4_streamHCPtr, cLevel);
+		cp = LZ4HC_getCLevelParams(cLevel);
+	}
+	LZ4HC_init_internal(ctxPtr, (const BYTE *)dictionary);
+	ctxPtr->end = (const BYTE *)dictionary + dictSize;
+	if (cp.strat == lz4mid) {
+		LZ4MID_fillHTable(ctxPtr, dictionary, (size_t)dictSize);
+	} else {
+		if (dictSize >= LZ4HC_HASHSIZE)
+			LZ4HC_Insert(ctxPtr, ctxPtr->end - 3);
+	}
+	return dictSize;
+}
+EXPORT_SYMBOL(LZ4_loadDictHC);
+
+void LZ4_attach_HC_dictionary(LZ4_streamHC_t *working_stream,
+			      const LZ4_streamHC_t *dictionary_stream)
+{
+	working_stream->internal_donotuse.dictCtx =
+		dictionary_stream != NULL ?
+			&(dictionary_stream->internal_donotuse) :
+			NULL;
+}
+
+/* compression */
+
+static void LZ4HC_setExternalDict(LZ4HC_CCtx_internal *ctxPtr,
+				  const BYTE *newBlock)
+{
+	DEBUGLOG(4, "LZ4HC_setExternalDict(%p, %p)", ctxPtr, newBlock);
+	if ((ctxPtr->end >= ctxPtr->prefixStart + 4) &&
+	    (LZ4HC_getCLevelParams(ctxPtr->compressionLevel).strat != lz4mid)) {
+		LZ4HC_Insert(
+			ctxPtr,
+			ctxPtr->end -
+				3); /* Referencing remaining dictionary content */
+	}
+
+	/* Only one memory segment for extDict, so any previous extDict is lost at this stage */
+	ctxPtr->lowLimit = ctxPtr->dictLimit;
+	ctxPtr->dictStart = ctxPtr->prefixStart;
+	ctxPtr->dictLimit += (U32)(ctxPtr->end - ctxPtr->prefixStart);
+	ctxPtr->prefixStart = newBlock;
+	ctxPtr->end = newBlock;
+	ctxPtr->nextToUpdate =
+		ctxPtr->dictLimit; /* match referencing will resume from there */
+
+	/* cannot reference an extDict and a dictCtx at the same time */
+	ctxPtr->dictCtx = NULL;
+}
+
+static int LZ4_compressHC_continue_generic(LZ4_streamHC_t *LZ4_streamHCPtr,
+					   const char *src, char *dst,
+					   int *srcSizePtr, int dstCapacity,
+					   limitedOutput_directive limit)
+{
+	LZ4HC_CCtx_internal *const ctxPtr = &LZ4_streamHCPtr->internal_donotuse;
+	DEBUGLOG(
+		5,
+		"LZ4_compressHC_continue_generic(ctx=%p, src=%p, srcSize=%d, limit=%d)",
+		LZ4_streamHCPtr, src, *srcSizePtr, limit);
+	assert(ctxPtr != NULL);
+	/* auto-init if forgotten */
+	if (ctxPtr->prefixStart == NULL)
+		LZ4HC_init_internal(ctxPtr, (const BYTE *)src);
+
+	/* Check overflow */
+	if ((size_t)(ctxPtr->end - ctxPtr->prefixStart) + ctxPtr->dictLimit >
+	    2 GB) {
+		size_t dictSize = (size_t)(ctxPtr->end - ctxPtr->prefixStart);
+		if (dictSize > 64 KB)
+			dictSize = 64 KB;
+		LZ4_loadDictHC(LZ4_streamHCPtr,
+			       (const char *)(ctxPtr->end) - dictSize,
+			       (int)dictSize);
+	}
+
+	/* Check if blocks follow each other */
+	if ((const BYTE *)src != ctxPtr->end)
+		LZ4HC_setExternalDict(ctxPtr, (const BYTE *)src);
+
+	/* Check overlapping input/dictionary space */
+	{
+		const BYTE *sourceEnd = (const BYTE *)src + *srcSizePtr;
+		const BYTE *const dictBegin = ctxPtr->dictStart;
+		const BYTE *const dictEnd =
+			ctxPtr->dictStart +
+			(ctxPtr->dictLimit - ctxPtr->lowLimit);
+		if ((sourceEnd > dictBegin) && ((const BYTE *)src < dictEnd)) {
+			if (sourceEnd > dictEnd)
+				sourceEnd = dictEnd;
+			ctxPtr->lowLimit +=
+				(U32)(sourceEnd - ctxPtr->dictStart);
+			ctxPtr->dictStart +=
+				(U32)(sourceEnd - ctxPtr->dictStart);
+			/* invalidate dictionary is it's too small */
+			if (ctxPtr->dictLimit - ctxPtr->lowLimit <
+			    LZ4HC_HASHSIZE) {
+				ctxPtr->lowLimit = ctxPtr->dictLimit;
+				ctxPtr->dictStart = ctxPtr->prefixStart;
+			}
+		}
+	}
+
+	return LZ4HC_compress_generic(ctxPtr, src, dst, srcSizePtr, dstCapacity,
+				      ctxPtr->compressionLevel, limit);
+}
+
+int LZ4_compress_HC_continue(LZ4_streamHC_t *LZ4_streamHCPtr, const char *src,
+			     char *dst, int srcSize, int dstCapacity)
+{
+	DEBUGLOG(5, "LZ4_compress_HC_continue");
+	if (dstCapacity < LZ4_compressBound(srcSize))
+		return LZ4_compressHC_continue_generic(LZ4_streamHCPtr, src,
+						       dst, &srcSize,
+						       dstCapacity,
+						       limitedOutput);
+	else
+		return LZ4_compressHC_continue_generic(LZ4_streamHCPtr, src,
+						       dst, &srcSize,
+						       dstCapacity, notLimited);
+}
+EXPORT_SYMBOL(LZ4_compress_HC_continue);
+
+int LZ4_compress_HC_continue_destSize(LZ4_streamHC_t *LZ4_streamHCPtr,
+				      const char *src, char *dst,
+				      int *srcSizePtr, int targetDestSize)
+{
+	return LZ4_compressHC_continue_generic(LZ4_streamHCPtr, src, dst,
+					       srcSizePtr, targetDestSize,
+					       fillOutput);
+}
+
+/* LZ4_saveDictHC :
+ * save history content
+ * into a user-provided buffer
+ * which is then used to continue compression
+ */
+int LZ4_saveDictHC(LZ4_streamHC_t *LZ4_streamHCPtr, char *safeBuffer,
+		   int dictSize)
+{
+	LZ4HC_CCtx_internal *const streamPtr =
+		&LZ4_streamHCPtr->internal_donotuse;
+	int const prefixSize = (int)(streamPtr->end - streamPtr->prefixStart);
+	DEBUGLOG(5, "LZ4_saveDictHC(%p, %p, %d)", LZ4_streamHCPtr, safeBuffer,
+		 dictSize);
+	assert(prefixSize >= 0);
+	if (dictSize > 64 KB)
+		dictSize = 64 KB;
+	if (dictSize < 4)
+		dictSize = 0;
+	if (dictSize > prefixSize)
+		dictSize = prefixSize;
+	if (safeBuffer == NULL)
+		assert(dictSize == 0);
+	if (dictSize > 0)
+		LZ4_memmove(safeBuffer, streamPtr->end - dictSize,
+			    (size_t)dictSize);
+	{
+		U32 const endIndex =
+			(U32)(streamPtr->end - streamPtr->prefixStart) +
+			streamPtr->dictLimit;
+		streamPtr->end = (safeBuffer == NULL) ?
+					 NULL :
+					 (const BYTE *)safeBuffer + dictSize;
+		streamPtr->prefixStart = (const BYTE *)safeBuffer;
+		streamPtr->dictLimit = endIndex - (U32)dictSize;
+		streamPtr->lowLimit = endIndex - (U32)dictSize;
+		streamPtr->dictStart = streamPtr->prefixStart;
+		if (streamPtr->nextToUpdate < streamPtr->dictLimit)
+			streamPtr->nextToUpdate = streamPtr->dictLimit;
+	}
+	return dictSize;
+}
+EXPORT_SYMBOL(LZ4_saveDictHC);
+
+/* ================================================
+ *  LZ4 Optimal parser (levels [LZ4HC_CLEVEL_OPT_MIN - LZ4HC_CLEVEL_MAX])
+ * ===============================================*/
+typedef struct {
+	int price;
+	int off;
+	int mlen;
+	int litlen;
+} LZ4HC_optimal_t;
+
+/* price in bytes */
+LZ4_FORCE_INLINE int LZ4HC_literalsPrice(int const litlen)
+{
+	int price = litlen;
+	assert(litlen >= 0);
+	if (litlen >= (int)RUN_MASK)
+		price += 1 + ((litlen - (int)RUN_MASK) / 255);
+	return price;
+}
+
+/* requires mlen >= MINMATCH */
+LZ4_FORCE_INLINE int LZ4HC_sequencePrice(int litlen, int mlen)
+{
+	int price = 1 + 2; /* token + 16-bit offset */
+	assert(litlen >= 0);
+	assert(mlen >= MINMATCH);
+
+	price += LZ4HC_literalsPrice(litlen);
+
+	if (mlen >= (int)(ML_MASK + MINMATCH))
+		price += 1 + ((mlen - (int)(ML_MASK + MINMATCH)) / 255);
+
+	return price;
+}
+
+LZ4_FORCE_INLINE LZ4HC_match_t LZ4HC_FindLongerMatch(
+	LZ4HC_CCtx_internal *const ctx, const BYTE *ip,
+	const BYTE *const iHighLimit, int minLen, int nbSearches,
+	const dictCtx_directive dict, const HCfavor_e favorDecSpeed)
+{
+	LZ4HC_match_t const match0 = { 0, 0, 0 };
+	/* note : LZ4HC_InsertAndGetWiderMatch() is able to modify the starting position of a match (*startpos),
+     * but this won't be the case here, as we define iLowLimit==ip,
+    ** so LZ4HC_InsertAndGetWiderMatch() won't be allowed to search past ip */
+	LZ4HC_match_t md = LZ4HC_InsertAndGetWiderMatch(
+		ctx, ip, ip, iHighLimit, minLen, nbSearches,
+		1 /*patternAnalysis*/, 1 /*chainSwap*/, dict, favorDecSpeed);
+	assert(md.back == 0);
+	if (md.len <= minLen)
+		return match0;
+	if (favorDecSpeed) {
+		if ((md.len > 18) & (md.len <= 36))
+			md.len = 18; /* favor dec.speed (shortcut) */
+	}
+	return md;
+}
+
+static int LZ4HC_compress_optimal(LZ4HC_CCtx_internal *ctx,
+				  const char *const source, char *dst,
+				  int *srcSizePtr, int dstCapacity,
+				  int const nbSearches, size_t sufficient_len,
+				  const limitedOutput_directive limit,
+				  int const fullUpdate,
+				  const dictCtx_directive dict,
+				  const HCfavor_e favorDecSpeed)
+{
+	int retval = 0;
+#define TRAILING_LITERALS 3
+#if defined(LZ4HC_HEAPMODE) && LZ4HC_HEAPMODE == 1
+	LZ4HC_optimal_t *const opt = (LZ4HC_optimal_t *)ALLOC(
+		sizeof(LZ4HC_optimal_t) * (LZ4_OPT_NUM + TRAILING_LITERALS));
+#else
+	LZ4HC_optimal_t
+		opt[LZ4_OPT_NUM +
+		    TRAILING_LITERALS]; /* ~64 KB, which is a bit large for stack... */
+#endif
+
+	const BYTE *ip = (const BYTE *)source;
+	const BYTE *anchor = ip;
+	const BYTE *const iend = ip + *srcSizePtr;
+	const BYTE *const mflimit = iend - MFLIMIT;
+	const BYTE *const matchlimit = iend - LASTLITERALS;
+	BYTE *op = (BYTE *)dst;
+	BYTE *opSaved = (BYTE *)dst;
+	BYTE *oend = op + dstCapacity;
+	int ovml = MINMATCH; /* overflow - last sequence */
+	int ovoff = 0;
+
+	/* init */
+#if defined(LZ4HC_HEAPMODE) && LZ4HC_HEAPMODE == 1
+	if (opt == NULL)
+		goto _return_label;
+#endif
+	DEBUGLOG(5, "LZ4HC_compress_optimal(dst=%p, dstCapa=%u)", dst,
+		 (unsigned)dstCapacity);
+	*srcSizePtr = 0;
+	if (limit == fillOutput)
+		oend -= LASTLITERALS; /* Hack for support LZ4 format restriction */
+	if (sufficient_len >= LZ4_OPT_NUM)
+		sufficient_len = LZ4_OPT_NUM - 1;
+
+	/* Main Loop */
+	while (ip <= mflimit) {
+		int const llen = (int)(ip - anchor);
+		int best_mlen, best_off;
+		int cur, last_match_pos = 0;
+
+		LZ4HC_match_t const firstMatch =
+			LZ4HC_FindLongerMatch(ctx, ip, matchlimit, MINMATCH - 1,
+					      nbSearches, dict, favorDecSpeed);
+		if (firstMatch.len == 0) {
+			ip++;
+			continue;
+		}
+
+		if ((size_t)firstMatch.len > sufficient_len) {
+			/* good enough solution : immediate encoding */
+			int const firstML = firstMatch.len;
+			opSaved = op;
+			if (LZ4HC_encodeSequence(
+				    UPDATABLE(ip, op, anchor), firstML,
+				    firstMatch.off, limit,
+				    oend)) { /* updates ip, op and anchor */
+				ovml = firstML;
+				ovoff = firstMatch.off;
+				goto _dest_overflow;
+			}
+			continue;
+		}
+
+		/* set prices for first positions (literals) */
+		{
+			int rPos;
+			for (rPos = 0; rPos < MINMATCH; rPos++) {
+				int const cost =
+					LZ4HC_literalsPrice(llen + rPos);
+				opt[rPos].mlen = 1;
+				opt[rPos].off = 0;
+				opt[rPos].litlen = llen + rPos;
+				opt[rPos].price = cost;
+				DEBUGLOG(
+					7,
+					"rPos:%3i => price:%3i (litlen=%i) -- initial setup",
+					rPos, cost, opt[rPos].litlen);
+			}
+		}
+		/* set prices using initial match */
+		{
+			int const matchML =
+				firstMatch
+					.len; /* necessarily < sufficient_len < LZ4_OPT_NUM */
+			int const offset = firstMatch.off;
+			int mlen;
+			assert(matchML < LZ4_OPT_NUM);
+			for (mlen = MINMATCH; mlen <= matchML; mlen++) {
+				int const cost =
+					LZ4HC_sequencePrice(llen, mlen);
+				opt[mlen].mlen = mlen;
+				opt[mlen].off = offset;
+				opt[mlen].litlen = llen;
+				opt[mlen].price = cost;
+				DEBUGLOG(
+					7,
+					"rPos:%3i => price:%3i (matchlen=%i) -- initial setup",
+					mlen, cost, mlen);
+			}
+		}
+		last_match_pos = firstMatch.len;
+		{
+			int addLit;
+			for (addLit = 1; addLit <= TRAILING_LITERALS;
+			     addLit++) {
+				opt[last_match_pos + addLit].mlen =
+					1; /* literal */
+				opt[last_match_pos + addLit].off = 0;
+				opt[last_match_pos + addLit].litlen = addLit;
+				opt[last_match_pos + addLit].price =
+					opt[last_match_pos].price +
+					LZ4HC_literalsPrice(addLit);
+				DEBUGLOG(
+					7,
+					"rPos:%3i => price:%3i (litlen=%i) -- initial setup",
+					last_match_pos + addLit,
+					opt[last_match_pos + addLit].price,
+					addLit);
+			}
+		}
+
+		/* check further positions */
+		for (cur = 1; cur < last_match_pos; cur++) {
+			const BYTE *const curPtr = ip + cur;
+			LZ4HC_match_t newMatch;
+
+			if (curPtr > mflimit)
+				break;
+			DEBUGLOG(7, "rPos:%u[%u] vs [%u]%u", cur,
+				 opt[cur].price, opt[cur + 1].price, cur + 1);
+			if (fullUpdate) {
+				/* not useful to search here if next position has same (or lower) cost */
+				if ((opt[cur + 1].price <= opt[cur].price)
+				    /* in some cases, next position has same cost, but cost rises sharply after, so a small match would still be beneficial */
+				    && (opt[cur + MINMATCH].price <
+					opt[cur].price + 3 /*min seq price*/))
+					continue;
+			} else {
+				/* not useful to search here if next position has same (or lower) cost */
+				if (opt[cur + 1].price <= opt[cur].price)
+					continue;
+			}
+
+			DEBUGLOG(7, "search at rPos:%u", cur);
+			if (fullUpdate)
+				newMatch = LZ4HC_FindLongerMatch(
+					ctx, curPtr, matchlimit, MINMATCH - 1,
+					nbSearches, dict, favorDecSpeed);
+			else
+				/* only test matches of minimum length; slightly faster, but misses a few bytes */
+				newMatch = LZ4HC_FindLongerMatch(
+					ctx, curPtr, matchlimit,
+					last_match_pos - cur, nbSearches, dict,
+					favorDecSpeed);
+			if (!newMatch.len)
+				continue;
+
+			if (((size_t)newMatch.len > sufficient_len) ||
+			    (newMatch.len + cur >= LZ4_OPT_NUM)) {
+				/* immediate encoding */
+				best_mlen = newMatch.len;
+				best_off = newMatch.off;
+				last_match_pos = cur + 1;
+				goto encode;
+			}
+
+			/* before match : set price with literals at beginning */
+			{
+				int const baseLitlen = opt[cur].litlen;
+				int litlen;
+				for (litlen = 1; litlen < MINMATCH; litlen++) {
+					int const price =
+						opt[cur].price -
+						LZ4HC_literalsPrice(
+							baseLitlen) +
+						LZ4HC_literalsPrice(baseLitlen +
+								    litlen);
+					int const pos = cur + litlen;
+					if (price < opt[pos].price) {
+						opt[pos].mlen = 1; /* literal */
+						opt[pos].off = 0;
+						opt[pos].litlen =
+							baseLitlen + litlen;
+						opt[pos].price = price;
+						DEBUGLOG(
+							7,
+							"rPos:%3i => price:%3i (litlen=%i)",
+							pos, price,
+							opt[pos].litlen);
+					}
+				}
+			}
+
+			/* set prices using match at position = cur */
+			{
+				int const matchML = newMatch.len;
+				int ml = MINMATCH;
+
+				assert(cur + newMatch.len < LZ4_OPT_NUM);
+				for (; ml <= matchML; ml++) {
+					int const pos = cur + ml;
+					int const offset = newMatch.off;
+					int price;
+					int ll;
+					DEBUGLOG(
+						7,
+						"testing price rPos %i (last_match_pos=%i)",
+						pos, last_match_pos);
+					if (opt[cur].mlen == 1) {
+						ll = opt[cur].litlen;
+						price = ((cur > ll) ?
+								 opt[cur - ll]
+									 .price :
+								 0) +
+							LZ4HC_sequencePrice(ll,
+									    ml);
+					} else {
+						ll = 0;
+						price = opt[cur].price +
+							LZ4HC_sequencePrice(0,
+									    ml);
+					}
+
+					assert((U32)favorDecSpeed <= 1);
+					if (pos > last_match_pos +
+							    TRAILING_LITERALS ||
+					    price <=
+						    opt[pos].price -
+							    (int)favorDecSpeed) {
+						DEBUGLOG(
+							7,
+							"rPos:%3i => price:%3i (matchlen=%i)",
+							pos, price, ml);
+						assert(pos < LZ4_OPT_NUM);
+						if ((ml ==
+						     matchML) /* last pos of last match */
+						    && (last_match_pos < pos))
+							last_match_pos = pos;
+						opt[pos].mlen = ml;
+						opt[pos].off = offset;
+						opt[pos].litlen = ll;
+						opt[pos].price = price;
+					}
+				}
+			}
+			/* complete following positions with literals */
+			{
+				int addLit;
+				for (addLit = 1; addLit <= TRAILING_LITERALS;
+				     addLit++) {
+					opt[last_match_pos + addLit].mlen =
+						1; /* literal */
+					opt[last_match_pos + addLit].off = 0;
+					opt[last_match_pos + addLit].litlen =
+						addLit;
+					opt[last_match_pos + addLit].price =
+						opt[last_match_pos].price +
+						LZ4HC_literalsPrice(addLit);
+					DEBUGLOG(
+						7,
+						"rPos:%3i => price:%3i (litlen=%i)",
+						last_match_pos + addLit,
+						opt[last_match_pos + addLit]
+							.price,
+						addLit);
+				}
+			}
+		} /* for (cur = 1; cur <= last_match_pos; cur++) */
+
+		assert(last_match_pos < LZ4_OPT_NUM + TRAILING_LITERALS);
+		best_mlen = opt[last_match_pos].mlen;
+		best_off = opt[last_match_pos].off;
+		cur = last_match_pos - best_mlen;
+
+	encode: /* cur, last_match_pos, best_mlen, best_off must be set */
+		assert(cur < LZ4_OPT_NUM);
+		assert(last_match_pos >= 1); /* == 1 when only one candidate */
+		DEBUGLOG(
+			6,
+			"reverse traversal, looking for shortest path (last_match_pos=%i)",
+			last_match_pos);
+		{
+			int candidate_pos = cur;
+			int selected_matchLength = best_mlen;
+			int selected_offset = best_off;
+			while (1) { /* from end to beginning */
+				int const next_matchLength =
+					opt[candidate_pos]
+						.mlen; /* can be 1, means literal */
+				int const next_offset = opt[candidate_pos].off;
+				DEBUGLOG(7, "pos %i: sequence length %i",
+					 candidate_pos, selected_matchLength);
+				opt[candidate_pos].mlen = selected_matchLength;
+				opt[candidate_pos].off = selected_offset;
+				selected_matchLength = next_matchLength;
+				selected_offset = next_offset;
+				if (next_matchLength > candidate_pos)
+					break; /* last match elected, first match to encode */
+				assert(next_matchLength >
+				       0); /* can be 1, means literal */
+				candidate_pos -= next_matchLength;
+			}
+		}
+
+		/* encode all recorded sequences in order */
+		{
+			int rPos = 0; /* relative position (to ip) */
+			while (rPos < last_match_pos) {
+				int const ml = opt[rPos].mlen;
+				int const offset = opt[rPos].off;
+				if (ml == 1) {
+					ip++;
+					rPos++;
+					continue;
+				} /* literal; note: can end up with several literals, in which case, skip them */
+				rPos += ml;
+				assert(ml >= MINMATCH);
+				assert((offset >= 1) &&
+				       (offset <= LZ4_DISTANCE_MAX));
+				opSaved = op;
+				if (LZ4HC_encodeSequence(
+					    UPDATABLE(ip, op, anchor), ml,
+					    offset, limit,
+					    oend)) { /* updates ip, op and anchor */
+					ovml = ml;
+					ovoff = offset;
+					goto _dest_overflow;
+				}
+			}
+		}
+	} /* while (ip <= mflimit) */
+
+_last_literals:
+	/* Encode Last Literals */
+	{
+		size_t lastRunSize = (size_t)(iend - anchor); /* literals */
+		size_t llAdd = (lastRunSize + 255 - RUN_MASK) / 255;
+		size_t const totalSize = 1 + llAdd + lastRunSize;
+		if (limit == fillOutput)
+			oend += LASTLITERALS; /* restore correct value */
+		if (limit && (op + totalSize > oend)) {
+			if (limit == limitedOutput) { /* Check output limit */
+				retval = 0;
+				goto _return_label;
+			}
+			/* adapt lastRunSize to fill 'dst' */
+			lastRunSize = (size_t)(oend - op) - 1 /*token*/;
+			llAdd = (lastRunSize + 256 - RUN_MASK) / 256;
+			lastRunSize -= llAdd;
+		}
+		DEBUGLOG(6, "Final literal run : %i literals",
+			 (int)lastRunSize);
+		ip = anchor +
+		     lastRunSize; /* can be != iend if limit==fillOutput */
+
+		if (lastRunSize >= RUN_MASK) {
+			size_t accumulator = lastRunSize - RUN_MASK;
+			*op++ = (RUN_MASK << ML_BITS);
+			for (; accumulator >= 255; accumulator -= 255)
+				*op++ = 255;
+			*op++ = (BYTE)accumulator;
+		} else {
+			*op++ = (BYTE)(lastRunSize << ML_BITS);
+		}
+		LZ4_memcpy(op, anchor, lastRunSize);
+		op += lastRunSize;
+	}
+
+	/* End */
+	*srcSizePtr = (int)(((const char *)ip) - source);
+	retval = (int)((char *)op - dst);
+	goto _return_label;
+
+_dest_overflow:
+	if (limit == fillOutput) {
+		/* Assumption : ip, anchor, ovml and ovref must be set correctly */
+		size_t const ll = (size_t)(ip - anchor);
+		size_t const ll_addbytes = (ll + 240) / 255;
+		size_t const ll_totalCost = 1 + ll_addbytes + ll;
+		BYTE *const maxLitPos =
+			oend - 3; /* 2 for offset, 1 for token */
+		DEBUGLOG(6,
+			 "Last sequence overflowing (only %i bytes remaining)",
+			 (int)(oend - 1 - opSaved));
+		op = opSaved; /* restore correct out pointer */
+		if (op + ll_totalCost <= maxLitPos) {
+			/* ll validated; now adjust match length */
+			size_t const bytesLeftForMl =
+				(size_t)(maxLitPos - (op + ll_totalCost));
+			size_t const maxMlSize = MINMATCH + (ML_MASK - 1) +
+						 (bytesLeftForMl * 255);
+			assert(maxMlSize < INT_MAX);
+			assert(ovml >= 0);
+			if ((size_t)ovml > maxMlSize)
+				ovml = (int)maxMlSize;
+			if ((oend + LASTLITERALS) - (op + ll_totalCost + 2) -
+				    1 + ovml >=
+			    MFLIMIT) {
+				DEBUGLOG(6, "Space to end : %i + ml (%i)",
+					 (int)((oend + LASTLITERALS) -
+					       (op + ll_totalCost + 2) - 1),
+					 ovml);
+				DEBUGLOG(6, "Before : ip = %p, anchor = %p", ip,
+					 anchor);
+				LZ4HC_encodeSequence(UPDATABLE(ip, op, anchor),
+						     ovml, ovoff, notLimited,
+						     oend);
+				DEBUGLOG(6, "After : ip = %p, anchor = %p", ip,
+					 anchor);
+			}
+		}
+		goto _last_literals;
+	}
+_return_label:
+#if defined(LZ4HC_HEAPMODE) && LZ4HC_HEAPMODE == 1
+	if (opt)
+		FREEMEM(opt);
+#endif
+	return retval;
+}
+
+/* state is presumed correctly sized, aka >= sizeof(LZ4_streamHC_t)
+ * @return : 0 on success, !=0 if error */
+int LZ4_resetStreamStateHC(void *state, char *inputBuffer)
+{
+	LZ4_streamHC_t *const hc4 = LZ4_initStreamHC(state, sizeof(*hc4));
+	if (hc4 == NULL)
+		return 1; /* init failed */
+	LZ4HC_init_internal(&hc4->internal_donotuse, (const BYTE *)inputBuffer);
+	return 0;
+}
+
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+void *LZ4_createHC(const char *inputBuffer)
+{
+	LZ4_streamHC_t *const hc4 = LZ4_createStreamHC();
+	if (hc4 == NULL)
+		return NULL; /* not enough memory */
+	LZ4HC_init_internal(&hc4->internal_donotuse, (const BYTE *)inputBuffer);
+	return hc4;
+}
+
+int LZ4_freeHC(void *LZ4HC_Data)
+{
+	if (!LZ4HC_Data)
+		return 0; /* support free on NULL */
+	FREEMEM(LZ4HC_Data);
+	return 0;
+}
+#endif
+
+int LZ4_compressHC2_continue(void *LZ4HC_Data, const char *src, char *dst,
+			     int srcSize, int cLevel)
+{
+	return LZ4HC_compress_generic(
+		&((LZ4_streamHC_t *)LZ4HC_Data)->internal_donotuse, src, dst,
+		&srcSize, 0, cLevel, notLimited);
+}
+
+int LZ4_compressHC2_limitedOutput_continue(void *LZ4HC_Data, const char *src,
+					   char *dst, int srcSize,
+					   int dstCapacity, int cLevel)
+{
+	return LZ4HC_compress_generic(
+		&((LZ4_streamHC_t *)LZ4HC_Data)->internal_donotuse, src, dst,
+		&srcSize, dstCapacity, cLevel, limitedOutput);
+}
+
+char *LZ4_slideInputBufferHC(void *LZ4HC_Data)
+{
+	LZ4HC_CCtx_internal *const s =
+		&((LZ4_streamHC_t *)LZ4HC_Data)->internal_donotuse;
+	const BYTE *const bufferStart =
+		s->prefixStart - s->dictLimit + s->lowLimit;
+	LZ4_resetStreamHC_fast((LZ4_streamHC_t *)LZ4HC_Data,
+			       s->compressionLevel);
+	/* ugly conversion trick, required to evade (const char*) -> (char*) cast-qual warning :( */
+	return (char *)(uptrval)bufferStart;
+}
Index: lib/lz4/lz4hc.h
===================================================================
diff --git a/lib/lz4/lz4hc.h b/lib/lz4/lz4hc.h
new file mode 100644
--- /dev/null	(revision b2497e4243461a835c25469028cd355bfc2e993f)
+++ b/lib/lz4/lz4hc.h	(revision b2497e4243461a835c25469028cd355bfc2e993f)
@@ -0,0 +1,451 @@
+/*
+   LZ4 HC - High Compression Mode of LZ4
+   Header File
+   Copyright (C) 2011-2020, Yann Collet.
+   BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)
+
+   Redistribution and use in source and binary forms, with or without
+   modification, are permitted provided that the following conditions are
+   met:
+
+       * Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+       * Redistributions in binary form must reproduce the above
+   copyright notice, this list of conditions and the following disclaimer
+   in the documentation and/or other materials provided with the
+   distribution.
+
+   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+   You can contact the author at :
+   - LZ4 source repository : https://github.com/lz4/lz4
+   - LZ4 public forum : https://groups.google.com/forum/#!forum/lz4c
+*/
+#ifndef LZ4_HC_H_19834876238432
+#define LZ4_HC_H_19834876238432
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+/* --- Dependency --- */
+/* note : lz4hc requires lz4.h/lz4.c for compilation */
+#include "lz4.h" /* stddef, LZ4LIB_API, LZ4_DEPRECATED */
+
+/* --- Useful constants --- */
+#define LZ4HC_CLEVEL_MIN 2
+#define LZ4HC_CLEVEL_DEFAULT 9
+#define LZ4HC_CLEVEL_OPT_MIN 10
+#define LZ4HC_CLEVEL_MAX 12
+
+/*-************************************
+ *  Block Compression
+ **************************************/
+/*! LZ4_compress_HC() :
+ *  Compress data from `src` into `dst`, using the powerful but slower "HC" algorithm.
+ * `dst` must be already allocated.
+ *  Compression is guaranteed to succeed if `dstCapacity >= LZ4_compressBound(srcSize)` (see "lz4.h")
+ *  Max supported `srcSize` value is LZ4_MAX_INPUT_SIZE (see "lz4.h")
+ * `compressionLevel` : any value between 1 and LZ4HC_CLEVEL_MAX will work.
+ *                      Values > LZ4HC_CLEVEL_MAX behave the same as LZ4HC_CLEVEL_MAX.
+ * @return : the number of bytes written into 'dst'
+ *           or 0 if compression fails.
+ */
+LZ4LIB_API int LZ4_compress_HC(const char *src, char *dst, int srcSize,
+			       int dstCapacity, int compressionLevel,
+			       void *wrkmem);
+
+/* Note :
+ *   Decompression functions are provided within "lz4.h" (BSD license)
+ */
+
+/*! LZ4_compress_HC_extStateHC() :
+ *  Same as LZ4_compress_HC(), but using an externally allocated memory segment for `state`.
+ * `state` size is provided by LZ4_sizeofStateHC().
+ *  Memory segment must be aligned on 8-bytes boundaries (which a normal malloc() should do properly).
+ */
+LZ4LIB_API int LZ4_sizeofStateHC(void);
+LZ4LIB_API int LZ4_compress_HC_extStateHC(void *stateHC, const char *src,
+					  char *dst, int srcSize,
+					  int maxDstSize, int compressionLevel);
+
+/*! LZ4_compress_HC_destSize() : v1.9.0+
+ *  Will compress as much data as possible from `src`
+ *  to fit into `targetDstSize` budget.
+ *  Result is provided in 2 parts :
+ * @return : the number of bytes written into 'dst' (necessarily <= targetDstSize)
+ *           or 0 if compression fails.
+ * `srcSizePtr` : on success, *srcSizePtr is updated to indicate how much bytes were read from `src`
+ */
+LZ4LIB_API int LZ4_compress_HC_destSize(void *stateHC, const char *src,
+					char *dst, int *srcSizePtr,
+					int targetDstSize,
+					int compressionLevel);
+
+/*-************************************
+ *  Streaming Compression
+ *  Bufferless synchronous API
+ **************************************/
+typedef union LZ4_streamHC_u LZ4_streamHC_t; /* incomplete type (defined later) */
+
+/*! LZ4_createStreamHC() and LZ4_freeStreamHC() :
+ *  These functions create and release memory for LZ4 HC streaming state.
+ *  Newly created states are automatically initialized.
+ *  A same state can be used multiple times consecutively,
+ *  starting with LZ4_resetStreamHC_fast() to start a new stream of blocks.
+ */
+LZ4LIB_API LZ4_streamHC_t *LZ4_createStreamHC(void);
+LZ4LIB_API int LZ4_freeStreamHC(LZ4_streamHC_t *streamHCPtr);
+
+/*
+  These functions compress data in successive blocks of any size,
+  using previous blocks as dictionary, to improve compression ratio.
+  One key assumption is that previous blocks (up to 64 KB) remain read-accessible while compressing next blocks.
+  There is an exception for ring buffers, which can be smaller than 64 KB.
+  Ring-buffer scenario is automatically detected and handled within LZ4_compress_HC_continue().
+
+  Before starting compression, state must be allocated and properly initialized.
+  LZ4_createStreamHC() does both, though compression level is set to LZ4HC_CLEVEL_DEFAULT.
+
+  Selecting the compression level can be done with LZ4_resetStreamHC_fast() (starts a new stream)
+  or LZ4_setCompressionLevel() (anytime, between blocks in the same stream) (experimental).
+  LZ4_resetStreamHC_fast() only works on states which have been properly initialized at least once,
+  which is automatically the case when state is created using LZ4_createStreamHC().
+
+  After reset, a first "fictional block" can be designated as initial dictionary,
+  using LZ4_loadDictHC() (Optional).
+  Note: In order for LZ4_loadDictHC() to create the correct data structure,
+  it is essential to set the compression level _before_ loading the dictionary.
+
+  Invoke LZ4_compress_HC_continue() to compress each successive block.
+  The number of blocks is unlimited.
+  Previous input blocks, including initial dictionary when present,
+  must remain accessible and unmodified during compression.
+
+  It's allowed to update compression level anytime between blocks,
+  using LZ4_setCompressionLevel() (experimental).
+
+ @dst buffer should be sized to handle worst case scenarios
+  (see LZ4_compressBound(), it ensures compression success).
+  In case of failure, the API does not guarantee recovery,
+  so the state _must_ be reset.
+  To ensure compression success
+  whenever @dst buffer size cannot be made >= LZ4_compressBound(),
+  consider using LZ4_compress_HC_continue_destSize().
+
+  Whenever previous input blocks can't be preserved unmodified in-place during compression of next blocks,
+  it's possible to copy the last blocks into a more stable memory space, using LZ4_saveDictHC().
+  Return value of LZ4_saveDictHC() is the size of dictionary effectively saved into 'safeBuffer' (<= 64 KB)
+
+  After completing a streaming compression,
+  it's possible to start a new stream of blocks, using the same LZ4_streamHC_t state,
+  just by resetting it, using LZ4_resetStreamHC_fast().
+*/
+
+LZ4LIB_API void LZ4_resetStreamHC_fast(LZ4_streamHC_t *streamHCPtr,
+				       int compressionLevel); /* v1.9.0+ */
+LZ4LIB_API int LZ4_loadDictHC(LZ4_streamHC_t *streamHCPtr,
+			      const char *dictionary, int dictSize);
+
+LZ4LIB_API int LZ4_compress_HC_continue(LZ4_streamHC_t *streamHCPtr,
+					const char *src, char *dst, int srcSize,
+					int maxDstSize);
+
+/*! LZ4_compress_HC_continue_destSize() : v1.9.0+
+ *  Similar to LZ4_compress_HC_continue(),
+ *  but will read as much data as possible from `src`
+ *  to fit into `targetDstSize` budget.
+ *  Result is provided into 2 parts :
+ * @return : the number of bytes written into 'dst' (necessarily <= targetDstSize)
+ *           or 0 if compression fails.
+ * `srcSizePtr` : on success, *srcSizePtr will be updated to indicate how much bytes were read from `src`.
+ *           Note that this function may not consume the entire input.
+ */
+LZ4LIB_API int
+LZ4_compress_HC_continue_destSize(LZ4_streamHC_t *LZ4_streamHCPtr,
+				  const char *src, char *dst, int *srcSizePtr,
+				  int targetDstSize);
+
+LZ4LIB_API int LZ4_saveDictHC(LZ4_streamHC_t *streamHCPtr, char *safeBuffer,
+			      int maxDictSize);
+
+/*! LZ4_attach_HC_dictionary() : stable since v1.10.0
+ *  This API allows for the efficient re-use of a static dictionary many times.
+ *
+ *  Rather than re-loading the dictionary buffer into a working context before
+ *  each compression, or copying a pre-loaded dictionary's LZ4_streamHC_t into a
+ *  working LZ4_streamHC_t, this function introduces a no-copy setup mechanism,
+ *  in which the working stream references the dictionary stream in-place.
+ *
+ *  Several assumptions are made about the state of the dictionary stream.
+ *  Currently, only streams which have been prepared by LZ4_loadDictHC() should
+ *  be expected to work.
+ *
+ *  Alternatively, the provided dictionary stream pointer may be NULL, in which
+ *  case any existing dictionary stream is unset.
+ *
+ *  A dictionary should only be attached to a stream without any history (i.e.,
+ *  a stream that has just been reset).
+ *
+ *  The dictionary will remain attached to the working stream only for the
+ *  current stream session. Calls to LZ4_resetStreamHC(_fast) will remove the
+ *  dictionary context association from the working stream. The dictionary
+ *  stream (and source buffer) must remain in-place / accessible / unchanged
+ *  through the lifetime of the stream session.
+ */
+LZ4LIB_API void
+LZ4_attach_HC_dictionary(LZ4_streamHC_t *working_stream,
+			 const LZ4_streamHC_t *dictionary_stream);
+
+/*^**********************************************
+ * !!!!!!   STATIC LINKING ONLY   !!!!!!
+ ***********************************************/
+
+/*-******************************************************************
+ * PRIVATE DEFINITIONS :
+ * Do not use these definitions directly.
+ * They are merely exposed to allow static allocation of `LZ4_streamHC_t`.
+ * Declare an `LZ4_streamHC_t` directly, rather than any type below.
+ * Even then, only do so in the context of static linking, as definitions may change between versions.
+ ********************************************************************/
+
+#define LZ4HC_DICTIONARY_LOGSIZE 16
+#define LZ4HC_MAXD (1 << LZ4HC_DICTIONARY_LOGSIZE)
+#define LZ4HC_MAXD_MASK (LZ4HC_MAXD - 1)
+
+#define LZ4HC_HASH_LOG 15
+#define LZ4HC_HASHTABLESIZE (1 << LZ4HC_HASH_LOG)
+#define LZ4HC_HASH_MASK (LZ4HC_HASHTABLESIZE - 1)
+
+/* Never ever use these definitions directly !
+ * Declare or allocate an LZ4_streamHC_t instead.
+**/
+typedef struct LZ4HC_CCtx_internal LZ4HC_CCtx_internal;
+struct LZ4HC_CCtx_internal {
+	LZ4_u32 hashTable[LZ4HC_HASHTABLESIZE];
+	LZ4_u16 chainTable[LZ4HC_MAXD];
+	const LZ4_byte *end; /* next block here to continue on current prefix */
+	const LZ4_byte *prefixStart; /* Indexes relative to this position */
+	const LZ4_byte *dictStart; /* alternate reference for extDict */
+	LZ4_u32 dictLimit; /* below that point, need extDict */
+	LZ4_u32 lowLimit; /* below that point, no more history */
+	LZ4_u32 nextToUpdate; /* index from which to continue dictionary update */
+	short compressionLevel;
+	LZ4_i8 favorDecSpeed; /* favor decompression speed if this flag set,
+                                otherwise, favor compression ratio */
+	LZ4_i8 dirty; /* stream has to be fully reset if this flag is set */
+	const LZ4HC_CCtx_internal *dictCtx;
+};
+
+#define LZ4_STREAMHC_MINSIZE                                                   \
+	262200 /* static size, for inter-version compatibility */
+union LZ4_streamHC_u {
+	char minStateSize[LZ4_STREAMHC_MINSIZE];
+	LZ4HC_CCtx_internal internal_donotuse;
+}; /* previously typedef'd to LZ4_streamHC_t */
+
+/* LZ4_streamHC_t :
+ * This structure allows static allocation of LZ4 HC streaming state.
+ * This can be used to allocate statically on stack, or as part of a larger structure.
+ *
+ * Such state **must** be initialized using LZ4_initStreamHC() before first use.
+ *
+ * Note that invoking LZ4_initStreamHC() is not required when
+ * the state was created using LZ4_createStreamHC() (which is recommended).
+ * Using the normal builder, a newly created state is automatically initialized.
+ *
+ * Static allocation shall only be used in combination with static linking.
+ */
+
+/* LZ4_initStreamHC() : v1.9.0+
+ * Required before first use of a statically allocated LZ4_streamHC_t.
+ * Before v1.9.0 : use LZ4_resetStreamHC() instead
+ */
+LZ4LIB_API LZ4_streamHC_t *LZ4_initStreamHC(void *buffer, size_t size);
+
+/*-************************************
+*  Deprecated Functions
+**************************************/
+/* see lz4.h LZ4_DISABLE_DEPRECATE_WARNINGS to turn off deprecation warnings */
+
+/* deprecated compression functions */
+LZ4_DEPRECATED("use LZ4_compress_HC() instead")
+LZ4LIB_API int LZ4_compressHC(const char *source, char *dest, int inputSize);
+LZ4_DEPRECATED("use LZ4_compress_HC() instead")
+LZ4LIB_API int LZ4_compressHC_limitedOutput(const char *source, char *dest,
+					    int inputSize, int maxOutputSize);
+LZ4_DEPRECATED("use LZ4_compress_HC() instead")
+LZ4LIB_API int LZ4_compressHC2(const char *source, char *dest, int inputSize,
+			       int compressionLevel);
+LZ4_DEPRECATED("use LZ4_compress_HC() instead")
+LZ4LIB_API int LZ4_compressHC2_limitedOutput(const char *source, char *dest,
+					     int inputSize, int maxOutputSize,
+					     int compressionLevel);
+LZ4_DEPRECATED("use LZ4_compress_HC_extStateHC() instead")
+LZ4LIB_API int LZ4_compressHC_withStateHC(void *state, const char *source,
+					  char *dest, int inputSize);
+LZ4_DEPRECATED("use LZ4_compress_HC_extStateHC() instead")
+LZ4LIB_API
+int LZ4_compressHC_limitedOutput_withStateHC(void *state, const char *source,
+					     char *dest, int inputSize,
+					     int maxOutputSize);
+LZ4_DEPRECATED("use LZ4_compress_HC_extStateHC() instead")
+LZ4LIB_API int LZ4_compressHC2_withStateHC(void *state, const char *source,
+					   char *dest, int inputSize,
+					   int compressionLevel);
+LZ4_DEPRECATED("use LZ4_compress_HC_extStateHC() instead")
+LZ4LIB_API
+int LZ4_compressHC2_limitedOutput_withStateHC(void *state, const char *source,
+					      char *dest, int inputSize,
+					      int maxOutputSize,
+					      int compressionLevel);
+LZ4_DEPRECATED("use LZ4_compress_HC_continue() instead")
+LZ4LIB_API int LZ4_compressHC_continue(LZ4_streamHC_t *LZ4_streamHCPtr,
+				       const char *source, char *dest,
+				       int inputSize);
+LZ4_DEPRECATED("use LZ4_compress_HC_continue() instead")
+LZ4LIB_API int
+LZ4_compressHC_limitedOutput_continue(LZ4_streamHC_t *LZ4_streamHCPtr,
+				      const char *source, char *dest,
+				      int inputSize, int maxOutputSize);
+
+/* Obsolete streaming functions; degraded functionality; do not use!
+ *
+ * In order to perform streaming compression, these functions depended on data
+ * that is no longer tracked in the state. They have been preserved as well as
+ * possible: using them will still produce a correct output. However, use of
+ * LZ4_slideInputBufferHC() will truncate the history of the stream, rather
+ * than preserve a window-sized chunk of history.
+ */
+#if !defined(LZ4_STATIC_LINKING_ONLY_DISABLE_MEMORY_ALLOCATION)
+LZ4_DEPRECATED("use LZ4_createStreamHC() instead")
+LZ4LIB_API void *LZ4_createHC(const char *inputBuffer);
+LZ4_DEPRECATED("use LZ4_freeStreamHC() instead")
+LZ4LIB_API int LZ4_freeHC(void *LZ4HC_Data);
+#endif
+LZ4_DEPRECATED("use LZ4_saveDictHC() instead")
+LZ4LIB_API char *LZ4_slideInputBufferHC(void *LZ4HC_Data);
+LZ4_DEPRECATED("use LZ4_compress_HC_continue() instead")
+LZ4LIB_API int LZ4_compressHC2_continue(void *LZ4HC_Data, const char *source,
+					char *dest, int inputSize,
+					int compressionLevel);
+LZ4_DEPRECATED("use LZ4_compress_HC_continue() instead")
+LZ4LIB_API int LZ4_compressHC2_limitedOutput_continue(void *LZ4HC_Data,
+						      const char *source,
+						      char *dest, int inputSize,
+						      int maxOutputSize,
+						      int compressionLevel);
+LZ4_DEPRECATED("use LZ4_createStreamHC() instead")
+LZ4LIB_API int LZ4_sizeofStreamStateHC(void);
+LZ4_DEPRECATED("use LZ4_initStreamHC() instead")
+LZ4LIB_API int LZ4_resetStreamStateHC(void *state, char *inputBuffer);
+
+/* LZ4_resetStreamHC() is now replaced by LZ4_initStreamHC().
+ * The intention is to emphasize the difference with LZ4_resetStreamHC_fast(),
+ * which is now the recommended function to start a new stream of blocks,
+ * but cannot be used to initialize a memory segment containing arbitrary garbage data.
+ *
+ * It is recommended to switch to LZ4_initStreamHC().
+ * LZ4_resetStreamHC() will generate deprecation warnings in a future version.
+ */
+LZ4LIB_API void LZ4_resetStreamHC(LZ4_streamHC_t *streamHCPtr,
+				  int compressionLevel);
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* LZ4_HC_H_19834876238432 */
+
+/*-**************************************************
+ * !!!!!     STATIC LINKING ONLY     !!!!!
+ * Following definitions are considered experimental.
+ * They should not be linked from DLL,
+ * as there is no guarantee of API stability yet.
+ * Prototypes will be promoted to "stable" status
+ * after successful usage in real-life scenarios.
+ ***************************************************/
+#ifdef LZ4_HC_STATIC_LINKING_ONLY /* protection macro */
+#ifndef LZ4_HC_SLO_098092834
+#define LZ4_HC_SLO_098092834
+
+#define LZ4_STATIC_LINKING_ONLY /* LZ4LIB_STATIC_API */
+#include "lz4.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+/*! LZ4_setCompressionLevel() : v1.8.0+ (experimental)
+ *  It's possible to change compression level
+ *  between successive invocations of LZ4_compress_HC_continue*()
+ *  for dynamic adaptation.
+ */
+LZ4LIB_STATIC_API void LZ4_setCompressionLevel(LZ4_streamHC_t *LZ4_streamHCPtr,
+					       int compressionLevel);
+
+/*! LZ4_favorDecompressionSpeed() : v1.8.2+ (experimental)
+ *  Opt. Parser will favor decompression speed over compression ratio.
+ *  Only applicable to levels >= LZ4HC_CLEVEL_OPT_MIN.
+ */
+LZ4LIB_STATIC_API void
+LZ4_favorDecompressionSpeed(LZ4_streamHC_t *LZ4_streamHCPtr, int favor);
+
+/*! LZ4_resetStreamHC_fast() : v1.9.0+
+ *  When an LZ4_streamHC_t is known to be in a internally coherent state,
+ *  it can often be prepared for a new compression with almost no work, only
+ *  sometimes falling back to the full, expensive reset that is always required
+ *  when the stream is in an indeterminate state (i.e., the reset performed by
+ *  LZ4_resetStreamHC()).
+ *
+ *  LZ4_streamHCs are guaranteed to be in a valid state when:
+ *  - returned from LZ4_createStreamHC()
+ *  - reset by LZ4_resetStreamHC()
+ *  - memset(stream, 0, sizeof(LZ4_streamHC_t))
+ *  - the stream was in a valid state and was reset by LZ4_resetStreamHC_fast()
+ *  - the stream was in a valid state and was then used in any compression call
+ *    that returned success
+ *  - the stream was in an indeterminate state and was used in a compression
+ *    call that fully reset the state (LZ4_compress_HC_extStateHC()) and that
+ *    returned success
+ *
+ *  Note:
+ *  A stream that was last used in a compression call that returned an error
+ *  may be passed to this function. However, it will be fully reset, which will
+ *  clear any existing history and settings from the context.
+ */
+LZ4LIB_STATIC_API void LZ4_resetStreamHC_fast(LZ4_streamHC_t *LZ4_streamHCPtr,
+					      int compressionLevel);
+
+/*! LZ4_compress_HC_extStateHC_fastReset() :
+ *  A variant of LZ4_compress_HC_extStateHC().
+ *
+ *  Using this variant avoids an expensive initialization step. It is only safe
+ *  to call if the state buffer is known to be correctly initialized already
+ *  (see above comment on LZ4_resetStreamHC_fast() for a definition of
+ *  "correctly initialized"). From a high level, the difference is that this
+ *  function initializes the provided state with a call to
+ *  LZ4_resetStreamHC_fast() while LZ4_compress_HC_extStateHC() starts with a
+ *  call to LZ4_resetStreamHC().
+ */
+LZ4LIB_STATIC_API int
+LZ4_compress_HC_extStateHC_fastReset(void *state, const char *src, char *dst,
+				     int srcSize, int dstCapacity,
+				     int compressionLevel);
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* LZ4_HC_SLO_098092834 */
+#endif /* LZ4_HC_STATIC_LINKING_ONLY */
Index: crypto/lz4.c
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/crypto/lz4.c b/crypto/lz4.c
--- a/crypto/lz4.c	(revision b2497e4243461a835c25469028cd355bfc2e993f)
+++ b/crypto/lz4.c	(revision fc5a9e7d3276f214f39df6195d290478513d39d1)
@@ -81,7 +81,13 @@
 static int __lz4_decompress_crypto(const u8 *src, unsigned int slen,
 				   u8 *dst, unsigned int *dlen, void *ctx)
 {
-	int out_len = LZ4_decompress_safe(src, dst, slen, *dlen);
+	int out_len;
+
+#if defined(CONFIG_ARM64) && defined(CONFIG_KERNEL_MODE_NEON)
+	out_len = LZ4_arm64_decompress_safe(src, dst, slen, *dlen, false);
+#else
+	out_len = LZ4_decompress_safe(src, dst, slen, *dlen);
+#endif
 
 	if (out_len < 0)
 		return -EINVAL;
Index: crypto/lz4hc.c
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/crypto/lz4hc.c b/crypto/lz4hc.c
--- a/crypto/lz4hc.c	(revision b2497e4243461a835c25469028cd355bfc2e993f)
+++ b/crypto/lz4hc.c	(revision fc5a9e7d3276f214f39df6195d290478513d39d1)
@@ -82,7 +82,13 @@
 static int __lz4hc_decompress_crypto(const u8 *src, unsigned int slen,
 				     u8 *dst, unsigned int *dlen, void *ctx)
 {
-	int out_len = LZ4_decompress_safe(src, dst, slen, *dlen);
+	int out_len;
+
+#if defined(CONFIG_ARM64) && defined(CONFIG_KERNEL_MODE_NEON)
+	out_len = LZ4_arm64_decompress_safe(src, dst, slen, *dlen, false);
+#else
+	out_len = LZ4_decompress_safe(src, dst, slen, *dlen);
+#endif
 
 	if (out_len < 0)
 		return -EINVAL;
Index: fs/incfs/data_mgmt.c
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fs/incfs/data_mgmt.c b/fs/incfs/data_mgmt.c
--- a/fs/incfs/data_mgmt.c	(revision fc5a9e7d3276f214f39df6195d290478513d39d1)
+++ b/fs/incfs/data_mgmt.c	(revision 40e8e52660dbefefdfa8e57cebab5dbc03796bf5)
@@ -472,8 +472,11 @@
 
 	switch (alg) {
 	case INCFS_BLOCK_COMPRESSED_LZ4:
-		result = LZ4_decompress_safe(src.data, dst.data, src.len,
-					     dst.len);
+#if defined(CONFIG_ARM64) && defined(CONFIG_KERNEL_MODE_NEON)
+	result = LZ4_arm64_decompress_safe(src.data, dst.data, src.len, dst.len, false);
+#else
+	result = LZ4_decompress_safe(src.data, dst.data, src.len, dst.len);
+#endif
 		if (result < 0)
 			return -EBADMSG;
 		return result;
Index: lib/lz4/lz4_compress.c
===================================================================
diff --git a/lib/lz4/lz4_compress.c b/lib/lz4/lz4_compress.c
deleted file mode 100644
--- a/lib/lz4/lz4_compress.c	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ /dev/null	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
@@ -1,940 +0,0 @@
-/*
- * LZ4 - Fast LZ compression algorithm
- * Copyright (C) 2011 - 2016, Yann Collet.
- * BSD 2 - Clause License (http://www.opensource.org/licenses/bsd - license.php)
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *	* Redistributions of source code must retain the above copyright
- *	  notice, this list of conditions and the following disclaimer.
- *	* Redistributions in binary form must reproduce the above
- * copyright notice, this list of conditions and the following disclaimer
- * in the documentation and/or other materials provided with the
- * distribution.
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- * You can contact the author at :
- *	- LZ4 homepage : http://www.lz4.org
- *	- LZ4 source repository : https://github.com/lz4/lz4
- *
- *	Changed for kernel usage by:
- *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>
- */
-
-/*-************************************
- *	Dependencies
- **************************************/
-#include <linux/lz4.h>
-#include "lz4defs.h"
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <asm/unaligned.h>
-
-static const int LZ4_minLength = (MFLIMIT + 1);
-static const int LZ4_64Klimit = ((64 * KB) + (MFLIMIT - 1));
-
-/*-******************************
- *	Compression functions
- ********************************/
-static FORCE_INLINE U32 LZ4_hash4(
-	U32 sequence,
-	tableType_t const tableType)
-{
-	if (tableType == byU16)
-		return ((sequence * 2654435761U)
-			>> ((MINMATCH * 8) - (LZ4_HASHLOG + 1)));
-	else
-		return ((sequence * 2654435761U)
-			>> ((MINMATCH * 8) - LZ4_HASHLOG));
-}
-
-static FORCE_INLINE U32 LZ4_hash5(
-	U64 sequence,
-	tableType_t const tableType)
-{
-	const U32 hashLog = (tableType == byU16)
-		? LZ4_HASHLOG + 1
-		: LZ4_HASHLOG;
-
-#if LZ4_LITTLE_ENDIAN
-	static const U64 prime5bytes = 889523592379ULL;
-
-	return (U32)(((sequence << 24) * prime5bytes) >> (64 - hashLog));
-#else
-	static const U64 prime8bytes = 11400714785074694791ULL;
-
-	return (U32)(((sequence >> 24) * prime8bytes) >> (64 - hashLog));
-#endif
-}
-
-static FORCE_INLINE U32 LZ4_hashPosition(
-	const void *p,
-	tableType_t const tableType)
-{
-#if LZ4_ARCH64
-	if (tableType == byU32)
-		return LZ4_hash5(LZ4_read_ARCH(p), tableType);
-#endif
-
-	return LZ4_hash4(LZ4_read32(p), tableType);
-}
-
-static void LZ4_putPositionOnHash(
-	const BYTE *p,
-	U32 h,
-	void *tableBase,
-	tableType_t const tableType,
-	const BYTE *srcBase)
-{
-	switch (tableType) {
-	case byPtr:
-	{
-		const BYTE **hashTable = (const BYTE **)tableBase;
-
-		hashTable[h] = p;
-		return;
-	}
-	case byU32:
-	{
-		U32 *hashTable = (U32 *) tableBase;
-
-		hashTable[h] = (U32)(p - srcBase);
-		return;
-	}
-	case byU16:
-	{
-		U16 *hashTable = (U16 *) tableBase;
-
-		hashTable[h] = (U16)(p - srcBase);
-		return;
-	}
-	}
-}
-
-static FORCE_INLINE void LZ4_putPosition(
-	const BYTE *p,
-	void *tableBase,
-	tableType_t tableType,
-	const BYTE *srcBase)
-{
-	U32 const h = LZ4_hashPosition(p, tableType);
-
-	LZ4_putPositionOnHash(p, h, tableBase, tableType, srcBase);
-}
-
-static const BYTE *LZ4_getPositionOnHash(
-	U32 h,
-	void *tableBase,
-	tableType_t tableType,
-	const BYTE *srcBase)
-{
-	if (tableType == byPtr) {
-		const BYTE **hashTable = (const BYTE **) tableBase;
-
-		return hashTable[h];
-	}
-
-	if (tableType == byU32) {
-		const U32 * const hashTable = (U32 *) tableBase;
-
-		return hashTable[h] + srcBase;
-	}
-
-	{
-		/* default, to ensure a return */
-		const U16 * const hashTable = (U16 *) tableBase;
-
-		return hashTable[h] + srcBase;
-	}
-}
-
-static FORCE_INLINE const BYTE *LZ4_getPosition(
-	const BYTE *p,
-	void *tableBase,
-	tableType_t tableType,
-	const BYTE *srcBase)
-{
-	U32 const h = LZ4_hashPosition(p, tableType);
-
-	return LZ4_getPositionOnHash(h, tableBase, tableType, srcBase);
-}
-
-
-/*
- * LZ4_compress_generic() :
- * inlined, to ensure branches are decided at compilation time
- */
-static FORCE_INLINE int LZ4_compress_generic(
-	LZ4_stream_t_internal * const dictPtr,
-	const char * const source,
-	char * const dest,
-	const int inputSize,
-	const int maxOutputSize,
-	const limitedOutput_directive outputLimited,
-	const tableType_t tableType,
-	const dict_directive dict,
-	const dictIssue_directive dictIssue,
-	const U32 acceleration)
-{
-	const BYTE *ip = (const BYTE *) source;
-	const BYTE *base;
-	const BYTE *lowLimit;
-	const BYTE * const lowRefLimit = ip - dictPtr->dictSize;
-	const BYTE * const dictionary = dictPtr->dictionary;
-	const BYTE * const dictEnd = dictionary + dictPtr->dictSize;
-	const size_t dictDelta = dictEnd - (const BYTE *)source;
-	const BYTE *anchor = (const BYTE *) source;
-	const BYTE * const iend = ip + inputSize;
-	const BYTE * const mflimit = iend - MFLIMIT;
-	const BYTE * const matchlimit = iend - LASTLITERALS;
-
-	BYTE *op = (BYTE *) dest;
-	BYTE * const olimit = op + maxOutputSize;
-
-	U32 forwardH;
-	size_t refDelta = 0;
-
-	/* Init conditions */
-	if ((U32)inputSize > (U32)LZ4_MAX_INPUT_SIZE) {
-		/* Unsupported inputSize, too large (or negative) */
-		return 0;
-	}
-
-	switch (dict) {
-	case noDict:
-	default:
-		base = (const BYTE *)source;
-		lowLimit = (const BYTE *)source;
-		break;
-	case withPrefix64k:
-		base = (const BYTE *)source - dictPtr->currentOffset;
-		lowLimit = (const BYTE *)source - dictPtr->dictSize;
-		break;
-	case usingExtDict:
-		base = (const BYTE *)source - dictPtr->currentOffset;
-		lowLimit = (const BYTE *)source;
-		break;
-	}
-
-	if ((tableType == byU16)
-		&& (inputSize >= LZ4_64Klimit)) {
-		/* Size too large (not within 64K limit) */
-		return 0;
-	}
-
-	if (inputSize < LZ4_minLength) {
-		/* Input too small, no compression (all literals) */
-		goto _last_literals;
-	}
-
-	/* First Byte */
-	LZ4_putPosition(ip, dictPtr->hashTable, tableType, base);
-	ip++;
-	forwardH = LZ4_hashPosition(ip, tableType);
-
-	/* Main Loop */
-	for ( ; ; ) {
-		const BYTE *match;
-		BYTE *token;
-
-		/* Find a match */
-		{
-			const BYTE *forwardIp = ip;
-			unsigned int step = 1;
-			unsigned int searchMatchNb = acceleration << LZ4_SKIPTRIGGER;
-
-			do {
-				U32 const h = forwardH;
-
-				ip = forwardIp;
-				forwardIp += step;
-				step = (searchMatchNb++ >> LZ4_SKIPTRIGGER);
-
-				if (unlikely(forwardIp > mflimit))
-					goto _last_literals;
-
-				match = LZ4_getPositionOnHash(h,
-					dictPtr->hashTable,
-					tableType, base);
-
-				if (dict == usingExtDict) {
-					if (match < (const BYTE *)source) {
-						refDelta = dictDelta;
-						lowLimit = dictionary;
-					} else {
-						refDelta = 0;
-						lowLimit = (const BYTE *)source;
-				}	 }
-
-				forwardH = LZ4_hashPosition(forwardIp,
-					tableType);
-
-				LZ4_putPositionOnHash(ip, h, dictPtr->hashTable,
-					tableType, base);
-			} while (((dictIssue == dictSmall)
-					? (match < lowRefLimit)
-					: 0)
-				|| ((tableType == byU16)
-					? 0
-					: (match + MAX_DISTANCE < ip))
-				|| (LZ4_read32(match + refDelta)
-					!= LZ4_read32(ip)));
-		}
-
-		/* Catch up */
-		while (((ip > anchor) & (match + refDelta > lowLimit))
-				&& (unlikely(ip[-1] == match[refDelta - 1]))) {
-			ip--;
-			match--;
-		}
-
-		/* Encode Literals */
-		{
-			unsigned const int litLength = (unsigned int)(ip - anchor);
-
-			token = op++;
-
-			if ((outputLimited) &&
-				/* Check output buffer overflow */
-				(unlikely(op + litLength +
-					(2 + 1 + LASTLITERALS) +
-					(litLength / 255) > olimit)))
-				return 0;
-
-			if (litLength >= RUN_MASK) {
-				int len = (int)litLength - RUN_MASK;
-
-				*token = (RUN_MASK << ML_BITS);
-
-				for (; len >= 255; len -= 255)
-					*op++ = 255;
-				*op++ = (BYTE)len;
-			} else
-				*token = (BYTE)(litLength << ML_BITS);
-
-			/* Copy Literals */
-			LZ4_wildCopy(op, anchor, op + litLength);
-			op += litLength;
-		}
-
-_next_match:
-		/* Encode Offset */
-		LZ4_writeLE16(op, (U16)(ip - match));
-		op += 2;
-
-		/* Encode MatchLength */
-		{
-			unsigned int matchCode;
-
-			if ((dict == usingExtDict)
-				&& (lowLimit == dictionary)) {
-				const BYTE *limit;
-
-				match += refDelta;
-				limit = ip + (dictEnd - match);
-
-				if (limit > matchlimit)
-					limit = matchlimit;
-
-				matchCode = LZ4_count(ip + MINMATCH,
-					match + MINMATCH, limit);
-
-				ip += MINMATCH + matchCode;
-
-				if (ip == limit) {
-					unsigned const int more = LZ4_count(ip,
-						(const BYTE *)source,
-						matchlimit);
-
-					matchCode += more;
-					ip += more;
-				}
-			} else {
-				matchCode = LZ4_count(ip + MINMATCH,
-					match + MINMATCH, matchlimit);
-				ip += MINMATCH + matchCode;
-			}
-
-			if (outputLimited &&
-				/* Check output buffer overflow */
-				(unlikely(op +
-					(1 + LASTLITERALS) +
-					(matchCode >> 8) > olimit)))
-				return 0;
-
-			if (matchCode >= ML_MASK) {
-				*token += ML_MASK;
-				matchCode -= ML_MASK;
-				LZ4_write32(op, 0xFFFFFFFF);
-
-				while (matchCode >= 4 * 255) {
-					op += 4;
-					LZ4_write32(op, 0xFFFFFFFF);
-					matchCode -= 4 * 255;
-				}
-
-				op += matchCode / 255;
-				*op++ = (BYTE)(matchCode % 255);
-			} else
-				*token += (BYTE)(matchCode);
-		}
-
-		anchor = ip;
-
-		/* Test end of chunk */
-		if (ip > mflimit)
-			break;
-
-		/* Fill table */
-		LZ4_putPosition(ip - 2, dictPtr->hashTable, tableType, base);
-
-		/* Test next position */
-		match = LZ4_getPosition(ip, dictPtr->hashTable,
-			tableType, base);
-
-		if (dict == usingExtDict) {
-			if (match < (const BYTE *)source) {
-				refDelta = dictDelta;
-				lowLimit = dictionary;
-			} else {
-				refDelta = 0;
-				lowLimit = (const BYTE *)source;
-			}
-		}
-
-		LZ4_putPosition(ip, dictPtr->hashTable, tableType, base);
-
-		if (((dictIssue == dictSmall) ? (match >= lowRefLimit) : 1)
-			&& (match + MAX_DISTANCE >= ip)
-			&& (LZ4_read32(match + refDelta) == LZ4_read32(ip))) {
-			token = op++;
-			*token = 0;
-			goto _next_match;
-		}
-
-		/* Prepare next loop */
-		forwardH = LZ4_hashPosition(++ip, tableType);
-	}
-
-_last_literals:
-	/* Encode Last Literals */
-	{
-		size_t const lastRun = (size_t)(iend - anchor);
-
-		if ((outputLimited) &&
-			/* Check output buffer overflow */
-			((op - (BYTE *)dest) + lastRun + 1 +
-			((lastRun + 255 - RUN_MASK) / 255) > (U32)maxOutputSize))
-			return 0;
-
-		if (lastRun >= RUN_MASK) {
-			size_t accumulator = lastRun - RUN_MASK;
-			*op++ = RUN_MASK << ML_BITS;
-			for (; accumulator >= 255; accumulator -= 255)
-				*op++ = 255;
-			*op++ = (BYTE) accumulator;
-		} else {
-			*op++ = (BYTE)(lastRun << ML_BITS);
-		}
-
-		LZ4_memcpy(op, anchor, lastRun);
-
-		op += lastRun;
-	}
-
-	/* End */
-	return (int) (((char *)op) - dest);
-}
-
-static int LZ4_compress_fast_extState(
-	void *state,
-	const char *source,
-	char *dest,
-	int inputSize,
-	int maxOutputSize,
-	int acceleration)
-{
-	LZ4_stream_t_internal *ctx = &((LZ4_stream_t *)state)->internal_donotuse;
-#if LZ4_ARCH64
-	const tableType_t tableType = byU32;
-#else
-	const tableType_t tableType = byPtr;
-#endif
-
-	LZ4_resetStream((LZ4_stream_t *)state);
-
-	if (acceleration < 1)
-		acceleration = LZ4_ACCELERATION_DEFAULT;
-
-	if (maxOutputSize >= LZ4_COMPRESSBOUND(inputSize)) {
-		if (inputSize < LZ4_64Klimit)
-			return LZ4_compress_generic(ctx, source,
-				dest, inputSize, 0,
-				noLimit, byU16, noDict,
-				noDictIssue, acceleration);
-		else
-			return LZ4_compress_generic(ctx, source,
-				dest, inputSize, 0,
-				noLimit, tableType, noDict,
-				noDictIssue, acceleration);
-	} else {
-		if (inputSize < LZ4_64Klimit)
-			return LZ4_compress_generic(ctx, source,
-				dest, inputSize,
-				maxOutputSize, limitedOutput, byU16, noDict,
-				noDictIssue, acceleration);
-		else
-			return LZ4_compress_generic(ctx, source,
-				dest, inputSize,
-				maxOutputSize, limitedOutput, tableType, noDict,
-				noDictIssue, acceleration);
-	}
-}
-
-int LZ4_compress_fast(const char *source, char *dest, int inputSize,
-	int maxOutputSize, int acceleration, void *wrkmem)
-{
-	return LZ4_compress_fast_extState(wrkmem, source, dest, inputSize,
-		maxOutputSize, acceleration);
-}
-EXPORT_SYMBOL(LZ4_compress_fast);
-
-int LZ4_compress_default(const char *source, char *dest, int inputSize,
-	int maxOutputSize, void *wrkmem)
-{
-	return LZ4_compress_fast(source, dest, inputSize,
-		maxOutputSize, LZ4_ACCELERATION_DEFAULT, wrkmem);
-}
-EXPORT_SYMBOL(LZ4_compress_default);
-
-/*-******************************
- *	*_destSize() variant
- ********************************/
-static int LZ4_compress_destSize_generic(
-	LZ4_stream_t_internal * const ctx,
-	const char * const src,
-	char * const dst,
-	int * const srcSizePtr,
-	const int targetDstSize,
-	const tableType_t tableType)
-{
-	const BYTE *ip = (const BYTE *) src;
-	const BYTE *base = (const BYTE *) src;
-	const BYTE *lowLimit = (const BYTE *) src;
-	const BYTE *anchor = ip;
-	const BYTE * const iend = ip + *srcSizePtr;
-	const BYTE * const mflimit = iend - MFLIMIT;
-	const BYTE * const matchlimit = iend - LASTLITERALS;
-
-	BYTE *op = (BYTE *) dst;
-	BYTE * const oend = op + targetDstSize;
-	BYTE * const oMaxLit = op + targetDstSize - 2 /* offset */
-		- 8 /* because 8 + MINMATCH == MFLIMIT */ - 1 /* token */;
-	BYTE * const oMaxMatch = op + targetDstSize
-		- (LASTLITERALS + 1 /* token */);
-	BYTE * const oMaxSeq = oMaxLit - 1 /* token */;
-
-	U32 forwardH;
-
-	/* Init conditions */
-	/* Impossible to store anything */
-	if (targetDstSize < 1)
-		return 0;
-	/* Unsupported input size, too large (or negative) */
-	if ((U32)*srcSizePtr > (U32)LZ4_MAX_INPUT_SIZE)
-		return 0;
-	/* Size too large (not within 64K limit) */
-	if ((tableType == byU16) && (*srcSizePtr >= LZ4_64Klimit))
-		return 0;
-	/* Input too small, no compression (all literals) */
-	if (*srcSizePtr < LZ4_minLength)
-		goto _last_literals;
-
-	/* First Byte */
-	*srcSizePtr = 0;
-	LZ4_putPosition(ip, ctx->hashTable, tableType, base);
-	ip++; forwardH = LZ4_hashPosition(ip, tableType);
-
-	/* Main Loop */
-	for ( ; ; ) {
-		const BYTE *match;
-		BYTE *token;
-
-		/* Find a match */
-		{
-			const BYTE *forwardIp = ip;
-			unsigned int step = 1;
-			unsigned int searchMatchNb = 1 << LZ4_SKIPTRIGGER;
-
-			do {
-				U32 h = forwardH;
-
-				ip = forwardIp;
-				forwardIp += step;
-				step = (searchMatchNb++ >> LZ4_SKIPTRIGGER);
-
-				if (unlikely(forwardIp > mflimit))
-					goto _last_literals;
-
-				match = LZ4_getPositionOnHash(h, ctx->hashTable,
-					tableType, base);
-				forwardH = LZ4_hashPosition(forwardIp,
-					tableType);
-				LZ4_putPositionOnHash(ip, h,
-					ctx->hashTable, tableType,
-					base);
-
-			} while (((tableType == byU16)
-				? 0
-				: (match + MAX_DISTANCE < ip))
-				|| (LZ4_read32(match) != LZ4_read32(ip)));
-		}
-
-		/* Catch up */
-		while ((ip > anchor)
-			&& (match > lowLimit)
-			&& (unlikely(ip[-1] == match[-1]))) {
-			ip--;
-			match--;
-		}
-
-		/* Encode Literal length */
-		{
-			unsigned int litLength = (unsigned int)(ip - anchor);
-
-			token = op++;
-			if (op + ((litLength + 240) / 255)
-				+ litLength > oMaxLit) {
-				/* Not enough space for a last match */
-				op--;
-				goto _last_literals;
-			}
-			if (litLength >= RUN_MASK) {
-				unsigned int len = litLength - RUN_MASK;
-				*token = (RUN_MASK<<ML_BITS);
-				for (; len >= 255; len -= 255)
-					*op++ = 255;
-				*op++ = (BYTE)len;
-			} else
-				*token = (BYTE)(litLength << ML_BITS);
-
-			/* Copy Literals */
-			LZ4_wildCopy(op, anchor, op + litLength);
-			op += litLength;
-		}
-
-_next_match:
-		/* Encode Offset */
-		LZ4_writeLE16(op, (U16)(ip - match)); op += 2;
-
-		/* Encode MatchLength */
-		{
-			size_t matchLength = LZ4_count(ip + MINMATCH,
-			match + MINMATCH, matchlimit);
-
-			if (op + ((matchLength + 240)/255) > oMaxMatch) {
-				/* Match description too long : reduce it */
-				matchLength = (15 - 1) + (oMaxMatch - op) * 255;
-			}
-			ip += MINMATCH + matchLength;
-
-			if (matchLength >= ML_MASK) {
-				*token += ML_MASK;
-				matchLength -= ML_MASK;
-				while (matchLength >= 255) {
-					matchLength -= 255;
-					*op++ = 255;
-				}
-				*op++ = (BYTE)matchLength;
-			} else
-				*token += (BYTE)(matchLength);
-		}
-
-		anchor = ip;
-
-		/* Test end of block */
-		if (ip > mflimit)
-			break;
-		if (op > oMaxSeq)
-			break;
-
-		/* Fill table */
-		LZ4_putPosition(ip - 2, ctx->hashTable, tableType, base);
-
-		/* Test next position */
-		match = LZ4_getPosition(ip, ctx->hashTable, tableType, base);
-		LZ4_putPosition(ip, ctx->hashTable, tableType, base);
-
-		if ((match + MAX_DISTANCE >= ip)
-			&& (LZ4_read32(match) == LZ4_read32(ip))) {
-			token = op++; *token = 0;
-			goto _next_match;
-		}
-
-		/* Prepare next loop */
-		forwardH = LZ4_hashPosition(++ip, tableType);
-	}
-
-_last_literals:
-	/* Encode Last Literals */
-	{
-		size_t lastRunSize = (size_t)(iend - anchor);
-
-		if (op + 1 /* token */
-			+ ((lastRunSize + 240) / 255) /* litLength */
-			+ lastRunSize /* literals */ > oend) {
-			/* adapt lastRunSize to fill 'dst' */
-			lastRunSize	= (oend - op) - 1;
-			lastRunSize -= (lastRunSize + 240) / 255;
-		}
-		ip = anchor + lastRunSize;
-
-		if (lastRunSize >= RUN_MASK) {
-			size_t accumulator = lastRunSize - RUN_MASK;
-
-			*op++ = RUN_MASK << ML_BITS;
-			for (; accumulator >= 255; accumulator -= 255)
-				*op++ = 255;
-			*op++ = (BYTE) accumulator;
-		} else {
-			*op++ = (BYTE)(lastRunSize<<ML_BITS);
-		}
-		LZ4_memcpy(op, anchor, lastRunSize);
-		op += lastRunSize;
-	}
-
-	/* End */
-	*srcSizePtr = (int) (((const char *)ip) - src);
-	return (int) (((char *)op) - dst);
-}
-
-static int LZ4_compress_destSize_extState(
-	LZ4_stream_t *state,
-	const char *src,
-	char *dst,
-	int *srcSizePtr,
-	int targetDstSize)
-{
-#if LZ4_ARCH64
-	const tableType_t tableType = byU32;
-#else
-	const tableType_t tableType = byPtr;
-#endif
-
-	LZ4_resetStream(state);
-
-	if (targetDstSize >= LZ4_COMPRESSBOUND(*srcSizePtr)) {
-		/* compression success is guaranteed */
-		return LZ4_compress_fast_extState(
-			state, src, dst, *srcSizePtr,
-			targetDstSize, 1);
-	} else {
-		if (*srcSizePtr < LZ4_64Klimit)
-			return LZ4_compress_destSize_generic(
-				&state->internal_donotuse,
-				src, dst, srcSizePtr,
-				targetDstSize, byU16);
-		else
-			return LZ4_compress_destSize_generic(
-				&state->internal_donotuse,
-				src, dst, srcSizePtr,
-				targetDstSize, tableType);
-	}
-}
-
-
-int LZ4_compress_destSize(
-	const char *src,
-	char *dst,
-	int *srcSizePtr,
-	int targetDstSize,
-	void *wrkmem)
-{
-	return LZ4_compress_destSize_extState(wrkmem, src, dst, srcSizePtr,
-		targetDstSize);
-}
-EXPORT_SYMBOL(LZ4_compress_destSize);
-
-/*-******************************
- *	Streaming functions
- ********************************/
-void LZ4_resetStream(LZ4_stream_t *LZ4_stream)
-{
-	memset(LZ4_stream, 0, sizeof(LZ4_stream_t));
-}
-
-int LZ4_loadDict(LZ4_stream_t *LZ4_dict,
-	const char *dictionary, int dictSize)
-{
-	LZ4_stream_t_internal *dict = &LZ4_dict->internal_donotuse;
-	const BYTE *p = (const BYTE *)dictionary;
-	const BYTE * const dictEnd = p + dictSize;
-	const BYTE *base;
-
-	if ((dict->initCheck)
-		|| (dict->currentOffset > 1 * GB)) {
-		/* Uninitialized structure, or reuse overflow */
-		LZ4_resetStream(LZ4_dict);
-	}
-
-	if (dictSize < (int)HASH_UNIT) {
-		dict->dictionary = NULL;
-		dict->dictSize = 0;
-		return 0;
-	}
-
-	if ((dictEnd - p) > 64 * KB)
-		p = dictEnd - 64 * KB;
-	dict->currentOffset += 64 * KB;
-	base = p - dict->currentOffset;
-	dict->dictionary = p;
-	dict->dictSize = (U32)(dictEnd - p);
-	dict->currentOffset += dict->dictSize;
-
-	while (p <= dictEnd - HASH_UNIT) {
-		LZ4_putPosition(p, dict->hashTable, byU32, base);
-		p += 3;
-	}
-
-	return dict->dictSize;
-}
-EXPORT_SYMBOL(LZ4_loadDict);
-
-static void LZ4_renormDictT(LZ4_stream_t_internal *LZ4_dict,
-	const BYTE *src)
-{
-	if ((LZ4_dict->currentOffset > 0x80000000) ||
-		((uptrval)LZ4_dict->currentOffset > (uptrval)src)) {
-		/* address space overflow */
-		/* rescale hash table */
-		U32 const delta = LZ4_dict->currentOffset - 64 * KB;
-		const BYTE *dictEnd = LZ4_dict->dictionary + LZ4_dict->dictSize;
-		int i;
-
-		for (i = 0; i < LZ4_HASH_SIZE_U32; i++) {
-			if (LZ4_dict->hashTable[i] < delta)
-				LZ4_dict->hashTable[i] = 0;
-			else
-				LZ4_dict->hashTable[i] -= delta;
-		}
-		LZ4_dict->currentOffset = 64 * KB;
-		if (LZ4_dict->dictSize > 64 * KB)
-			LZ4_dict->dictSize = 64 * KB;
-		LZ4_dict->dictionary = dictEnd - LZ4_dict->dictSize;
-	}
-}
-
-int LZ4_saveDict(LZ4_stream_t *LZ4_dict, char *safeBuffer, int dictSize)
-{
-	LZ4_stream_t_internal * const dict = &LZ4_dict->internal_donotuse;
-	const BYTE * const previousDictEnd = dict->dictionary + dict->dictSize;
-
-	if ((U32)dictSize > 64 * KB) {
-		/* useless to define a dictionary > 64 * KB */
-		dictSize = 64 * KB;
-	}
-	if ((U32)dictSize > dict->dictSize)
-		dictSize = dict->dictSize;
-
-	memmove(safeBuffer, previousDictEnd - dictSize, dictSize);
-
-	dict->dictionary = (const BYTE *)safeBuffer;
-	dict->dictSize = (U32)dictSize;
-
-	return dictSize;
-}
-EXPORT_SYMBOL(LZ4_saveDict);
-
-int LZ4_compress_fast_continue(LZ4_stream_t *LZ4_stream, const char *source,
-	char *dest, int inputSize, int maxOutputSize, int acceleration)
-{
-	LZ4_stream_t_internal *streamPtr = &LZ4_stream->internal_donotuse;
-	const BYTE * const dictEnd = streamPtr->dictionary
-		+ streamPtr->dictSize;
-
-	const BYTE *smallest = (const BYTE *) source;
-
-	if (streamPtr->initCheck) {
-		/* Uninitialized structure detected */
-		return 0;
-	}
-
-	if ((streamPtr->dictSize > 0) && (smallest > dictEnd))
-		smallest = dictEnd;
-
-	LZ4_renormDictT(streamPtr, smallest);
-
-	if (acceleration < 1)
-		acceleration = LZ4_ACCELERATION_DEFAULT;
-
-	/* Check overlapping input/dictionary space */
-	{
-		const BYTE *sourceEnd = (const BYTE *) source + inputSize;
-
-		if ((sourceEnd > streamPtr->dictionary)
-			&& (sourceEnd < dictEnd)) {
-			streamPtr->dictSize = (U32)(dictEnd - sourceEnd);
-			if (streamPtr->dictSize > 64 * KB)
-				streamPtr->dictSize = 64 * KB;
-			if (streamPtr->dictSize < 4)
-				streamPtr->dictSize = 0;
-			streamPtr->dictionary = dictEnd - streamPtr->dictSize;
-		}
-	}
-
-	/* prefix mode : source data follows dictionary */
-	if (dictEnd == (const BYTE *)source) {
-		int result;
-
-		if ((streamPtr->dictSize < 64 * KB) &&
-			(streamPtr->dictSize < streamPtr->currentOffset)) {
-			result = LZ4_compress_generic(
-				streamPtr, source, dest, inputSize,
-				maxOutputSize, limitedOutput, byU32,
-				withPrefix64k, dictSmall, acceleration);
-		} else {
-			result = LZ4_compress_generic(
-				streamPtr, source, dest, inputSize,
-				maxOutputSize, limitedOutput, byU32,
-				withPrefix64k, noDictIssue, acceleration);
-		}
-		streamPtr->dictSize += (U32)inputSize;
-		streamPtr->currentOffset += (U32)inputSize;
-		return result;
-	}
-
-	/* external dictionary mode */
-	{
-		int result;
-
-		if ((streamPtr->dictSize < 64 * KB) &&
-			(streamPtr->dictSize < streamPtr->currentOffset)) {
-			result = LZ4_compress_generic(
-				streamPtr, source, dest, inputSize,
-				maxOutputSize, limitedOutput, byU32,
-				usingExtDict, dictSmall, acceleration);
-		} else {
-			result = LZ4_compress_generic(
-				streamPtr, source, dest, inputSize,
-				maxOutputSize, limitedOutput, byU32,
-				usingExtDict, noDictIssue, acceleration);
-		}
-		streamPtr->dictionary = (const BYTE *)source;
-		streamPtr->dictSize = (U32)inputSize;
-		streamPtr->currentOffset += (U32)inputSize;
-		return result;
-	}
-}
-EXPORT_SYMBOL(LZ4_compress_fast_continue);
-
-MODULE_LICENSE("Dual BSD/GPL");
-MODULE_DESCRIPTION("LZ4 compressor");
Index: lib/lz4/lz4_decompress.c
===================================================================
diff --git a/lib/lz4/lz4_decompress.c b/lib/lz4/lz4_decompress.c
deleted file mode 100644
--- a/lib/lz4/lz4_decompress.c	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ /dev/null	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
@@ -1,720 +0,0 @@
-/*
- * LZ4 - Fast LZ compression algorithm
- * Copyright (C) 2011 - 2016, Yann Collet.
- * BSD 2 - Clause License (http://www.opensource.org/licenses/bsd - license.php)
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *	* Redistributions of source code must retain the above copyright
- *	  notice, this list of conditions and the following disclaimer.
- *	* Redistributions in binary form must reproduce the above
- * copyright notice, this list of conditions and the following disclaimer
- * in the documentation and/or other materials provided with the
- * distribution.
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- * You can contact the author at :
- *	- LZ4 homepage : http://www.lz4.org
- *	- LZ4 source repository : https://github.com/lz4/lz4
- *
- *	Changed for kernel usage by:
- *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>
- */
-
-/*-************************************
- *	Dependencies
- **************************************/
-#include <linux/lz4.h>
-#include "lz4defs.h"
-#include <linux/init.h>
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <asm/unaligned.h>
-
-/*-*****************************
- *	Decompression functions
- *******************************/
-
-#define DEBUGLOG(l, ...) {}	/* disabled */
-
-#ifndef assert
-#define assert(condition) ((void)0)
-#endif
-
-/*
- * LZ4_decompress_generic() :
- * This generic decompression function covers all use cases.
- * It shall be instantiated several times, using different sets of directives.
- * Note that it is important for performance that this function really get inlined,
- * in order to remove useless branches during compilation optimization.
- */
-static FORCE_INLINE int LZ4_decompress_generic(
-	 const char * const src,
-	 char * const dst,
-	 int srcSize,
-		/*
-		 * If endOnInput == endOnInputSize,
-		 * this value is `dstCapacity`
-		 */
-	 int outputSize,
-	 /* endOnOutputSize, endOnInputSize */
-	 endCondition_directive endOnInput,
-	 /* full, partial */
-	 earlyEnd_directive partialDecoding,
-	 /* noDict, withPrefix64k, usingExtDict */
-	 dict_directive dict,
-	 /* always <= dst, == dst when no prefix */
-	 const BYTE * const lowPrefix,
-	 /* only if dict == usingExtDict */
-	 const BYTE * const dictStart,
-	 /* note : = 0 if noDict */
-	 const size_t dictSize
-	 )
-{
-	const BYTE *ip = (const BYTE *) src;
-	const BYTE * const iend = ip + srcSize;
-
-	BYTE *op = (BYTE *) dst;
-	BYTE * const oend = op + outputSize;
-	BYTE *cpy;
-
-	const BYTE * const dictEnd = (const BYTE *)dictStart + dictSize;
-	static const unsigned int inc32table[8] = {0, 1, 2, 1, 0, 4, 4, 4};
-	static const int dec64table[8] = {0, 0, 0, -1, -4, 1, 2, 3};
-
-	const int safeDecode = (endOnInput == endOnInputSize);
-	const int checkOffset = ((safeDecode) && (dictSize < (int)(64 * KB)));
-
-	/* Set up the "end" pointers for the shortcut. */
-	const BYTE *const shortiend = iend -
-		(endOnInput ? 14 : 8) /*maxLL*/ - 2 /*offset*/;
-	const BYTE *const shortoend = oend -
-		(endOnInput ? 14 : 8) /*maxLL*/ - 18 /*maxML*/;
-
-	DEBUGLOG(5, "%s (srcSize:%i, dstSize:%i)", __func__,
-		 srcSize, outputSize);
-
-	/* Special cases */
-	assert(lowPrefix <= op);
-	assert(src != NULL);
-
-	/* Empty output buffer */
-	if ((endOnInput) && (unlikely(outputSize == 0)))
-		return ((srcSize == 1) && (*ip == 0)) ? 0 : -1;
-
-	if ((!endOnInput) && (unlikely(outputSize == 0)))
-		return (*ip == 0 ? 1 : -1);
-
-	if ((endOnInput) && unlikely(srcSize == 0))
-		return -1;
-
-	/* Main Loop : decode sequences */
-	while (1) {
-		size_t length;
-		const BYTE *match;
-		size_t offset;
-
-		/* get literal length */
-		unsigned int const token = *ip++;
-		length = token>>ML_BITS;
-
-		/* ip < iend before the increment */
-		assert(!endOnInput || ip <= iend);
-
-		/*
-		 * A two-stage shortcut for the most common case:
-		 * 1) If the literal length is 0..14, and there is enough
-		 * space, enter the shortcut and copy 16 bytes on behalf
-		 * of the literals (in the fast mode, only 8 bytes can be
-		 * safely copied this way).
-		 * 2) Further if the match length is 4..18, copy 18 bytes
-		 * in a similar manner; but we ensure that there's enough
-		 * space in the output for those 18 bytes earlier, upon
-		 * entering the shortcut (in other words, there is a
-		 * combined check for both stages).
-		 *
-		 * The & in the likely() below is intentionally not && so that
-		 * some compilers can produce better parallelized runtime code
-		 */
-		if ((endOnInput ? length != RUN_MASK : length <= 8)
-		   /*
-		    * strictly "less than" on input, to re-enter
-		    * the loop with at least one byte
-		    */
-		   && likely((endOnInput ? ip < shortiend : 1) &
-			     (op <= shortoend))) {
-			/* Copy the literals */
-			LZ4_memcpy(op, ip, endOnInput ? 16 : 8);
-			op += length; ip += length;
-
-			/*
-			 * The second stage:
-			 * prepare for match copying, decode full info.
-			 * If it doesn't work out, the info won't be wasted.
-			 */
-			length = token & ML_MASK; /* match length */
-			offset = LZ4_readLE16(ip);
-			ip += 2;
-			match = op - offset;
-			assert(match <= op); /* check overflow */
-
-			/* Do not deal with overlapping matches. */
-			if ((length != ML_MASK) &&
-			    (offset >= 8) &&
-			    (dict == withPrefix64k || match >= lowPrefix)) {
-				/* Copy the match. */
-				LZ4_memcpy(op + 0, match + 0, 8);
-				LZ4_memcpy(op + 8, match + 8, 8);
-				LZ4_memcpy(op + 16, match + 16, 2);
-				op += length + MINMATCH;
-				/* Both stages worked, load the next token. */
-				continue;
-			}
-
-			/*
-			 * The second stage didn't work out, but the info
-			 * is ready. Propel it right to the point of match
-			 * copying.
-			 */
-			goto _copy_match;
-		}
-
-		/* decode literal length */
-		if (length == RUN_MASK) {
-			unsigned int s;
-
-			if (unlikely(endOnInput ? ip >= iend - RUN_MASK : 0)) {
-				/* overflow detection */
-				goto _output_error;
-			}
-			do {
-				s = *ip++;
-				length += s;
-			} while (likely(endOnInput
-				? ip < iend - RUN_MASK
-				: 1) & (s == 255));
-
-			if ((safeDecode)
-			    && unlikely((uptrval)(op) +
-					length < (uptrval)(op))) {
-				/* overflow detection */
-				goto _output_error;
-			}
-			if ((safeDecode)
-			    && unlikely((uptrval)(ip) +
-					length < (uptrval)(ip))) {
-				/* overflow detection */
-				goto _output_error;
-			}
-		}
-
-		/* copy literals */
-		cpy = op + length;
-		LZ4_STATIC_ASSERT(MFLIMIT >= WILDCOPYLENGTH);
-
-		if (((endOnInput) && ((cpy > oend - MFLIMIT)
-			|| (ip + length > iend - (2 + 1 + LASTLITERALS))))
-			|| ((!endOnInput) && (cpy > oend - WILDCOPYLENGTH))) {
-			if (partialDecoding) {
-				if (cpy > oend) {
-					/*
-					 * Partial decoding :
-					 * stop in the middle of literal segment
-					 */
-					cpy = oend;
-					length = oend - op;
-				}
-				if ((endOnInput)
-					&& (ip + length > iend)) {
-					/*
-					 * Error :
-					 * read attempt beyond
-					 * end of input buffer
-					 */
-					goto _output_error;
-				}
-			} else {
-				if ((!endOnInput)
-					&& (cpy != oend)) {
-					/*
-					 * Error :
-					 * block decoding must
-					 * stop exactly there
-					 */
-					goto _output_error;
-				}
-				if ((endOnInput)
-					&& ((ip + length != iend)
-					|| (cpy > oend))) {
-					/*
-					 * Error :
-					 * input must be consumed
-					 */
-					goto _output_error;
-				}
-			}
-
-			/*
-			 * supports overlapping memory regions; only matters
-			 * for in-place decompression scenarios
-			 */
-			LZ4_memmove(op, ip, length);
-			ip += length;
-			op += length;
-
-			/* Necessarily EOF when !partialDecoding.
-			 * When partialDecoding, it is EOF if we've either
-			 * filled the output buffer or
-			 * can't proceed with reading an offset for following match.
-			 */
-			if (!partialDecoding || (cpy == oend) || (ip >= (iend - 2)))
-				break;
-		} else {
-			/* may overwrite up to WILDCOPYLENGTH beyond cpy */
-			LZ4_wildCopy(op, ip, cpy);
-			ip += length;
-			op = cpy;
-		}
-
-		/* get offset */
-		offset = LZ4_readLE16(ip);
-		ip += 2;
-		match = op - offset;
-
-		/* get matchlength */
-		length = token & ML_MASK;
-
-_copy_match:
-		if ((checkOffset) && (unlikely(match + dictSize < lowPrefix))) {
-			/* Error : offset outside buffers */
-			goto _output_error;
-		}
-
-		/* costs ~1%; silence an msan warning when offset == 0 */
-		/*
-		 * note : when partialDecoding, there is no guarantee that
-		 * at least 4 bytes remain available in output buffer
-		 */
-		if (!partialDecoding) {
-			assert(oend > op);
-			assert(oend - op >= 4);
-
-			LZ4_write32(op, (U32)offset);
-		}
-
-		if (length == ML_MASK) {
-			unsigned int s;
-
-			do {
-				s = *ip++;
-
-				if ((endOnInput) && (ip > iend - LASTLITERALS))
-					goto _output_error;
-
-				length += s;
-			} while (s == 255);
-
-			if ((safeDecode)
-				&& unlikely(
-					(uptrval)(op) + length < (uptrval)op)) {
-				/* overflow detection */
-				goto _output_error;
-			}
-		}
-
-		length += MINMATCH;
-
-		/* match starting within external dictionary */
-		if ((dict == usingExtDict) && (match < lowPrefix)) {
-			if (unlikely(op + length > oend - LASTLITERALS)) {
-				/* doesn't respect parsing restriction */
-				if (!partialDecoding)
-					goto _output_error;
-				length = min(length, (size_t)(oend - op));
-			}
-
-			if (length <= (size_t)(lowPrefix - match)) {
-				/*
-				 * match fits entirely within external
-				 * dictionary : just copy
-				 */
-				memmove(op, dictEnd - (lowPrefix - match),
-					length);
-				op += length;
-			} else {
-				/*
-				 * match stretches into both external
-				 * dictionary and current block
-				 */
-				size_t const copySize = (size_t)(lowPrefix - match);
-				size_t const restSize = length - copySize;
-
-				LZ4_memcpy(op, dictEnd - copySize, copySize);
-				op += copySize;
-				if (restSize > (size_t)(op - lowPrefix)) {
-					/* overlap copy */
-					BYTE * const endOfMatch = op + restSize;
-					const BYTE *copyFrom = lowPrefix;
-
-					while (op < endOfMatch)
-						*op++ = *copyFrom++;
-				} else {
-					LZ4_memcpy(op, lowPrefix, restSize);
-					op += restSize;
-				}
-			}
-			continue;
-		}
-
-		/* copy match within block */
-		cpy = op + length;
-
-		/*
-		 * partialDecoding :
-		 * may not respect endBlock parsing restrictions
-		 */
-		assert(op <= oend);
-		if (partialDecoding &&
-		    (cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
-			size_t const mlen = min(length, (size_t)(oend - op));
-			const BYTE * const matchEnd = match + mlen;
-			BYTE * const copyEnd = op + mlen;
-
-			if (matchEnd > op) {
-				/* overlap copy */
-				while (op < copyEnd)
-					*op++ = *match++;
-			} else {
-				LZ4_memcpy(op, match, mlen);
-			}
-			op = copyEnd;
-			if (op == oend)
-				break;
-			continue;
-		}
-
-		if (unlikely(offset < 8)) {
-			op[0] = match[0];
-			op[1] = match[1];
-			op[2] = match[2];
-			op[3] = match[3];
-			match += inc32table[offset];
-			LZ4_memcpy(op + 4, match, 4);
-			match -= dec64table[offset];
-		} else {
-			LZ4_copy8(op, match);
-			match += 8;
-		}
-
-		op += 8;
-
-		if (unlikely(cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
-			BYTE * const oCopyLimit = oend - (WILDCOPYLENGTH - 1);
-
-			if (cpy > oend - LASTLITERALS) {
-				/*
-				 * Error : last LASTLITERALS bytes
-				 * must be literals (uncompressed)
-				 */
-				goto _output_error;
-			}
-
-			if (op < oCopyLimit) {
-				LZ4_wildCopy(op, match, oCopyLimit);
-				match += oCopyLimit - op;
-				op = oCopyLimit;
-			}
-			while (op < cpy)
-				*op++ = *match++;
-		} else {
-			LZ4_copy8(op, match);
-			if (length > 16)
-				LZ4_wildCopy(op + 8, match + 8, cpy);
-		}
-		op = cpy; /* wildcopy correction */
-	}
-
-	/* end of decoding */
-	if (endOnInput) {
-		/* Nb of output bytes decoded */
-		return (int) (((char *)op) - dst);
-	} else {
-		/* Nb of input bytes read */
-		return (int) (((const char *)ip) - src);
-	}
-
-	/* Overflow error detected */
-_output_error:
-	return (int) (-(((const char *)ip) - src)) - 1;
-}
-
-int LZ4_decompress_safe(const char *source, char *dest,
-	int compressedSize, int maxDecompressedSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      compressedSize, maxDecompressedSize,
-				      endOnInputSize, decode_full_block,
-				      noDict, (BYTE *)dest, NULL, 0);
-}
-
-int LZ4_decompress_safe_partial(const char *src, char *dst,
-	int compressedSize, int targetOutputSize, int dstCapacity)
-{
-	dstCapacity = min(targetOutputSize, dstCapacity);
-	return LZ4_decompress_generic(src, dst, compressedSize, dstCapacity,
-				      endOnInputSize, partial_decode,
-				      noDict, (BYTE *)dst, NULL, 0);
-}
-
-int LZ4_decompress_fast(const char *source, char *dest, int originalSize)
-{
-	return LZ4_decompress_generic(source, dest, 0, originalSize,
-				      endOnOutputSize, decode_full_block,
-				      withPrefix64k,
-				      (BYTE *)dest - 64 * KB, NULL, 0);
-}
-
-/* ===== Instantiate a few more decoding cases, used more than once. ===== */
-
-static int LZ4_decompress_safe_withPrefix64k(const char *source, char *dest,
-				      int compressedSize, int maxOutputSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      compressedSize, maxOutputSize,
-				      endOnInputSize, decode_full_block,
-				      withPrefix64k,
-				      (BYTE *)dest - 64 * KB, NULL, 0);
-}
-
-static int LZ4_decompress_safe_withSmallPrefix(const char *source, char *dest,
-					       int compressedSize,
-					       int maxOutputSize,
-					       size_t prefixSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      compressedSize, maxOutputSize,
-				      endOnInputSize, decode_full_block,
-				      noDict,
-				      (BYTE *)dest - prefixSize, NULL, 0);
-}
-
-static int LZ4_decompress_safe_forceExtDict(const char *source, char *dest,
-					    int compressedSize, int maxOutputSize,
-					    const void *dictStart, size_t dictSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      compressedSize, maxOutputSize,
-				      endOnInputSize, decode_full_block,
-				      usingExtDict, (BYTE *)dest,
-				      (const BYTE *)dictStart, dictSize);
-}
-
-static int LZ4_decompress_fast_extDict(const char *source, char *dest,
-				       int originalSize,
-				       const void *dictStart, size_t dictSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      0, originalSize,
-				      endOnOutputSize, decode_full_block,
-				      usingExtDict, (BYTE *)dest,
-				      (const BYTE *)dictStart, dictSize);
-}
-
-/*
- * The "double dictionary" mode, for use with e.g. ring buffers: the first part
- * of the dictionary is passed as prefix, and the second via dictStart + dictSize.
- * These routines are used only once, in LZ4_decompress_*_continue().
- */
-static FORCE_INLINE
-int LZ4_decompress_safe_doubleDict(const char *source, char *dest,
-				   int compressedSize, int maxOutputSize,
-				   size_t prefixSize,
-				   const void *dictStart, size_t dictSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      compressedSize, maxOutputSize,
-				      endOnInputSize, decode_full_block,
-				      usingExtDict, (BYTE *)dest - prefixSize,
-				      (const BYTE *)dictStart, dictSize);
-}
-
-static FORCE_INLINE
-int LZ4_decompress_fast_doubleDict(const char *source, char *dest,
-				   int originalSize, size_t prefixSize,
-				   const void *dictStart, size_t dictSize)
-{
-	return LZ4_decompress_generic(source, dest,
-				      0, originalSize,
-				      endOnOutputSize, decode_full_block,
-				      usingExtDict, (BYTE *)dest - prefixSize,
-				      (const BYTE *)dictStart, dictSize);
-}
-
-/* ===== streaming decompression functions ===== */
-
-int LZ4_setStreamDecode(LZ4_streamDecode_t *LZ4_streamDecode,
-	const char *dictionary, int dictSize)
-{
-	LZ4_streamDecode_t_internal *lz4sd =
-		&LZ4_streamDecode->internal_donotuse;
-
-	lz4sd->prefixSize = (size_t) dictSize;
-	lz4sd->prefixEnd = (const BYTE *) dictionary + dictSize;
-	lz4sd->externalDict = NULL;
-	lz4sd->extDictSize	= 0;
-	return 1;
-}
-
-/*
- * *_continue() :
- * These decoding functions allow decompression of multiple blocks
- * in "streaming" mode.
- * Previously decoded blocks must still be available at the memory
- * position where they were decoded.
- * If it's not possible, save the relevant part of
- * decoded data into a safe buffer,
- * and indicate where it stands using LZ4_setStreamDecode()
- */
-int LZ4_decompress_safe_continue(LZ4_streamDecode_t *LZ4_streamDecode,
-	const char *source, char *dest, int compressedSize, int maxOutputSize)
-{
-	LZ4_streamDecode_t_internal *lz4sd =
-		&LZ4_streamDecode->internal_donotuse;
-	int result;
-
-	if (lz4sd->prefixSize == 0) {
-		/* The first call, no dictionary yet. */
-		assert(lz4sd->extDictSize == 0);
-		result = LZ4_decompress_safe(source, dest,
-			compressedSize, maxOutputSize);
-		if (result <= 0)
-			return result;
-		lz4sd->prefixSize = result;
-		lz4sd->prefixEnd = (BYTE *)dest + result;
-	} else if (lz4sd->prefixEnd == (BYTE *)dest) {
-		/* They're rolling the current segment. */
-		if (lz4sd->prefixSize >= 64 * KB - 1)
-			result = LZ4_decompress_safe_withPrefix64k(source, dest,
-				compressedSize, maxOutputSize);
-		else if (lz4sd->extDictSize == 0)
-			result = LZ4_decompress_safe_withSmallPrefix(source,
-				dest, compressedSize, maxOutputSize,
-				lz4sd->prefixSize);
-		else
-			result = LZ4_decompress_safe_doubleDict(source, dest,
-				compressedSize, maxOutputSize,
-				lz4sd->prefixSize,
-				lz4sd->externalDict, lz4sd->extDictSize);
-		if (result <= 0)
-			return result;
-		lz4sd->prefixSize += result;
-		lz4sd->prefixEnd  += result;
-	} else {
-		/*
-		 * The buffer wraps around, or they're
-		 * switching to another buffer.
-		 */
-		lz4sd->extDictSize = lz4sd->prefixSize;
-		lz4sd->externalDict = lz4sd->prefixEnd - lz4sd->extDictSize;
-		result = LZ4_decompress_safe_forceExtDict(source, dest,
-			compressedSize, maxOutputSize,
-			lz4sd->externalDict, lz4sd->extDictSize);
-		if (result <= 0)
-			return result;
-		lz4sd->prefixSize = result;
-		lz4sd->prefixEnd  = (BYTE *)dest + result;
-	}
-
-	return result;
-}
-
-int LZ4_decompress_fast_continue(LZ4_streamDecode_t *LZ4_streamDecode,
-	const char *source, char *dest, int originalSize)
-{
-	LZ4_streamDecode_t_internal *lz4sd = &LZ4_streamDecode->internal_donotuse;
-	int result;
-
-	if (lz4sd->prefixSize == 0) {
-		assert(lz4sd->extDictSize == 0);
-		result = LZ4_decompress_fast(source, dest, originalSize);
-		if (result <= 0)
-			return result;
-		lz4sd->prefixSize = originalSize;
-		lz4sd->prefixEnd = (BYTE *)dest + originalSize;
-	} else if (lz4sd->prefixEnd == (BYTE *)dest) {
-		if (lz4sd->prefixSize >= 64 * KB - 1 ||
-		    lz4sd->extDictSize == 0)
-			result = LZ4_decompress_fast(source, dest,
-						     originalSize);
-		else
-			result = LZ4_decompress_fast_doubleDict(source, dest,
-				originalSize, lz4sd->prefixSize,
-				lz4sd->externalDict, lz4sd->extDictSize);
-		if (result <= 0)
-			return result;
-		lz4sd->prefixSize += originalSize;
-		lz4sd->prefixEnd  += originalSize;
-	} else {
-		lz4sd->extDictSize = lz4sd->prefixSize;
-		lz4sd->externalDict = lz4sd->prefixEnd - lz4sd->extDictSize;
-		result = LZ4_decompress_fast_extDict(source, dest,
-			originalSize, lz4sd->externalDict, lz4sd->extDictSize);
-		if (result <= 0)
-			return result;
-		lz4sd->prefixSize = originalSize;
-		lz4sd->prefixEnd = (BYTE *)dest + originalSize;
-	}
-	return result;
-}
-
-int LZ4_decompress_safe_usingDict(const char *source, char *dest,
-				  int compressedSize, int maxOutputSize,
-				  const char *dictStart, int dictSize)
-{
-	if (dictSize == 0)
-		return LZ4_decompress_safe(source, dest,
-					   compressedSize, maxOutputSize);
-	if (dictStart+dictSize == dest) {
-		if (dictSize >= 64 * KB - 1)
-			return LZ4_decompress_safe_withPrefix64k(source, dest,
-				compressedSize, maxOutputSize);
-		return LZ4_decompress_safe_withSmallPrefix(source, dest,
-			compressedSize, maxOutputSize, dictSize);
-	}
-	return LZ4_decompress_safe_forceExtDict(source, dest,
-		compressedSize, maxOutputSize, dictStart, dictSize);
-}
-
-int LZ4_decompress_fast_usingDict(const char *source, char *dest,
-				  int originalSize,
-				  const char *dictStart, int dictSize)
-{
-	if (dictSize == 0 || dictStart + dictSize == dest)
-		return LZ4_decompress_fast(source, dest, originalSize);
-
-	return LZ4_decompress_fast_extDict(source, dest, originalSize,
-		dictStart, dictSize);
-}
-
-#ifndef STATIC
-EXPORT_SYMBOL(LZ4_decompress_safe);
-EXPORT_SYMBOL(LZ4_decompress_safe_partial);
-EXPORT_SYMBOL(LZ4_decompress_fast);
-EXPORT_SYMBOL(LZ4_setStreamDecode);
-EXPORT_SYMBOL(LZ4_decompress_safe_continue);
-EXPORT_SYMBOL(LZ4_decompress_fast_continue);
-EXPORT_SYMBOL(LZ4_decompress_safe_usingDict);
-EXPORT_SYMBOL(LZ4_decompress_fast_usingDict);
-
-MODULE_LICENSE("Dual BSD/GPL");
-MODULE_DESCRIPTION("LZ4 decompressor");
-#endif
Index: lib/lz4/lz4defs.h
===================================================================
diff --git a/lib/lz4/lz4defs.h b/lib/lz4/lz4defs.h
deleted file mode 100644
--- a/lib/lz4/lz4defs.h	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ /dev/null	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
@@ -1,247 +0,0 @@
-#ifndef __LZ4DEFS_H__
-#define __LZ4DEFS_H__
-
-/*
- * lz4defs.h -- common and architecture specific defines for the kernel usage
-
- * LZ4 - Fast LZ compression algorithm
- * Copyright (C) 2011-2016, Yann Collet.
- * BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *	* Redistributions of source code must retain the above copyright
- *	  notice, this list of conditions and the following disclaimer.
- *	* Redistributions in binary form must reproduce the above
- * copyright notice, this list of conditions and the following disclaimer
- * in the documentation and/or other materials provided with the
- * distribution.
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- * You can contact the author at :
- *	- LZ4 homepage : http://www.lz4.org
- *	- LZ4 source repository : https://github.com/lz4/lz4
- *
- *	Changed for kernel usage by:
- *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>
- */
-
-#include <asm/unaligned.h>
-
-#include <linux/bitops.h>
-#include <linux/string.h>	 /* memset, memcpy */
-
-#define FORCE_INLINE __always_inline
-
-/*-************************************
- *	Basic Types
- **************************************/
-#include <linux/types.h>
-
-typedef	uint8_t BYTE;
-typedef uint16_t U16;
-typedef uint32_t U32;
-typedef	int32_t S32;
-typedef uint64_t U64;
-typedef uintptr_t uptrval;
-
-/*-************************************
- *	Architecture specifics
- **************************************/
-#if defined(CONFIG_64BIT)
-#define LZ4_ARCH64 1
-#else
-#define LZ4_ARCH64 0
-#endif
-
-#if defined(__LITTLE_ENDIAN)
-#define LZ4_LITTLE_ENDIAN 1
-#else
-#define LZ4_LITTLE_ENDIAN 0
-#endif
-
-/*-************************************
- *	Constants
- **************************************/
-#define MINMATCH 4
-
-#define WILDCOPYLENGTH 8
-#define LASTLITERALS 5
-#define MFLIMIT (WILDCOPYLENGTH + MINMATCH)
-/*
- * ensure it's possible to write 2 x wildcopyLength
- * without overflowing output buffer
- */
-#define MATCH_SAFEGUARD_DISTANCE  ((2 * WILDCOPYLENGTH) - MINMATCH)
-
-/* Increase this value ==> compression run slower on incompressible data */
-#define LZ4_SKIPTRIGGER 6
-
-#define HASH_UNIT sizeof(size_t)
-
-#define KB (1 << 10)
-#define MB (1 << 20)
-#define GB (1U << 30)
-
-#define MAXD_LOG 16
-#define MAX_DISTANCE ((1 << MAXD_LOG) - 1)
-#define STEPSIZE sizeof(size_t)
-
-#define ML_BITS	4
-#define ML_MASK	((1U << ML_BITS) - 1)
-#define RUN_BITS (8 - ML_BITS)
-#define RUN_MASK ((1U << RUN_BITS) - 1)
-
-/*-************************************
- *	Reading and writing into memory
- **************************************/
-static FORCE_INLINE U16 LZ4_read16(const void *ptr)
-{
-	return get_unaligned((const U16 *)ptr);
-}
-
-static FORCE_INLINE U32 LZ4_read32(const void *ptr)
-{
-	return get_unaligned((const U32 *)ptr);
-}
-
-static FORCE_INLINE size_t LZ4_read_ARCH(const void *ptr)
-{
-	return get_unaligned((const size_t *)ptr);
-}
-
-static FORCE_INLINE void LZ4_write16(void *memPtr, U16 value)
-{
-	put_unaligned(value, (U16 *)memPtr);
-}
-
-static FORCE_INLINE void LZ4_write32(void *memPtr, U32 value)
-{
-	put_unaligned(value, (U32 *)memPtr);
-}
-
-static FORCE_INLINE U16 LZ4_readLE16(const void *memPtr)
-{
-	return get_unaligned_le16(memPtr);
-}
-
-static FORCE_INLINE void LZ4_writeLE16(void *memPtr, U16 value)
-{
-	return put_unaligned_le16(value, memPtr);
-}
-
-/*
- * LZ4 relies on memcpy with a constant size being inlined. In freestanding
- * environments, the compiler can't assume the implementation of memcpy() is
- * standard compliant, so apply its specialized memcpy() inlining logic. When
- * possible, use __builtin_memcpy() to tell the compiler to analyze memcpy()
- * as-if it were standard compliant, so it can inline it in freestanding
- * environments. This is needed when decompressing the Linux Kernel, for example.
- */
-#define LZ4_memcpy(dst, src, size) __builtin_memcpy(dst, src, size)
-#define LZ4_memmove(dst, src, size) __builtin_memmove(dst, src, size)
-
-static FORCE_INLINE void LZ4_copy8(void *dst, const void *src)
-{
-#if LZ4_ARCH64
-	U64 a = get_unaligned((const U64 *)src);
-
-	put_unaligned(a, (U64 *)dst);
-#else
-	U32 a = get_unaligned((const U32 *)src);
-	U32 b = get_unaligned((const U32 *)src + 1);
-
-	put_unaligned(a, (U32 *)dst);
-	put_unaligned(b, (U32 *)dst + 1);
-#endif
-}
-
-/*
- * customized variant of memcpy,
- * which can overwrite up to 7 bytes beyond dstEnd
- */
-static FORCE_INLINE void LZ4_wildCopy(void *dstPtr,
-	const void *srcPtr, void *dstEnd)
-{
-	BYTE *d = (BYTE *)dstPtr;
-	const BYTE *s = (const BYTE *)srcPtr;
-	BYTE *const e = (BYTE *)dstEnd;
-
-	do {
-		LZ4_copy8(d, s);
-		d += 8;
-		s += 8;
-	} while (d < e);
-}
-
-static FORCE_INLINE unsigned int LZ4_NbCommonBytes(register size_t val)
-{
-#if LZ4_LITTLE_ENDIAN
-	return __ffs(val) >> 3;
-#else
-	return (BITS_PER_LONG - 1 - __fls(val)) >> 3;
-#endif
-}
-
-static FORCE_INLINE unsigned int LZ4_count(
-	const BYTE *pIn,
-	const BYTE *pMatch,
-	const BYTE *pInLimit)
-{
-	const BYTE *const pStart = pIn;
-
-	while (likely(pIn < pInLimit - (STEPSIZE - 1))) {
-		size_t const diff = LZ4_read_ARCH(pMatch) ^ LZ4_read_ARCH(pIn);
-
-		if (!diff) {
-			pIn += STEPSIZE;
-			pMatch += STEPSIZE;
-			continue;
-		}
-
-		pIn += LZ4_NbCommonBytes(diff);
-
-		return (unsigned int)(pIn - pStart);
-	}
-
-#if LZ4_ARCH64
-	if ((pIn < (pInLimit - 3))
-		&& (LZ4_read32(pMatch) == LZ4_read32(pIn))) {
-		pIn += 4;
-		pMatch += 4;
-	}
-#endif
-
-	if ((pIn < (pInLimit - 1))
-		&& (LZ4_read16(pMatch) == LZ4_read16(pIn))) {
-		pIn += 2;
-		pMatch += 2;
-	}
-
-	if ((pIn < pInLimit) && (*pMatch == *pIn))
-		pIn++;
-
-	return (unsigned int)(pIn - pStart);
-}
-
-typedef enum { noLimit = 0, limitedOutput = 1 } limitedOutput_directive;
-typedef enum { byPtr, byU32, byU16 } tableType_t;
-
-typedef enum { noDict = 0, withPrefix64k, usingExtDict } dict_directive;
-typedef enum { noDictIssue = 0, dictSmall } dictIssue_directive;
-
-typedef enum { endOnOutputSize = 0, endOnInputSize = 1 } endCondition_directive;
-typedef enum { decode_full_block = 0, partial_decode = 1 } earlyEnd_directive;
-
-#define LZ4_STATIC_ASSERT(c)	BUILD_BUG_ON(!(c))
-
-#endif
Index: lib/lz4/lz4hc_compress.c
===================================================================
diff --git a/lib/lz4/lz4hc_compress.c b/lib/lz4/lz4hc_compress.c
deleted file mode 100644
--- a/lib/lz4/lz4hc_compress.c	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
+++ /dev/null	(revision 802d968fb2c726f0a9dd88fed80a003d724769d4)
@@ -1,768 +0,0 @@
-/*
- * LZ4 HC - High Compression Mode of LZ4
- * Copyright (C) 2011-2015, Yann Collet.
- *
- * BSD 2 - Clause License (http://www.opensource.org/licenses/bsd - license.php)
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *	* Redistributions of source code must retain the above copyright
- *	  notice, this list of conditions and the following disclaimer.
- *	* Redistributions in binary form must reproduce the above
- * copyright notice, this list of conditions and the following disclaimer
- * in the documentation and/or other materials provided with the
- * distribution.
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- * You can contact the author at :
- *	- LZ4 homepage : http://www.lz4.org
- *	- LZ4 source repository : https://github.com/lz4/lz4
- *
- *	Changed for kernel usage by:
- *	Sven Schmidt <4sschmid@informatik.uni-hamburg.de>
- */
-
-/*-************************************
- *	Dependencies
- **************************************/
-#include <linux/lz4.h>
-#include "lz4defs.h"
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/string.h> /* memset */
-
-/* *************************************
- *	Local Constants and types
- ***************************************/
-
-#define OPTIMAL_ML (int)((ML_MASK - 1) + MINMATCH)
-
-#define HASH_FUNCTION(i)	(((i) * 2654435761U) \
-	>> ((MINMATCH*8) - LZ4HC_HASH_LOG))
-#define DELTANEXTU16(p)	chainTable[(U16)(p)] /* faster */
-
-static U32 LZ4HC_hashPtr(const void *ptr)
-{
-	return HASH_FUNCTION(LZ4_read32(ptr));
-}
-
-/**************************************
- *	HC Compression
- **************************************/
-static void LZ4HC_init(LZ4HC_CCtx_internal *hc4, const BYTE *start)
-{
-	memset((void *)hc4->hashTable, 0, sizeof(hc4->hashTable));
-	memset(hc4->chainTable, 0xFF, sizeof(hc4->chainTable));
-	hc4->nextToUpdate = 64 * KB;
-	hc4->base = start - 64 * KB;
-	hc4->end = start;
-	hc4->dictBase = start - 64 * KB;
-	hc4->dictLimit = 64 * KB;
-	hc4->lowLimit = 64 * KB;
-}
-
-/* Update chains up to ip (excluded) */
-static FORCE_INLINE void LZ4HC_Insert(LZ4HC_CCtx_internal *hc4,
-	const BYTE *ip)
-{
-	U16 * const chainTable = hc4->chainTable;
-	U32 * const hashTable	= hc4->hashTable;
-	const BYTE * const base = hc4->base;
-	U32 const target = (U32)(ip - base);
-	U32 idx = hc4->nextToUpdate;
-
-	while (idx < target) {
-		U32 const h = LZ4HC_hashPtr(base + idx);
-		size_t delta = idx - hashTable[h];
-
-		if (delta > MAX_DISTANCE)
-			delta = MAX_DISTANCE;
-
-		DELTANEXTU16(idx) = (U16)delta;
-
-		hashTable[h] = idx;
-		idx++;
-	}
-
-	hc4->nextToUpdate = target;
-}
-
-static FORCE_INLINE int LZ4HC_InsertAndFindBestMatch(
-	LZ4HC_CCtx_internal *hc4, /* Index table will be updated */
-	const BYTE *ip,
-	const BYTE * const iLimit,
-	const BYTE **matchpos,
-	const int maxNbAttempts)
-{
-	U16 * const chainTable = hc4->chainTable;
-	U32 * const HashTable = hc4->hashTable;
-	const BYTE * const base = hc4->base;
-	const BYTE * const dictBase = hc4->dictBase;
-	const U32 dictLimit = hc4->dictLimit;
-	const U32 lowLimit = (hc4->lowLimit + 64 * KB > (U32)(ip - base))
-		? hc4->lowLimit
-		: (U32)(ip - base) - (64 * KB - 1);
-	U32 matchIndex;
-	int nbAttempts = maxNbAttempts;
-	size_t ml = 0;
-
-	/* HC4 match finder */
-	LZ4HC_Insert(hc4, ip);
-	matchIndex = HashTable[LZ4HC_hashPtr(ip)];
-
-	while ((matchIndex >= lowLimit)
-		&& (nbAttempts)) {
-		nbAttempts--;
-		if (matchIndex >= dictLimit) {
-			const BYTE * const match = base + matchIndex;
-
-			if (*(match + ml) == *(ip + ml)
-				&& (LZ4_read32(match) == LZ4_read32(ip))) {
-				size_t const mlt = LZ4_count(ip + MINMATCH,
-					match + MINMATCH, iLimit) + MINMATCH;
-
-				if (mlt > ml) {
-					ml = mlt;
-					*matchpos = match;
-				}
-			}
-		} else {
-			const BYTE * const match = dictBase + matchIndex;
-
-			if (LZ4_read32(match) == LZ4_read32(ip)) {
-				size_t mlt;
-				const BYTE *vLimit = ip
-					+ (dictLimit - matchIndex);
-
-				if (vLimit > iLimit)
-					vLimit = iLimit;
-				mlt = LZ4_count(ip + MINMATCH,
-					match + MINMATCH, vLimit) + MINMATCH;
-				if ((ip + mlt == vLimit)
-					&& (vLimit < iLimit))
-					mlt += LZ4_count(ip + mlt,
-						base + dictLimit,
-						iLimit);
-				if (mlt > ml) {
-					/* virtual matchpos */
-					ml = mlt;
-					*matchpos = base + matchIndex;
-				}
-			}
-		}
-		matchIndex -= DELTANEXTU16(matchIndex);
-	}
-
-	return (int)ml;
-}
-
-static FORCE_INLINE int LZ4HC_InsertAndGetWiderMatch(
-	LZ4HC_CCtx_internal *hc4,
-	const BYTE * const ip,
-	const BYTE * const iLowLimit,
-	const BYTE * const iHighLimit,
-	int longest,
-	const BYTE **matchpos,
-	const BYTE **startpos,
-	const int maxNbAttempts)
-{
-	U16 * const chainTable = hc4->chainTable;
-	U32 * const HashTable = hc4->hashTable;
-	const BYTE * const base = hc4->base;
-	const U32 dictLimit = hc4->dictLimit;
-	const BYTE * const lowPrefixPtr = base + dictLimit;
-	const U32 lowLimit = (hc4->lowLimit + 64 * KB > (U32)(ip - base))
-		? hc4->lowLimit
-		: (U32)(ip - base) - (64 * KB - 1);
-	const BYTE * const dictBase = hc4->dictBase;
-	U32 matchIndex;
-	int nbAttempts = maxNbAttempts;
-	int delta = (int)(ip - iLowLimit);
-
-	/* First Match */
-	LZ4HC_Insert(hc4, ip);
-	matchIndex = HashTable[LZ4HC_hashPtr(ip)];
-
-	while ((matchIndex >= lowLimit)
-		&& (nbAttempts)) {
-		nbAttempts--;
-		if (matchIndex >= dictLimit) {
-			const BYTE *matchPtr = base + matchIndex;
-
-			if (*(iLowLimit + longest)
-				== *(matchPtr - delta + longest)) {
-				if (LZ4_read32(matchPtr) == LZ4_read32(ip)) {
-					int mlt = MINMATCH + LZ4_count(
-						ip + MINMATCH,
-						matchPtr + MINMATCH,
-						iHighLimit);
-					int back = 0;
-
-					while ((ip + back > iLowLimit)
-						&& (matchPtr + back > lowPrefixPtr)
-						&& (ip[back - 1] == matchPtr[back - 1]))
-						back--;
-
-					mlt -= back;
-
-					if (mlt > longest) {
-						longest = (int)mlt;
-						*matchpos = matchPtr + back;
-						*startpos = ip + back;
-					}
-				}
-			}
-		} else {
-			const BYTE * const matchPtr = dictBase + matchIndex;
-
-			if (LZ4_read32(matchPtr) == LZ4_read32(ip)) {
-				size_t mlt;
-				int back = 0;
-				const BYTE *vLimit = ip + (dictLimit - matchIndex);
-
-				if (vLimit > iHighLimit)
-					vLimit = iHighLimit;
-
-				mlt = LZ4_count(ip + MINMATCH,
-					matchPtr + MINMATCH, vLimit) + MINMATCH;
-
-				if ((ip + mlt == vLimit) && (vLimit < iHighLimit))
-					mlt += LZ4_count(ip + mlt, base + dictLimit,
-						iHighLimit);
-				while ((ip + back > iLowLimit)
-					&& (matchIndex + back > lowLimit)
-					&& (ip[back - 1] == matchPtr[back - 1]))
-					back--;
-
-				mlt -= back;
-
-				if ((int)mlt > longest) {
-					longest = (int)mlt;
-					*matchpos = base + matchIndex + back;
-					*startpos = ip + back;
-				}
-			}
-		}
-
-		matchIndex -= DELTANEXTU16(matchIndex);
-	}
-
-	return longest;
-}
-
-static FORCE_INLINE int LZ4HC_encodeSequence(
-	const BYTE **ip,
-	BYTE **op,
-	const BYTE **anchor,
-	int matchLength,
-	const BYTE * const match,
-	limitedOutput_directive limitedOutputBuffer,
-	BYTE *oend)
-{
-	int length;
-	BYTE *token;
-
-	/* Encode Literal length */
-	length = (int)(*ip - *anchor);
-	token = (*op)++;
-
-	if ((limitedOutputBuffer)
-		&& ((*op + (length>>8)
-			+ length + (2 + 1 + LASTLITERALS)) > oend)) {
-		/* Check output limit */
-		return 1;
-	}
-	if (length >= (int)RUN_MASK) {
-		int len;
-
-		*token = (RUN_MASK<<ML_BITS);
-		len = length - RUN_MASK;
-		for (; len > 254 ; len -= 255)
-			*(*op)++ = 255;
-		*(*op)++ = (BYTE)len;
-	} else
-		*token = (BYTE)(length<<ML_BITS);
-
-	/* Copy Literals */
-	LZ4_wildCopy(*op, *anchor, (*op) + length);
-	*op += length;
-
-	/* Encode Offset */
-	LZ4_writeLE16(*op, (U16)(*ip - match));
-	*op += 2;
-
-	/* Encode MatchLength */
-	length = (int)(matchLength - MINMATCH);
-
-	if ((limitedOutputBuffer)
-		&& (*op + (length>>8)
-			+ (1 + LASTLITERALS) > oend)) {
-		/* Check output limit */
-		return 1;
-	}
-
-	if (length >= (int)ML_MASK) {
-		*token += ML_MASK;
-		length -= ML_MASK;
-
-		for (; length > 509 ; length -= 510) {
-			*(*op)++ = 255;
-			*(*op)++ = 255;
-		}
-
-		if (length > 254) {
-			length -= 255;
-			*(*op)++ = 255;
-		}
-
-		*(*op)++ = (BYTE)length;
-	} else
-		*token += (BYTE)(length);
-
-	/* Prepare next loop */
-	*ip += matchLength;
-	*anchor = *ip;
-
-	return 0;
-}
-
-static int LZ4HC_compress_generic(
-	LZ4HC_CCtx_internal *const ctx,
-	const char * const source,
-	char * const dest,
-	int const inputSize,
-	int const maxOutputSize,
-	int compressionLevel,
-	limitedOutput_directive limit
-	)
-{
-	const BYTE *ip = (const BYTE *) source;
-	const BYTE *anchor = ip;
-	const BYTE * const iend = ip + inputSize;
-	const BYTE * const mflimit = iend - MFLIMIT;
-	const BYTE * const matchlimit = (iend - LASTLITERALS);
-
-	BYTE *op = (BYTE *) dest;
-	BYTE * const oend = op + maxOutputSize;
-
-	unsigned int maxNbAttempts;
-	int ml, ml2, ml3, ml0;
-	const BYTE *ref = NULL;
-	const BYTE *start2 = NULL;
-	const BYTE *ref2 = NULL;
-	const BYTE *start3 = NULL;
-	const BYTE *ref3 = NULL;
-	const BYTE *start0;
-	const BYTE *ref0;
-
-	/* init */
-	if (compressionLevel > LZ4HC_MAX_CLEVEL)
-		compressionLevel = LZ4HC_MAX_CLEVEL;
-	if (compressionLevel < 1)
-		compressionLevel = LZ4HC_DEFAULT_CLEVEL;
-	maxNbAttempts = 1 << (compressionLevel - 1);
-	ctx->end += inputSize;
-
-	ip++;
-
-	/* Main Loop */
-	while (ip < mflimit) {
-		ml = LZ4HC_InsertAndFindBestMatch(ctx, ip,
-			matchlimit, (&ref), maxNbAttempts);
-		if (!ml) {
-			ip++;
-			continue;
-		}
-
-		/* saved, in case we would skip too much */
-		start0 = ip;
-		ref0 = ref;
-		ml0 = ml;
-
-_Search2:
-		if (ip + ml < mflimit)
-			ml2 = LZ4HC_InsertAndGetWiderMatch(ctx,
-				ip + ml - 2, ip + 0,
-				matchlimit, ml, &ref2,
-				&start2, maxNbAttempts);
-		else
-			ml2 = ml;
-
-		if (ml2 == ml) {
-			/* No better match */
-			if (LZ4HC_encodeSequence(&ip, &op,
-				&anchor, ml, ref, limit, oend))
-				return 0;
-			continue;
-		}
-
-		if (start0 < ip) {
-			if (start2 < ip + ml0) {
-				/* empirical */
-				ip = start0;
-				ref = ref0;
-				ml = ml0;
-			}
-		}
-
-		/* Here, start0 == ip */
-		if ((start2 - ip) < 3) {
-			/* First Match too small : removed */
-			ml = ml2;
-			ip = start2;
-			ref = ref2;
-			goto _Search2;
-		}
-
-_Search3:
-		/*
-		* Currently we have :
-		* ml2 > ml1, and
-		* ip1 + 3 <= ip2 (usually < ip1 + ml1)
-		*/
-		if ((start2 - ip) < OPTIMAL_ML) {
-			int correction;
-			int new_ml = ml;
-
-			if (new_ml > OPTIMAL_ML)
-				new_ml = OPTIMAL_ML;
-			if (ip + new_ml > start2 + ml2 - MINMATCH)
-				new_ml = (int)(start2 - ip) + ml2 - MINMATCH;
-
-			correction = new_ml - (int)(start2 - ip);
-
-			if (correction > 0) {
-				start2 += correction;
-				ref2 += correction;
-				ml2 -= correction;
-			}
-		}
-		/*
-		 * Now, we have start2 = ip + new_ml,
-		 * with new_ml = min(ml, OPTIMAL_ML = 18)
-		 */
-
-		if (start2 + ml2 < mflimit)
-			ml3 = LZ4HC_InsertAndGetWiderMatch(ctx,
-				start2 + ml2 - 3, start2,
-				matchlimit, ml2, &ref3, &start3,
-				maxNbAttempts);
-		else
-			ml3 = ml2;
-
-		if (ml3 == ml2) {
-			/* No better match : 2 sequences to encode */
-			/* ip & ref are known; Now for ml */
-			if (start2 < ip + ml)
-				ml = (int)(start2 - ip);
-			/* Now, encode 2 sequences */
-			if (LZ4HC_encodeSequence(&ip, &op, &anchor,
-				ml, ref, limit, oend))
-				return 0;
-			ip = start2;
-			if (LZ4HC_encodeSequence(&ip, &op, &anchor,
-				ml2, ref2, limit, oend))
-				return 0;
-			continue;
-		}
-
-		if (start3 < ip + ml + 3) {
-			/* Not enough space for match 2 : remove it */
-			if (start3 >= (ip + ml)) {
-				/* can write Seq1 immediately
-				 * ==> Seq2 is removed,
-				 * so Seq3 becomes Seq1
-				 */
-				if (start2 < ip + ml) {
-					int correction = (int)(ip + ml - start2);
-
-					start2 += correction;
-					ref2 += correction;
-					ml2 -= correction;
-					if (ml2 < MINMATCH) {
-						start2 = start3;
-						ref2 = ref3;
-						ml2 = ml3;
-					}
-				}
-
-				if (LZ4HC_encodeSequence(&ip, &op, &anchor,
-					ml, ref, limit, oend))
-					return 0;
-				ip = start3;
-				ref = ref3;
-				ml = ml3;
-
-				start0 = start2;
-				ref0 = ref2;
-				ml0 = ml2;
-				goto _Search2;
-			}
-
-			start2 = start3;
-			ref2 = ref3;
-			ml2 = ml3;
-			goto _Search3;
-		}
-
-		/*
-		* OK, now we have 3 ascending matches;
-		* let's write at least the first one
-		* ip & ref are known; Now for ml
-		*/
-		if (start2 < ip + ml) {
-			if ((start2 - ip) < (int)ML_MASK) {
-				int correction;
-
-				if (ml > OPTIMAL_ML)
-					ml = OPTIMAL_ML;
-				if (ip + ml > start2 + ml2 - MINMATCH)
-					ml = (int)(start2 - ip) + ml2 - MINMATCH;
-				correction = ml - (int)(start2 - ip);
-				if (correction > 0) {
-					start2 += correction;
-					ref2 += correction;
-					ml2 -= correction;
-				}
-			} else
-				ml = (int)(start2 - ip);
-		}
-		if (LZ4HC_encodeSequence(&ip, &op, &anchor, ml,
-			ref, limit, oend))
-			return 0;
-
-		ip = start2;
-		ref = ref2;
-		ml = ml2;
-
-		start2 = start3;
-		ref2 = ref3;
-		ml2 = ml3;
-
-		goto _Search3;
-	}
-
-	/* Encode Last Literals */
-	{
-		int lastRun = (int)(iend - anchor);
-
-		if ((limit)
-			&& (((char *)op - dest) + lastRun + 1
-				+ ((lastRun + 255 - RUN_MASK)/255)
-					> (U32)maxOutputSize)) {
-			/* Check output limit */
-			return 0;
-		}
-		if (lastRun >= (int)RUN_MASK) {
-			*op++ = (RUN_MASK<<ML_BITS);
-			lastRun -= RUN_MASK;
-			for (; lastRun > 254 ; lastRun -= 255)
-				*op++ = 255;
-			*op++ = (BYTE) lastRun;
-		} else
-			*op++ = (BYTE)(lastRun<<ML_BITS);
-		LZ4_memcpy(op, anchor, iend - anchor);
-		op += iend - anchor;
-	}
-
-	/* End */
-	return (int) (((char *)op) - dest);
-}
-
-static int LZ4_compress_HC_extStateHC(
-	void *state,
-	const char *src,
-	char *dst,
-	int srcSize,
-	int maxDstSize,
-	int compressionLevel)
-{
-	LZ4HC_CCtx_internal *ctx = &((LZ4_streamHC_t *)state)->internal_donotuse;
-
-	if (((size_t)(state)&(sizeof(void *) - 1)) != 0) {
-		/* Error : state is not aligned
-		 * for pointers (32 or 64 bits)
-		 */
-		return 0;
-	}
-
-	LZ4HC_init(ctx, (const BYTE *)src);
-
-	if (maxDstSize < LZ4_compressBound(srcSize))
-		return LZ4HC_compress_generic(ctx, src, dst,
-			srcSize, maxDstSize, compressionLevel, limitedOutput);
-	else
-		return LZ4HC_compress_generic(ctx, src, dst,
-			srcSize, maxDstSize, compressionLevel, noLimit);
-}
-
-int LZ4_compress_HC(const char *src, char *dst, int srcSize,
-	int maxDstSize, int compressionLevel, void *wrkmem)
-{
-	return LZ4_compress_HC_extStateHC(wrkmem, src, dst,
-		srcSize, maxDstSize, compressionLevel);
-}
-EXPORT_SYMBOL(LZ4_compress_HC);
-
-/**************************************
- *	Streaming Functions
- **************************************/
-void LZ4_resetStreamHC(LZ4_streamHC_t *LZ4_streamHCPtr, int compressionLevel)
-{
-	LZ4_streamHCPtr->internal_donotuse.base = NULL;
-	LZ4_streamHCPtr->internal_donotuse.compressionLevel = (unsigned int)compressionLevel;
-}
-
-int LZ4_loadDictHC(LZ4_streamHC_t *LZ4_streamHCPtr,
-	const char *dictionary,
-	int dictSize)
-{
-	LZ4HC_CCtx_internal *ctxPtr = &LZ4_streamHCPtr->internal_donotuse;
-
-	if (dictSize > 64 * KB) {
-		dictionary += dictSize - 64 * KB;
-		dictSize = 64 * KB;
-	}
-	LZ4HC_init(ctxPtr, (const BYTE *)dictionary);
-	if (dictSize >= 4)
-		LZ4HC_Insert(ctxPtr, (const BYTE *)dictionary + (dictSize - 3));
-	ctxPtr->end = (const BYTE *)dictionary + dictSize;
-	return dictSize;
-}
-EXPORT_SYMBOL(LZ4_loadDictHC);
-
-/* compression */
-
-static void LZ4HC_setExternalDict(
-	LZ4HC_CCtx_internal *ctxPtr,
-	const BYTE *newBlock)
-{
-	if (ctxPtr->end >= ctxPtr->base + 4) {
-		/* Referencing remaining dictionary content */
-		LZ4HC_Insert(ctxPtr, ctxPtr->end - 3);
-	}
-
-	/*
-	 * Only one memory segment for extDict,
-	 * so any previous extDict is lost at this stage
-	 */
-	ctxPtr->lowLimit	= ctxPtr->dictLimit;
-	ctxPtr->dictLimit = (U32)(ctxPtr->end - ctxPtr->base);
-	ctxPtr->dictBase	= ctxPtr->base;
-	ctxPtr->base = newBlock - ctxPtr->dictLimit;
-	ctxPtr->end	= newBlock;
-	/* match referencing will resume from there */
-	ctxPtr->nextToUpdate = ctxPtr->dictLimit;
-}
-
-static int LZ4_compressHC_continue_generic(
-	LZ4_streamHC_t *LZ4_streamHCPtr,
-	const char *source,
-	char *dest,
-	int inputSize,
-	int maxOutputSize,
-	limitedOutput_directive limit)
-{
-	LZ4HC_CCtx_internal *ctxPtr = &LZ4_streamHCPtr->internal_donotuse;
-
-	/* auto - init if forgotten */
-	if (ctxPtr->base == NULL)
-		LZ4HC_init(ctxPtr, (const BYTE *) source);
-
-	/* Check overflow */
-	if ((size_t)(ctxPtr->end - ctxPtr->base) > 2 * GB) {
-		size_t dictSize = (size_t)(ctxPtr->end - ctxPtr->base)
-			- ctxPtr->dictLimit;
-		if (dictSize > 64 * KB)
-			dictSize = 64 * KB;
-		LZ4_loadDictHC(LZ4_streamHCPtr,
-			(const char *)(ctxPtr->end) - dictSize, (int)dictSize);
-	}
-
-	/* Check if blocks follow each other */
-	if ((const BYTE *)source != ctxPtr->end)
-		LZ4HC_setExternalDict(ctxPtr, (const BYTE *)source);
-
-	/* Check overlapping input/dictionary space */
-	{
-		const BYTE *sourceEnd = (const BYTE *) source + inputSize;
-		const BYTE * const dictBegin = ctxPtr->dictBase + ctxPtr->lowLimit;
-		const BYTE * const dictEnd = ctxPtr->dictBase + ctxPtr->dictLimit;
-
-		if ((sourceEnd > dictBegin)
-			&& ((const BYTE *)source < dictEnd)) {
-			if (sourceEnd > dictEnd)
-				sourceEnd = dictEnd;
-			ctxPtr->lowLimit = (U32)(sourceEnd - ctxPtr->dictBase);
-
-			if (ctxPtr->dictLimit - ctxPtr->lowLimit < 4)
-				ctxPtr->lowLimit = ctxPtr->dictLimit;
-		}
-	}
-
-	return LZ4HC_compress_generic(ctxPtr, source, dest,
-		inputSize, maxOutputSize, ctxPtr->compressionLevel, limit);
-}
-
-int LZ4_compress_HC_continue(
-	LZ4_streamHC_t *LZ4_streamHCPtr,
-	const char *source,
-	char *dest,
-	int inputSize,
-	int maxOutputSize)
-{
-	if (maxOutputSize < LZ4_compressBound(inputSize))
-		return LZ4_compressHC_continue_generic(LZ4_streamHCPtr,
-			source, dest, inputSize, maxOutputSize, limitedOutput);
-	else
-		return LZ4_compressHC_continue_generic(LZ4_streamHCPtr,
-			source, dest, inputSize, maxOutputSize, noLimit);
-}
-EXPORT_SYMBOL(LZ4_compress_HC_continue);
-
-/* dictionary saving */
-
-int LZ4_saveDictHC(
-	LZ4_streamHC_t *LZ4_streamHCPtr,
-	char *safeBuffer,
-	int dictSize)
-{
-	LZ4HC_CCtx_internal *const streamPtr = &LZ4_streamHCPtr->internal_donotuse;
-	int const prefixSize = (int)(streamPtr->end
-		- (streamPtr->base + streamPtr->dictLimit));
-
-	if (dictSize > 64 * KB)
-		dictSize = 64 * KB;
-	if (dictSize < 4)
-		dictSize = 0;
-	if (dictSize > prefixSize)
-		dictSize = prefixSize;
-
-	memmove(safeBuffer, streamPtr->end - dictSize, dictSize);
-
-	{
-		U32 const endIndex = (U32)(streamPtr->end - streamPtr->base);
-
-		streamPtr->end = (const BYTE *)safeBuffer + dictSize;
-		streamPtr->base = streamPtr->end - endIndex;
-		streamPtr->dictLimit = endIndex - dictSize;
-		streamPtr->lowLimit = endIndex - dictSize;
-
-		if (streamPtr->nextToUpdate < streamPtr->dictLimit)
-			streamPtr->nextToUpdate = streamPtr->dictLimit;
-	}
-	return dictSize;
-}
-EXPORT_SYMBOL(LZ4_saveDictHC);
-
-MODULE_LICENSE("Dual BSD/GPL");
-MODULE_DESCRIPTION("LZ4 HC compressor");
diff --git a/fs/f2fs/lz4armv8/lz4accel.h b/lib/lz4/lz4armv8/lz4accel.h
rename from fs/f2fs/lz4armv8/lz4accel.h
rename to lib/lz4/lz4armv8/lz4accel.h
diff --git a/fs/f2fs/lz4armv8/lz4armv8.S b/lib/lz4/lz4armv8/lz4armv8.S
rename from fs/f2fs/lz4armv8/lz4armv8.S
rename to lib/lz4/lz4armv8/lz4armv8.S
index b9e6e0f7191575015669e3e954bf4959f43c7836..1d836f686dd9e2322a24a8707c6cafc8a83c6906
GIT binary patch
literal 8088
zc$~dhdvDvw5&xU~6k8hwY=e=wq$JC_IA|_buEx1`VCRaqX+TiqN+Lp$G?$VUANt)p
zv&+}=Vf*gt8iqjbynZ`7ujTMWCnqF)9J%RcHyzyJ+tt5D#AjZ-Nm9mn5Jx0VvS1TD
zx*5JKT+Vy~4<Irbk?Z-*H3=e~rCSf_pkb)%he70pTc44)VGwQiL-_sXuGnDxu10eC
zX6SOxHj9v@DoIN1Sc>+Mqz_L7R0ps+6*>oW%*f|8BTJWOZRiH>t+5+7T*cWs4K@UX
zQD+)&vmjyy-zG_%X5_D1_kjguetYxrjx2+a@$Woi8v=U_c0sfv%Qz()cOPuFn}%+l
zFy;?X={(BP?<9$XDC+_(c>a>OsXN1`y-CiAw@p(9>^|dwM|#`!eH0-{mI^)!e04g{
zTtJ+}Jjk6-_Ev+1MZU^l*KF~4n|iEeW&|ubsU<!J+A?&d8!j$CiJhb@6B{uxGWgc>
zK$}6{P<&mwLAXsBkcIr|44jvFYbRvUDqD-S>5POy#!@$g)nf5%M!Y!rUhJ{(fSE4~
zU}6yhPCxY75)M&Ot!ewyl2GZ-Nz;;q>YQa7ZyeAElrsW<!;{>)bOTVpjv|r8IX^XP
z{B_r>LJv;1Ni<D+pj7fhP$gvNrliIsS@x9H#Ny?WvkV2Ta<y0k)@m_f=_Yd_og6$+
zq}dhZ1a|J!sCzmpiQL^W$LAS}jMY`1dbm1iSjP2Y`{3kYNv<xhfBA>=_xbJJ<;}J8
z;pSpa-kp=5=eO7Mt77?RY><&b%wyFlj&f=ble#xl+8$^u<m^d#I(<R`5+QVgCDs&7
z1^@~14%;Cy6r5!kKj3)`Hm;Y(#9K4(o7ihX)HM%|Xm1-JrE819nrjzv#Pk6u-RS*t
zv`3KUl0#7i5R66OB>KSsu?fZmoz`<;>T<M4kd~5em<i!RN4tbMD;BXd@H9V8(&gqH
zbn-00)B&Ampx6fOUwC4W!#8yD^-sFCiFXy5CxTx(MCF9225%%=z81&gLZYKb0#WX~
zZ|D&%#0Un_FMIo|CIv)cJIUj+VnW?}vt+_bdxE|wm|Q<~LQPE_gM2F93x;O?7~}(_
zyNoKitm|~9gAHB7E~@%#!B#;u6BNv<s@B84aDTW@gGC(bBzah~h(JNzrcgVfDq){0
znv~E`2rr<IK{}EH&~hNu9Dtq!X<-$|RpB0>+)$i=R^&Rmy|x09cxF08Mpu}%^FSX!
z(*Ii?K03+AWRqukIEJEx5e7$X;4+3-$ceSoHDo6V&q53O>BfMgF_8VMh&;?nOE{5S
zfb)C?MfMwuX2oePG?DM}*uf6XPc>z(Pd+0n`SE+kBs)(vwdflX;o(RMtytFR3Njud
zUu%|)LektXOVK(#4<uh}2{qJ!Z4g&P-<^*{3o9b00qN~Ip(8RQQA{>*TJ|xLlNYdE
zV;RDM;AHa1AY)MpPAJHYWDcX6lmjubP=o{4fToKM=}4ZIHd*$9ItNf=0M&;GOfF$j
z9tobvKn89cK@e}5n3{Y9+^ZmmXv<*u2^x+HL54(EdF~1mc{t6jc0`Bti9Bp;JVsBj
zd(B=5Pn1AKqX+vVi4F^a6UNc1#JJ`T9nZ>?Qw9u?W!18rB4jcmNKp^%Cb*nt%KU25
zUYfDvKD=QZCuNdp_gFF&%)*HiEZB9bgr3u?j#8uLk#q`qztG0u#Yr7OVLU6H1=fks
z59cPV<!|QK-|d`QG08_4Q7N0*d#Y5Oe8v$>^?1WmQ|BLfnt&VTj$Et^J4@6G3*&!Q
z*p}HLVt4VV6#^e&L}tD$OA0bbG?N4zdf(0s178U8?V=S?uy3hL9z&B%#X-cHw9^xP
zd8#8y=#p7Xvb7xlvBd~f;%E<k$g4-)>B>YeAT;V8J<B1S)v8KQJ&?9Ks=>5Y4KRFL
z=hHcM^%sN!y-56a`EC{S?&>gIR6AO;6kBg5d{={K$V*J!jrpF=Y8fF7%2}bdpN`2b
zkEW*M#UMvV;9|HY<$5E(2<NaPq7`L_wx#HxHEfj*+p3ic+A0;&lfp^Op{fY?kcTiM
zpoT=dhY8^x2RV=c*=!MNJ99uM3xoW+qxkS2gX}DP$f>kNwIvk?4*aY3gEb4I4p%8o
zA7D|iU|H$Hu2ZXZ9=MB+38Xy`O1od>T&r5hOQrOUl6lHY8DrgbltQFYyXR19Mf{K1
ze--rWK%>w`p$*WyndYYiHy)W30Gy86LAzq@pna7lC7e5MJGTI!G|8ri>KHJIdVMJA
zPr7MbxT&~{Wo)16gT#Uku!_h?*YE!MH|OIo*B_j_&+l(PA9Z|RItGxvH?~TCetpp`
zEm$tEujjYU)%zdkS4XmQ7zEd^oU5A;@2`$!ACcwo4Yn9<z7H}7YoM-+>*P$fxjkI_
z#l2MYuIjqcY^8_d4q4<_V|4yf$mMZZ-eby#M(0P1vpPmo*Gqsid5X2$XFYr>h~*?w
zyEn;s1of89qo^y&&9~49cR5M4P5kJ9pRq0@;<+3rk|Gaa&bKLfTW=b0BituAtY9-q
z+zfUX$=hOvZ88ww<5HF|H^bgpaPZ)f9o>GqJ2oUAO2aGOpPN@!@flJ8p~o{q5kv)t
zj*;30!Smu&wCj9GC@;%3bg&+z9q0hXy$N~o11d*Yuy2dJAxH50=iK_S;*=7jbv&z8
z4e%_<Z+?CLk(LIPB`G}+-wX%%fq^se)ml3kq_OX2?twuL+=D9Frh2)znLfZ-guvOp
z;y)q7VZKKg{r>+ZX=RcculB2$v=oz;@j4HRNxNy%HeTUbF*)imIWqp&uZqcWx5;r!
z@(-;jCMUfnCtdQtb5SvQ`jpAj9$mkAT`@U*#^khL`)?pACeNNXdG@607g5zxV7|mA
zHJ?%Z9D{0;zR)J6&ztS%QEj#gP`^HEy~u1An`*PG0`)7S_DjvGoT|;964bAWwu{X3
z%&N_?UT>z&`J?Emwq@l1X7Otp))b$uQ}GNBBg2}AMbWXaZw&@$K&qe<gPt06YS1&o
z0s$&yY#}xllBXyP^BHOvC_x5Qz=SbK;v^phI_B-?$89GPpTVD;x;69Jxw};JUra*w
A;Q#;t

